20180310:
更换高速的ubuntu16源:
	sudo gedit /etc/apt/sources.list
	注释其他源，添加：
		# 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释
		deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial main restricted universe multiverse
		# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial main restricted universe multiverse
		deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse
		# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse
		deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse
		# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse
		deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-security main restricted universe multiverse
		# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-security main restricted universe multiverse		
		# 预发布软件源，不建议启用
		# deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse
		# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse
	sudo apt-get update

更换高速的pip3源：
	1、在用户目录下（/home/XXX）创建.pip文件夹，并创建pip.conf文件
	2、在pip.conf下输入：（注意：这里更换的是阿里云镜像源）
		[global]
		trusted-host = mirrors.aliyun.com
		index-url = http://mirrors.aliyun.com/pypi/simple
	3、 sudo apt-get update

python 虚拟环境：
	pip3 install virtualenv
	sudo pip3 install virtualenvwrapper	//虚拟环境管理模块
	mkdir $HOME/.local/virtualenvs	  //创建虚拟环境管理目录 (不要加sudo)
	sudo gedit ~/.bashrc 	//末尾添加:
		# by william
		# setting about virtualenvwrapper
		export VIRTUALENVWRAPPER_PYTHON=/usr/bin/python3
		export VIRTUALENV_USE_DISTRIBUTE=1        #  总是使用 pip/distribute                                        
		export WORKON_HOME=$HOME/.local/virtualenvs       # 所有虚拟环境存储的目录
		if [ -e $HOME/.local/bin/virtualenvwrapper.sh ];then
		   source $HOME/.local/bin/virtualenvwrapper.sh                                                
		else if [ -e /usr/local/bin/virtualenvwrapper.sh ];then
		         source /usr/local/bin/virtualenvwrapper.sh
		     fi
		fi
		export PIP_VIRTUALENV_BASE=$WORKON_HOME
		export PIP_RESPECT_VIRTUALENV=true
	source ~/.bashrc	//启动 virtualenvwrapper

	简单创建虚拟环境:
		virtualenv aaa  	//创建一个独立环境空间aaa,在当前文件夹建立一个aaa文件夹,
					//这种默认情况下,会把默认的解释机,和对应的默认软件库加入环境aaa
		virtualenv --no-site-packages bbb //创建一个独立环境空间aaa,在当前文件夹建立一个aaa文件夹,
							  //这情况下,不会把默认的软件库加入环境bbb,
		virtualenv ccc --python=python2   //创建一个独立环境空间ccc,在当前文件夹建立一个ccc文件夹,
						  //这种默认情况下,会把默认的软件库,和默认的解释机加入环境ccc
		启用虚拟环境
		cd ccc	//进入环境文件夹
		source ./bin/activate
		cd ~ //进入要执行的项目的文件夹,例如~
		查看当前状态
		(ccc) kingders@kingders-ThinkPad-T420:~$ 	//先可以直观看到(ccc)前缀,就是说现在处于 ccc 的独立python 工作环境里下
		退出虚拟环境
		deactivate
	通过管理套件创建虚拟环境:
		mkvirtualenv aaa -p python3	//创建
		workon aaa		//进入
		workon			//查看
		deactivate		//退出
		




!@!



20180319
IndentationError: expected an indented block
	这个问题要注意缩进！！








20180401
Tensorflow 常用:
batch 机制 的功能作用的描述 !!
	整理下认识,一般我们求出实际y与训练结果Y'的误差来反向传导更新模型
	而误差,我们可以取,方差,标准差,交叉熵什么的,
	注意,这些是 一个样本下的误差,我们说的方差交叉熵什么的,是基于同一个样本得到的,
	例如,图片a实际标签向量y:[y1,y2,..y10],训练结果标签向量Y':[y'1,y'2,..y'10].
	图片a的方差,是:	{ (y1-y'1)^2 + ..+ (y10 - y'10)^2 } /10
	那么cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction),reduction_indices=[1]))如何理解,
	我们知道,在一次训练里,输入了100张图片,即100张图片通过同样的 weights,blase,得到各自的训练结果标签向量Y'
	tf.reduce_mean括号里算出的是 每张图片自己的交叉熵,
	交叉熵指,有10个特征值的标签向量,把每个特征值的混乱程度加到一块得到的一个值
	然后通过 tf.reduce_mean,把 100 个交叉熵 算出一个 平均值,
	我们最后才通过这个 交叉熵平均值 来反向传导更新一次学习模型的参数weights和biase
	即我们是通过100张图片得到100个交叉熵,然后取100个交叉熵的平均值
	并不是通过100张图片得到1个交叉熵,而且也不知道如何得到
	也不是100个交叉熵,每个交叉熵都反向传导更新模型一次,即不是在一次训练里更新100次模型!!!
	我们只是取 100个交叉熵的平均值来更新 1 次模型
	所以.模型中的 batch 设置为100,那个100,就是这种意思
参数:
tf.Variable.init(initial_value, trainable=True, collections=None, validate_shape=True, name=None)
	initial_value 	所有可以转换为Tensor的类型 	变量的初始值
	trainable 	bool 	如果为True，会把它加入到GraphKeys.TRAINABLE_VARIABLES，才能对它使用Optimizer
	collections 	list 	指定该图变量的类型、默认为[GraphKeys.GLOBAL_VARIABLES]
	validate_shape 	bool 	如果为False，则不进行类型和维度检查
	name 		string 	变量的名称，如果没有指定则系统会自动分配一个唯一的值
从正态分布中输出随机值:	
tf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None) 
	shape: 一维的张量，也是输出的张量。
	mean: 正态分布的均值。
	stddev: 正态分布的标准差。
	dtype: 输出的类型。
	seed: 一个整数，当设置之后，每次生成的随机数都一样。
	name: 操作的名字。
	例子:
	|2,6,7| = tf.random_normal([2.3])
	|9,1,4|
占位符号:
tf.placeholder(dtype, shape=None, name=None)
	此函数可以理解为形参，用于定义过程，在执行的时候再赋具体的值
	dtype：数据类型。常用的是tf.float32,tf.float64等数值类型
	shape：数据形状。默认是None，行不定，比如[2,3]表示列是3，行是2, [None, 3]表示列是3，行不定
	name：名称。
二维数组(二维矩阵)的叠加函数	
tf.reduce_sum()
	例子1:
	[2,2,2] = tf.reduce_sum(|1,1,1|, reduction_indices=[0] )
				|1,1,1|
	|3| = tf.reduce_sum(|1,1,1|, reduction_indices=[1] )
	|3|		    |1,1,1|	
	例子2:
	6 = tf.reduce_sum(|1,1,1|, reduction_indices=[0,1] )
			  |1,1,1|	
	就是先reduction_indices=[0]得到[2,2,2],再reduction_indices=[1] 得到 6 

二维数组的乘函数
tf.matmul()
	a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3]) => [[1. 2. 3.]
	                                                      [4. 5. 6.]]

	b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2]) => [[7. 8.]
	                                                         [9. 10.]
	                                                         [11. 12.]]
	c = tf.matmul(a, b) => [[58 64]
        	                [139 154]]
二维数组的 2次方
tf.square()
	|1,  4, 9| = tf.reduce_sum(|1,2,3|)
	|16,25,36|		   |4,5,6|
二维数组的 平均值:
tf.reduce_mean()
	  2.5 = tf.reduce_mean(|1,2|)
			       |3,4|
	|2,3| = tf.reduce_mean(|1,2|, 0)
			       |3,4|
	|1.5| = tf.reduce_mean(|1,2|, 1)
	|3.5|		       |3,4|

二维数组的 最大值位置:
tf.argmax(|1, 2, 3|,0)=[3,3,1]	//数组从选出 每列中最大值的位置 (从0数)
          |2, 3, 4|
          |5, 4, 3|
          |8, 7, 2|
tf.argmax(|1, 2, 3|,1)=[2, 2, 0, 0] //数组从选出 每行中最大值的位置 (从0数)
          |2, 3, 4|
          |5, 4, 3|
          |8, 7, 2|
tf.cast 类型转换 函数:
	tf.cast([2, 3, 4], tf.float32) //把一维数组的每个int值转换为float值

tf.reduce_sum
	# 'x' is [[1, 1, 1]
	#         [1, 1, 1]]
	tf.reduce_sum(x) ==> 6
	tf.reduce_sum(x, 0) ==> [2, 2, 2]
	tf.reduce_sum(x, 1) ==> [3, 3]
	tf.reduce_sum(x, 1, keep_dims=True) ==> [[3], [3]]
	tf.reduce_sum(x, [0, 1]) ==> 6

按正太分布随机生成 多维数组:
tf.truncated_normal(shape, mean, stddev)
	shape表示生成张量的维度，mean是均值，stddev是标准差。这个函数产生正太分布，均值和标准差自己设定。
	例子: 
	|1.95758033,-0.68666345,-1.83860338, 0.78213859|= tf.truncated_normal(shape=[2,4], mean=0, stddev=1) 
        |0.38035342, 0.57904619,-0.57145643,-1.22899497|
生成tensor：
	tf.zeros(shape, dtype=tf.float32, name=None)	//零矩阵
	tf.zeros_like(tensor, dtype=None, name=None)
	tf.constant(value, dtype=None, shape=None, name='Const') //值都为value的矩阵
	tf.fill(dims, value, name=None)
	tf.ones_like(tensor, dtype=None, name=None)
	tf.ones(shape, dtype=tf.float32, name=None)
生成序列
	tf.range(start, limit, delta=1, name='range')
	tf.linspace(start, stop, num, name=None)
生成随机数
	tf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)
	tf.truncated_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)
	tf.random_uniform(shape, minval=0.0, maxval=1.0, dtype=tf.float32, seed=None, name=None)
	tf.random_shuffle(value, seed=None, name=None)
卷积操作
tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, name=None)
	第一个参数input：指需要做卷积的输入图像，它要求是一个Tensor，
		具有[batch, in_height, in_width, in_channels]这样的shape，
		具体含义是[训练时一个batch的图片数量, 图片高度, 图片宽度, 图像通道数]，
		即这个参数是一个多维数组!!
		注意这是一个4维的Tensor，要求类型为float32和float64其中之一
		然而实际操作是 [batah,in_height*in_width*in_channels]二维数组(二维tensor)
	第二个参数filter：相当于CNN中的卷积核，它要求是一个Tensor，
		具有[filter_height, filter_width, in_channels, out_channels]这样的shape，
		即这个参数是一个多维数组!!
		具体含义是[卷积核的高度，卷积核的宽度，图像通道数，输出图像通道数]，
		要求类型与参数input相同，有一个地方需要注意，第三维in_channels，就是参数input的第四维
		注意,这个参数的含义
		譬如 input是一张28*28有32个通道的图片,即有32张特征图片, 
			有一个 shape=[2,2,32,64]的filter多维向量 ,即说明
			shape=[2,2,32,64]说明 输入的这张图片是有32个通道的,即有32张特征图片的,
			shape=[2,2,32,64]也要求卷积输出后的图片只有有64个通道的,即有64张特征图片的
			shape=[2,2,32,64]也说明,filter里共有 32x64 个独立 2x2 的卷积核
			所以按道理卷积出来的 通道应该有 32x64 个,而不是64 个
			所以这里卷积的过程与我们理论学习的过程有些详细的区别!!
			理论上,输出64通道的话,32个输入通道,每个分配两个卷积核就可以了!
			而这里是每个分配 64 个卷积核,一个通道就能卷积出64这个特征图片了,
			但是接着把每个通道卷积出的64个特征图片,求平均得出2个平均特征图片
			每个通道得出2个平均特征图片,32个就得出64个,
			这64个平均特征图片就凑成最后要输出的64个通道
	第三个参数strides：卷积时在图像每一维的步长，这是一个一维的向量，长度4
		由于对于图片,只有长宽,所以,只用到中间两个值表示步数,其他为1,
		如 strides=[1, 4, 4, 1],表示长宽步长都为4,
		即不在batch和channels上做卷积
	第四个参数padding：string类型的量，只能是"SAME","VALID"其中之一
		padding可'SAME' 可`VALID, SAME表示卷积所有像素,VALID则不是
		VALID:
		 1 2 3 4 5 6 7 8 9 10 11 12 13	多出的12,13不卷积
		|___________|
			  |_____________|
		SAME:
		0 |1 2 3 4 5 6 7 8 9 10 11 12 13| 0 0	卷积所有像素,不够的补0
		|___________|                   |
			  |____________|        |
	                            |________________|	
	第五个参数：use_cudnn_on_gpu:bool类型，是否使用cudnn加速，默认为true
	做后一个是当前卷积操作的名字
		结果返回一个Tensor，这个输出，
		就是我们常说的feature map，shape仍然是[batch, height, width, channels]这种形式。
		即是下一层的input,下一层的卷积的输入图像
池化操作:
tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')	
	建立一个池化操作
	池化输出的是邻近区域的概括统计量，一般是矩形区域。池化有最大池化、平均池化、滑动平均池化、L2范数池化等
	池化与卷积意义上不一样,虽然操作上有很多地方相同
	第一个参数value：需要池化的输入，一般池化层接在卷积层后面，
		所以输入通常是feature map，依然是[batch, height, width, channels]这样的shape
	第二个参数ksize：池化窗口的大小，取一个四维向量，一般是[1, height, width, 1]，
		因为我们不想在batch和channels上做池化，所以这两个维度设为了1
	第三个参数strides：和卷积类似，窗口在每一个维度上滑动的步长，一般也是[1, stride,stride, 1]
	第四个参数padding：和卷积类似，可以取'VALID' 或者'SAME'	

tf.transpose(outputs, [1,0,2]:表示把 batch steps 两维转换
	即outputs 变成了 [steps,batch,输出向量维数]
	图例直观解析
		假如 outputs本来是这样的 shape:[3,2,5]:
		[

			[
				[1,1,1,1,1,1]
				[2,2,2,2,2,2]		
			]
			,

			[
				[3,3,3,3,3,3]
				[4,4,4,4,4,4]		
			]
			,

			[
				[5,5,5,5,5,5]
				[6,6,6,6,6,6]		
			]
			,
		]
		transpose(outputs, [1,0,2] -> outputs shape:[2,3,5]:
		只换第 batch steps 维, 
		输出向量维数 这维内容不变, 这维可以简单标记:
			A = [1,1,1,1,1,1]
			B = [3,3,3,3,3,3]
			C = [5,5,5,5,5,5]
			D = [2,2,2,2,2,2]		
			E = [4,4,4,4,4,4]		
			F = [6,6,6,6,6,6]
    		outputs 简单记为:
		[[A,B]
		 [C,D]
		 [E,F]]
		换第 batch steps 后:
		[[A,C,E]
		 [B,D,F]]
		把标记 ABCDEF换换回去就得:
		[

			[
				[1,1,1,1,1,1]
				[3,3,3,3,3,3]
				[5,5,5,5,5,5]

			]
			,

			[
				[2,2,2,2,2,2]		
				[4,4,4,4,4,4]		
				[6,6,6,6,6,6]		
			]
			,
		]		

解构数组:tf.unstack()
	没有其他参数,默认解构最前的一维,即 steps维,即第0维
	解构图示:
		假如 outputs是这样子的:
		outputs = [
	
				[
					[1,1,1,1,1,1]
					[2,2,2,2,2,2]		
				]
				,

					[
					[3,3,3,3,3,3]
					[4,4,4,4,4,4]		
				]
				,

				[
					[5,5,5,5,5,5]
					[6,6,6,6,6,6]		
				]

			]
		tf.unstack(outputs) 后,得到
		outputs[0] = [ [1,1,1,1,1,1]
			       [2,2,2,2,2,2] ]
		outputs[1] = [ [3,3,3,3,3,3]
			       [4,4,4,4,4,4] ]
		outputs[2] = [ [5,5,5,5,5,5]
			       [6,6,6,6,6,6] ]
		output 从 变量 变成了 数组变量 !!



>>>>>>>>>>>>>>>>

20180401
回归:
/home/william/AI/machine learning/回归/code.py
通过散点数据训练一个模型,找到散点的落入规律,
这里找到的规律是,散点落入一个二次函数范畴的规律,也就说通过模型得到的散点建立的曲线越来越像二次曲线 
先人为制作一个 二次函数曲线 的散点图,作为样本参数 这里是建立 300 个散点
	x_data = np.linspace(-1, 1, 300, dtype=np.float32)[:, np.newaxis]
	noise = np.random.normal(0, 0.05, x_data.shape).astype(np.float32)
	y_data = np.square(x_data) - 0.5 + noise  
再通过 matplotlib.pyplot 显示散点图!! 
	plt.scatter(x_data, y_data)	
	plt.show()	//这里训练和显示散点图是冲突的,要训练,就要屏蔽显示散点图
训练模型建立:
//定义如何建立层
def add_layer(inputs, in_size, out_size, activation_function=None): //定义如何建立层
	重点如何地定义变量定义:例如:
	Weights = tf.Variable(tf.random_normal([in_size, out_size]))
	tf.random_normal 得随机变量值,这里出来是随机二维数组 in_size*out_size,其他参数默认,
	tf.Variable 把这个随机二位数组值变成 tensorflow变量
//定义训练模型的输入输出变量占位符
	xs = tf.placeholder(tf.float32, [None, 1])
	ys = tf.placeholder(tf.float32, [None, 1])
	//输入的xs.输出的ys是一维数组,而[None, 1]表示列是1，行不定的一维维数组,
	//注意是一维数组,不是一维向量,是有多个一维向量组成的一维数组
	//之所一维数组,是因为在这个例子里,样本是一个个的点坐标,而每一次训练是一次性输入XX个样本,统一计算
	//这堆样本的x分量会放入xs里,变成一个有XX个一维向量的一维数组
	//这堆样本的y分量会放入ys里,变成一个有XX个一维向量的一维数组	
//构建多层模型
	这里只有两层: l1隐藏层,和 prediction预测输出层
	l1 = add_layer(xs, 1, 10, activation_function=tf.nn.relu)
	prediction = add_layer(l1, 10, 1, activation_function=None)
	//li层,会对结果执行relu激活算法,使第一层的输出有10个变量的一维数组的变量值在0-1附近
	//使用激活函数,可以优化避免梯度消失和梯度爆炸的情况发生
//设置训练方式
	loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys-prediction), reduction_indices=[1]))
	//例子说明:例如一次训练直接输入5个样本点 [a,A][b,B][c,C][d,D][e,E]
	//那么 xs=|a| ,  ys=|A|  通过 xs 得到的 prediction=|Y|
	//	  |b|       |B|				  |H|
	//	  |c|       |C|				  |Z|
	//	  |d|       |D|				  |T|
	//	  |e|       |E|     			  |V|
	//那么 tf.square就得到 |(A-Y)^2|
	//		      |(B-H)^2|
	//		      |(C-Z)^2|
	//		      |(D-T)^2|
	//		      |(E-V)^2|
	//然后 tf.reduce_sum(..,reduction_indices=[1]) 得到:
	//	|(A-Y)^2|
	//	|(B-H)^2|
	//	|(C-Z)^2|
	//	|(D-T)^2|
	//	|(E-V)^2|
	//	因为是每行只有一个量,所 tf.reduce_sum 后并没有变化
	//然后 tf.reduce_mean()得到平均值:
	//	( |(A-Y)^2| + |(B-H)^2| + |(C-Z)^2| + |(D-T)^2| + |(E-V)^2| ) / 5 	
	train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)
	//使用 普通的梯度下降的优化方法(GradientDescent),来训练优化 loss, 学习率是 0.1
	//最终这次训练会更新所有的 weights 和 biases
//初始化tf训练环境
	sess = tf.Session(),	
//初始化tensorflow的所有变量
	init = tf.global_variables_initializer()
	sess.run(init)
//开始训练
	for i in range(1000):
    		sess.run(train_step, feed_dict={xs: x_data, ys: y_data})
	//执行1000次训练,每次训练都 读入 300 个散点,即 xs数组有300行, ys数组有300行
	//而这里我们就只有300个样本,所以每次训练都读入同一组数据
    		if i % 50 == 0:
        	print(sess.run(loss, feed_dict={xs: x_data, ys: y_data}))
	//每50次训练后,打印一次 loss

/home/william/AI/machine learning/回归/code.py
这个例子补充主要是图像监测部分!!
使用:import matplotlib.pyplot as plt
	fig = plt.figure()		//创建一个独立的视图窗口
	ax = fig.add_subplot(1,1,1)	//在窗口添加一个子视图ax
	ax.scatter(x_data, y_data)	//子视图的 x,y 轴对应 x_data, y_data
	plt.ion()			//使用交互形式,
	plt.show()			//一直显示图,(如果不开启交互模式,默认是阻塞模式,)
					//交互模式下一直显示图,图会一直显示,而程序也会继续plt.show()后的内容
					//阻塞模式下一直显示图,会一直卡在plt.show(),
	lines = ax.plot(x_data, prediction_value, 'r-', lw=5)
					//据 x_data, prediction_value 的一堆散点画出一条线
					//x值,y值,红色,宽度5
	plt.pause(1)		//暂停一秒
	ax.lines.remove(lines[0])//把刚刚画的线去掉,(这样就画下一条线,就不会挡住什么的)







20180401
学习使用tensorboard监视模型:/home/william/AI/machine learning/tensorboard
code3.py
重点是,在使用 tf.xxxx之前,先添加 with tf.name_scope('XXXX'):
例如:
with tf.name_scope('layer'):
    with tf.name_scope('weights'):
        Weights = tf.Variable(tf.random_normal([in_size, out_size]), name='W')
with tf.name_scope('inputs'):
    xs = tf.placeholder(tf.float32, [None, 1], name='x_input')
with tf.name_scope('loss'):
    loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction), reduction_indices=[1]))
with tf.name_scope('train'):
    train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)
然后,sess = tf.Session()后,init = tf.global_variables_initializer()前
    writer = tf.summary.FileWriter("logs/", sess.graph)
这样执行代码时,会把模型图加载到logs/里,
终端运行 tensorboard --logdir=logs后,就可以在浏览器http://127.0.0.1:6006/查看到模型内容

code4.py
重点添加训练参数的跟踪记录表
例如:
    Weights = tf.Variable(tf.random_normal([in_size, out_size]), name='W')
    tf.summary.histogram(layer_name + '/weights', Weights) 
    //在tensorboard的histogram和distribution栏添加Weights 的训练跟踪记录表
    loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction),reduction_indices=[1]))
    tf.summary.scalar('loss', loss)
    //在tensorboard的scalar栏添加loss 的训练跟踪记录表
然后,sess = tf.Session()后,init = tf.global_variables_initializer()前
    merged = tf.summary.merge_all()
    writer = tf.summary.FileWriter("logs2/", sess.graph)
这样执行代码时,会把模型图,还有训练跟踪表设置加载到logs2/里,
然后每隔n次训练后,给所有训练跟踪表添加新数据
    result = sess.run(merged, feed_dict={xs: x_data, ys: y_data})
    writer.add_summary(result, i)
最后终端运行 tensorboard --logdir=logs后,就可以在浏览器http://127.0.0.1:6006/查看到模型内容









20180401
分类:/home/william/AI/machine learning/分类
code5.py
注意,这里使用的交叉熵跟我之前分析的交叉熵有所区别
我之前分析的是 基于一个图像样品,得到的交叉熵再反向传导学习,
而这里却是 100个 样本的交叉熵,这里的交叉熵有点像平均值的意思,然后再反向传导学习
http://www.360doc.com/content/17/0118/20/10408243_623338635.shtml
经过慎重分析发现:
	注意不是 100个 样本的交叉熵,
	而是每个样本一个交叉熵,共100个,然后把他们都加起来除以100
	得到一个交叉熵的平均值,使用这个平均值反向传导训练模型
模型训练的思路分析:
每张图片有728个像素点: [x(1)1,x(1)2,x(1)3,...x(1)728],
	xs符合[None, 784],none=100行,即xs包含100张图片,即:
	xs = [x(1)1,x(1)2,x(1)3,x(1)4,...x(1)728],
	     [x(2)1,x(2)2,x(2)3,x(2)4,...x(2)728]
	     .......
	     [x(100)1,x(100)2,x(100)3,x(728)4,...x(100)728]
Weights符合[in_size, out_size],in_size=728,out_size=10,即
	Weights = [w(1)1,w(2)1,..w(10)1],
	          [w(1)2,w(2)2,..w(10)2]
		  [w(1)3,w(2)3,..w(10)3]
		  [w(1)4,w(2)4,..w(10)4]
		  .......
		  [w(1)728,w(2)728,..w(10)728]
bias符合[1,out_size],out_size=10,即
	bias = [b1,b2,b3,b4,..b10]
图片对应的标签向量: [y(1)1,y(1)2,..y(1)10]
	ys符合[None, 10],none=100行,即 ys 对应100张图片的 100 个标签向量:
	ys = [y(1)1,y(1)2,..y(1)10]
	     [y(2)1,y(2)2,..y(2)10]
	     [y(3)1,y(3)2,..y(3)10]
	     [y(4)1,y(4)2,..y(4)10]
	     ........
	     [y(100)1,y(100)2,..y(100)10]
那么,Wx_plus_b=y:
[x(1)1,x(1)2,x(1)3,x(1)4,...x(1)728]          * [w(1)1,w(2)1,..w(10)1]      + [b1,b2,b3,b4,..b10] = [y(1)1,y(1)2,..y(1)10]
[x(2)1,x(2)2,x(2)3,x(2)4,...x(2)728]            [w(1)2,w(2)2,..w(10)2]                              [y(2)1,y(2)2,..y(2)10]
 .......                                        [w(1)3,w(2)3,..w(10)3]                              [y(3)1,y(3)2,..y(3)10]
[x(100)1,x(100)2,x(100)3,x(728)4,...x(100)728]  [w(1)4,w(2)4,..w(10)4]                              [y(4)1,y(4)2,..y(4)10]
                                                .......						    ........
	                                        [w(1)728,w(2)728,..w(10)728]                        [y(100)1,y(100)2,..y(100)10]

batch 机制的功能作用的描述 !!
	整理下认识,一般我们求出实际y与训练结果Y'的误差来反向传导更新模型
	而误差,我们可以取,方差,标准差,交叉熵什么的,
	注意,这些是 一个样本下的误差,我们说的方差交叉熵什么的,是基于同一个样本得到的,
	例如,图片a实际标签向量y:[y1,y2,..y10],训练结果标签向量Y':[y'1,y'2,..y'10].
	图片a的方差,是:	{ (y1-y'1)^2 + ..+ (y10 - y'10)^2 } /10
	那么cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction),reduction_indices=[1]))如何理解,
	我们知道,在一次训练里,输入了100张图片,即100张图片通过同样的 weights,blase,得到各自的训练结果标签向量Y'
	tf.reduce_mean括号里算出的是 每张图片自己的交叉熵,
	交叉熵指,有10个特征值的标签向量,把每个特征值的混乱程度加到一块得到的一个值
	然后通过 tf.reduce_mean,把 100 个交叉熵 算出一个 平均值,
	我们最后才通过这个 交叉熵平均值 来反向传导更新一次学习模型的参数weights和biase
	即我们是通过100张图片得到100个交叉熵,然后取100个交叉熵的平均值
	并不是通过100张图片得到1个交叉熵,而且也不知道如何得到
	也不是100个交叉熵,每个交叉熵都反向传导更新模型一次,即不是在一次训练里更新100次模型!!!
	我们只是取 100个交叉熵的平均值来更新 1 次模型
	所以.模型中的 batch 设置为100,那个100,就是这种意思
其他重点内容:
	mnist = input_data.read_data_sets('MNIST_data', one_hot=True) //这里是导入官方训练样品库的方法
	batch_xs, batch_ys = mnist.train.next_batch(100)//从训练集,取出100个28*28图片样本和对应标签向量
	mnist.test.images, mnist.test.labels//检验集的图片样品,和对应标签向量
	
code6.py
重点是使用了sklearn 生成的样本,可以辅助我们学习使用tensorflow做很多模拟事情
	from sklearn.datasets import load_digits
	from sklearn.model_selection import train_test_split
	from sklearn.preprocessing import LabelBinarizer
	# load data
	digits = load_digits()
	X = digits.data
	y = digits.target
	y = LabelBinarizer().fit_transform(y)
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3)
还有的是,清晰告诉我们如何使用 dropout
	def add_layer(inputs, in_size, out_size, layer_name, activation_function=None, ):
	    # add one more layer and return the output of this layer
	    Weights = tf.Variable(tf.random_normal([in_size, out_size]))
	    biases = tf.Variable(tf.zeros([1, out_size]) + 0.1, )
	    Wx_plus_b = tf.matmul(inputs, Weights) + biases
	    # here to dropout
	    Wx_plus_b = tf.nn.dropout(Wx_plus_b, keep_prob)
	    if activation_function is None:
	        outputs = Wx_plus_b
	    else:
	        outputs = activation_function(Wx_plus_b, )
	    tf.summary.histogram(layer_name + '/outputs', outputs)
	    return outputs
	之前学习到:dropout不算是一个正规正矩的优化器，他的工作是，每次网络工作时，
	都随机抛弃一部分的神经元的作用，从而避免过度拟合
	从这个层建设定义中dropout的位置可看出,dropout不属于激活函数,
	同时也不能算作是一个优化器











20180403
CNN: code7.py
准确率计算:
def compute_accuracy(v_xs, v_ys):
    global prediction
    y_pre = sess.run(prediction, feed_dict={xs: v_xs, keep_prob: 1})
    correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(v_ys,1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
    result = sess.run(accuracy, feed_dict={xs: v_xs, ys: v_ys, keep_prob: 1})
    return result
	我们知道 图片对应的标签向量我们成为 one-hot 向量,即这样表示:
		数字图片0:[1,0,0,0,0,0,0,0,0,0]
		数字图片1:[0,1,0,0,0,0,0,0,0,0]
		...
		数字图片9:[0,0,0,0,0,0,0,0,0,1]
	而通过学习模型学习到的标签向量,往往不是整数的,例如:
		数字图片1:[0.01, 0.98, 0, 0.001, 0.1, 0.1, 0, 0, 0.02, 0.4]
		而标签向量 与 学习得到的标签向量, 位置1(从0数)的值都是最大的.
		这样就认为 模型准确学习识别出数字1的图片
	重点看: correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(v_ys,1))
		先tf.argmax算出 标签向量 与 学习得到的标签向量,的最大值位置
		比较这两个位置是否一样.
		注意 y_pre, v_ys 在这里是二维数值,即包含多个标签向量,
		tf.argmax后是一个表示位置意义的一维数组
		tf.equal后是一个表示正确与否意义的一维数组
	        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
		先把 correct_prediction 向量的每个值都转为 float类型,然后把加起来求平均
		这里,如果模型相当好,correct_prediction 向量的每个值基本为1,
		最后的平均值就接近1了
模型参数设置!!
权重weight
def weight_variable(shape):
    initial = tf.truncated_normal(shape, stddev=0.1)
    return tf.Variable(initial)
	按正太分布随机生成 张量的维度为shanpe 的权重数组 initial,
	然后计入 tf.Variable()
def bias_variable(shape):
    initial = tf.constant(0.1, shape=shape)
    return tf.Variable(initial)
	生成值为 0.1 ,张量的维度为shanpe 的 常量数组 inital
	然后计入 tf.Variable()
def conv2d(x, W):
    # stride [1, x_movement, y_movement, 1]
    # Must have strides[0] = strides[3] = 1
    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')
	建立一个卷积操作,注意strides=[1, 1, 1, 1]描述的是卷积核移动步长
	由于对于图片,只有长宽,所以,只用到中间两个值表示步数,其他为1,
	如 strides=[1, 4, 4, 1],表示长宽步长都为4
	padding可'SAME' 可`VALID, SAME表示卷积所有像素,VALID则不是
	VALID:
	 1 2 3 4 5 6 7 8 9 10 11 12 13	多出的12,13不卷积
	|___________|
		  |_____________|
	SAME:
	0 |1 2 3 4 5 6 7 8 9 10 11 12 13| 0 0	卷积所有像素,不够的补0
	|___________|                   |
		  |____________|        |
                            |________________|		

def max_pool_2x2(x):
    # stride [1, x_movement, y_movement, 1]
    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')
	建立一个池化操作
	池化输出的是邻近区域的概括统计量，一般是矩形区域。池化有最大池化、平均池化、滑动平均池化、L2范数池化等
	池化与卷积意义上不一样,虽然操作上有很多地方相同
	max_pool 是指最大池化的意思
最后补充内容:
	batch_xs, batch_ys = mnist.train.next_batch(100)
	sess.run(train_step, feed_dict={xs: batch_xs, ys: batch_ys, keep_prob: 0.5})
	先是,获取100张图片 的数据,
	然后加载到 占位符空间里,成为真正的参数
	compute_accuracy(mnist.test.images[:1000], mnist.test.labels[:1000]))
	输入1000用于mnist测试图片,即 batch=1000,使用定义的compute_accuracy函数检测准确率
code8.py是code7的后续
主要分析如何建设层模型!!
	# define placeholder for inputs to network
	xs = tf.placeholder(tf.float32, [None, 784])/255.   # 28x28	
	ys = tf.placeholder(tf.float32, [None, 10])
	keep_prob = tf.placeholder(tf.float32)
	x_image = tf.reshape(xs, [-1, 28, 28, 1])
	# print(x_image.shape)  # [n_samples, 28,28,1]
		占位符 xs 表示输入的每张图片是28X18=784个像素点,未知有多少图片输入所以none
		由于图片像素点值都是从值 0-255 来记录颜色的!!,增大后续计算量级,所以 除以255,
		把值域从 0-255 压缩到 0-1,只是值的比例缩小了,没有改变值记录的图像信息
		xs 是一个二维数组,一维表示图片,一维表示图片数量(即batch大小)
		所以需要转换传换成 4维数组, [batch,高,寬,深度(通道数)]
	## conv1 layer ##
	W_conv1 = weight_variable([5,5, 1,32]) # patch 5x5, in size 1, out size 32
	b_conv1 = bias_variable([32])
	h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1) # output size 28x28x32
	h_pool1 = max_pool_2x2(h_conv1)                                         # output size 14x14x32
		第一层CNN
		1*32 个 5x5 卷积核 卷积batch张,28x28的图片,最后得batch张 含32个通道的卷积后图片
		然后,卷积后图片 都经过池化得到  batch张 含32个通道的池化后图片	
	## conv2 layer ##
	W_conv2 = weight_variable([5,5, 32, 64]) # patch 5x5, in size 32, out size 64
	b_conv2 = bias_variable([64])
	h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2) # output size 14x14x64
	h_pool2 = max_pool_2x2(h_conv2)                                         # output size 7x7x64
		第二层CNN
		32*64 个 5x5 卷积核 卷积batch张,含32个通道的上一层池化后图片,
		先得到batch张,含 32*64 个通道的卷积后图片
		然后,平均压简成 batch张,含 64 个通道的卷积后图片
		然后,卷积后图片 都经过池化得到  batch张 含64个通道的池化后图片				
	## fc1 layer ##
	W_fc1 = weight_variable([7*7*64, 1024])
	b_fc1 = bias_variable([1024])
	# [n_samples, 7, 7, 64] ->> [n_samples, 7*7*64]
	h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])
	h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)
	h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)
		全连接层1
		第二层CNN 得到的  batch张 含64个通道的池化后图片 是一个[batch,高,寬,深度(通道数)]4维数组
		转换成 二维数组[batch,第二层池化后图片],
		然后换 Wx_plus_b 的层模型,使用relu激励函数 继续构建
		这里添加了 dropout 处理,是为了避免过拟合问题	
	## fc2 layer ##
	W_fc2 = weight_variable([1024, 10])
	b_fc2 = bias_variable([10])
	prediction = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)
		全连接层1
		同样使用 Wx_plus_b 的层模型 使用softmax激励函数 最后得到 学习的 one-hot 标签向量!!
	# the error between prediction and real data
	cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction),
                                              reduction_indices=[1]))       # loss
	train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)
		同样通过 -tf.reduce_sum(ys * tf.log(prediction),reduction_indices=[1])
		算出每张图片的 交叉熵,然后 tf.reduce_mean 求平均得到一个平均交叉熵
		通过 优化器 AdamOptimizer 优化器处理平均交叉熵 来执行反向传导,
		更新一次所有的学习参数(包括所有的卷积核,所有的weight和biase)










20180404
RNN 
code9.py 根据sin 画出cos
首先看 执行训练的部分:
	_, cost, state, pred = sess.run(
            [model.train_op, model.cost, model.cell_final_state, model.pred],
            feed_dict=feed_dict)
	sess.run先后执行了4个函数模块 model.train_op, model.cost, model.cell_final_state, model.pred
	sess.run执行 train_op时,会回溯执行嵌套操作
		当前的 cost,cell_final_state, pred,最后更新了一次学习模型
	执行完train_op后 ,sess.run继续执行 cost
		这时执行的 cost,得到的是 更新学习模型后的状态下得到的 cost
	sess.run继续执行 cell_final_state,得到的是 更新学习模型后的状态下得到的 cell_final_state
	sess.run继续执行 pred,得到的是 更新学习模型后的状态下得到的 pred
初步了解 (只考虑batch为1,即喂一堆段数据的情况)
	这里每次input是一段数据有 20 个数据单元(20个x数据得到的sin(x) ) ,
	然后每个数据单元通过 同一输入转化矩阵 wx_plus_b 扩成 10维输入向量:x1,x2...x10
	就是说输入转化矩阵有 10 个 W 和 B 需要学习
	然后 lstm 的cell 和hidden_unit 状态向量 都是10维向量 , 
	每次 给 lstm 喂 一个 10维输入向量 得到 一个 cell 和 hidden_unit 状态向量
	这个 cell 和 hidden_unit 状态向量,又作为下一次 喂 输入向量需要 的 cell 状态向量 和hidden_unit 状态向量
	有20个数据单元,所以一共喂 20次 10维输入向量
	收集 每次得到lstm 输出的 hidden_unit 状态向量,作为输出向量 即收集到 20个
	每个输出向量又通过 另一个 相同的 矩阵乘法 wx_plus_b 缩成一个输出数据
	输出转化矩阵有 10 个 W 和 B 需要学习
	得到20个输出数据(20个),
	再与20个真正的输出数据(20个x数据对应cos(x) 比较 得到一个 误差值(不是误差向量) 
	误差越小, 表示 从sin 推导出的 cos 越准确 !! 
	 
code10.py 同样先初步了解
这里是,一串一串地输入图片像数流数据,最后学习分辨出是什么图片!!!
初步了解
	这里每次input是一段数据有 28 条像素数据,
	每条像素数据 含28个像素点信息.
	每条像素数据 通过 同一个输入转化矩阵 wx_plus_b  扩成 128维向量x1,x2...x128
	输入转化矩阵有 128*28 个 W 和 128 个 B 需要学习
	然后 lstm 的 cell 和hidden_unit 状态向量 都是128维向量 , 
	每次 给 lstm 喂 一个 128维输入向量 得到 一个 cell 状态向量 和 hidden_unit 状态向量
	这个 cell 和 hidden_unit 状态向量,又作为下一次 喂 输入向量需要 的 cell 和hidden_unit 状态向量
	有 28 条像素数据,所以一共喂 28次 128维输入向量
	收集 每次得到lstm 输出的 hidden_unit 状态向量,作为输出向量 即收集到 20个
	把这 28 个 输出向量组成 一个 28维数组,即 28*128 矩阵,
	然而其实,我们只需要最后一个输出向量,(第28个),其他的丢弃
	后一个输出向量 乘以一个 转化矩阵 得到  一个 10维变量
	这个转化矩阵的 W 和 B 也需要学习
	这个10维变量 与 实际图片对应的 one-hot 变量比较 得到一个 误差值(不是误差向量)
	误差越小, 识别图片的准确率越高. 
参考:	
	http://dy.163.com/v2/article/detail/CTIPFRJF0511K58A.html
	https://www.zhihu.com/question/40819582
	https://blog.csdn.net/u014595019/article/details/52605693
	https://blog.csdn.net/u014595019/article/details/52759104
	https://www.jianshu.com/p/4e285112b988
lstm单元:
	t表示当前时间

                                     ht
		  ___________________|____
	C(t-1)----|                      |-----Ct
	          |                      |
		  |                      |
	          |                      |
	h(t-1)----|______________________|-----ht
	            |
		   xt

	xt     当前输入值,是一个向量!!
	h(t-1) 前一刻的 h 状态值,是一个向量!!  h 俗称 hidden_unit
	C(t-1) 前一刻的 C 状态值,也是一个向量!! C 俗称 Cell	
	ht     当前的 h 状态值,是一个向量!!  
	Ct     当前的 C 状态值,也是一个向量!! 
	注意: xt, h(t-1), ht, Ct, C(t-1) 向量维数相同,(一维数组 也称为向量)
	还有,xt是处理过的输入,比如,这代表一个句子中的一个词语,xt不是词语本身,而是对应处理过得到的向量
 	同样,yt是未处理的输入,yt这个向量需要做处理后才得到我们直接观察的结果
	三个门向量:
		输入门: it = sigmod{ (Wxi)(xt) + (Whi)(h(t-1)) }
		忘记门: ft = sigmod{ (Wxf)(xt) + (Whf)(h(t-1)) }
		输出门: ot = sigmod{ (Wxo)(xt) + (Who)(h(t-1)) }
	候选值向量:
		       ~
	               Ct  =  tanh{ (Wxc)(xt) + (Whc)(h(t-1)) }
 	当前的 C 状态值:
		                                     ~
	               Ct  =  ft ⊙  C(t-1)  +  it ⊙  Ct
	当前的 h 状态值:
		       ht  =  ot ⊙  Ct
	注意:⊙ 是 自定义乘法, 这里是按元素乘法 ,例如:门[0,1,1,0,0,1] X 向量[2,3,4,5,6,7] = [0,3,4,0,0,7]
	向量的按元素乘法也叫:Hadamard product (also known as the Schur product
	这里由于门向量的成员基本上不是0,就是1,所以就有所谓开关的意义,
	所以就可以让一部分内容向后传输,阻隔一部分输出,达到长短记忆的作用
	而且也说明为啥 xt, h(t-1), ht, Ct, C(t-1) 向量维数相同
	注意 sigmod 得到的值不是无限接近0就是无限接近1
	注意 tanh   得到的值是 -1到1 之间
	后向传播:(BPTT算法) 参考
		   :https://blog.csdn.net/dark_scope/article/details/47056361
		   :https://blog.csdn.net/u012319493/article/details/52802302
rnn-lstm输入例子图:	
	          ht        h(t+1)        h(t+2)        h(t+3)
	       ___|__    ___|__        ___|__        ___|__
	C(t-1)-|    |-Ct-|    |-C(t+1)-|    |-C(t+2)-|    |-C(t+3)-....
	       |    |    |    |        |    |        |    |
	       |  A |    |  A |        |  A |        |  A |
	       |    |    |    |        |    |        |    |
	h(t-1)-|____|-ht-|____|-h(t+1)-|____|-h(t+2)-|____|-h(t+3)-....
	        |         |             |             |
	        xt       x(t+1)        x(t+2)        x(t+3)
rnn-lstm多测层网络例子图:	
		           .         .              .              .
		           .         .              .              .
		           .         .              .              .
		          h3t      h3(t+1)        h3(t+2)        h3(t+3)
		        __|___     __|___         __|___         __|___
		C3(t-1)-|    |-C3t-|    |-C3(t+1)-|    |-C3(t+2)-|    |-C3(t+3)-....
		        |    |     |    |         |    |         |    |
	第三层	        |  C |     |  C |         |  C |         |  C |
		        |    |     |    |         |    |         |    |
		h3(t-1)-|____|-h3t-|____|-h3(t+1)-|____|-h3(t+2)-|____|-h3(t+3)-....
		          |          |              |              |
		          h2t      h2(t+1)        h2(t+2)        h2(t+3)
		        __|___     __|___         __|___         __|___
		C2(t-1)-|    |-C2t-|    |-C2(t+1)-|    |-C2(t+2)-|    |-C2(t+3)-....
		        |    |     |    |         |    |         |    |
	第二层	        |  B |     |  B |         |  B |         |  B |
		        |    |     |    |         |    |         |    |
		h2(t-1)-|____|-h2t-|____|-h2(t+1)-|____|-h2(t+2)-|____|-h2(t+3)-....
		          |          |              |              |
		          h1t      h1(t+1)        h1(t+2)        h1(t+3)
		        __|___     __|___         __|___         __|___
		C1(t-1)-|    |-C1t-|    |-C1(t+1)-|    |-C1(t+2)-|    |-C1(t+3)-....
		        |    |     |    |         |    |         |    |
	第一层	        |  A |     |  A |         |  A |         |  A |
		        |    |     |    |         |    |         |    |
		h1(t-1)-|____|-h1t-|____|-h1(t+1)-|____|-h1(t+2)-|____|-h1(t+3)-....
		          |          |              |              |
		         xt        x(t+1)         x(t+2)         x(t+3)
一次典型的训练过程:
	例如,每次给 rnn 喂一句话,然后反向传导训练一次网络.的过程
	首先,每句话后假设有 30 个单词,少于30个词语的,也假作有30个单词,剩余用"空白"代替单词位置
	于是可以把句子 分成 30 个steps, 又把每个单词通过 Wx_plus_b 转成 输入向量.
	向量成员数 与 rnn 的 cell/hidden_unit的成员数一样
	假如我们 使用的 是一个 3层lstm 网络,如上图, 那么只有三个lstm单元 A B C
	第1刻  输入第一个单词 向量 xt 到 A单元, 
	       A单元 输出的  h1t     输入到 B单元,
	       B单元 输出的  h2t     输入到 C单元,
	       最后  C单元输出  h3t
	       每个 lstm 单元都需要的 h?(t-1),C?(t-1)可能是最新一次训练的到的值,也可能是第一次训练,所以随机值
	第2刻  输入单词向量  x(t+1)  到A单元, 
	       A单元 输出的  h1(t+1) 输入到 B单元,
	       B单元 输出的  h2(t+1) 输入到 C单元,
	       最后  C单元输出  h3(t+1)
	....
	第30刻 输入单词向量  x(t+29) 到A单元,
	       A单元 输出的 h1(t+29) 输入到B单元,
	       B单元 输出的 h2(t+29) 输入到C单元,
	       最后  C单元输出  h3(t+29)
	根据目的选取结果: h3t,h3(t+1),..h3(t+29) 组成的就是一个回答向量组
		然后再对每个向量执行 另一套 Wx_plus_b 转成 单词	最后组成答句.
		对比本来的答句 得到 误差 然后就可以执行反向传导 
		更新 3个lstm A,B,C,单元的参数,还有输入输出转化矩阵的参数
		特别注意, 还会更新 输入单词 转 输入向量的 Wx_plus_b
			  还会更新 输出h3(t+?) 转 输出单词的 Wx_plus_b
	也就是对于 lstm 3个单元 A,B,C 会连续 喂 30 遍数据,算出最后得到的结果,
	才会执行一次反向传导更新 A,B,C 单元里的内容 !!	
	还有一种情况,就是 只选取 最后一个 h3(t+29) 作为输出
		其他时间点的 h3(t+?) 直接丢弃
		根据 h3(t+29) 经输出转化矩阵得到的结果 与真实结果比较 得到误差
		最后,反向传导,更新 更新 3个lstm A,B,C,单元的参数,还有输入输出转化矩阵的参数

深入分析code9.py:
首先获取 batch 段数据, 每段数据由 TIME_STEPS 个数据单元组成,一次训练 喂 batch段数据
	所以每次输入数据的 self.xs 的 shape 是[(batch, n_steps, 输入数据单元)]
	把每个数据单元 转成 cell_size 维数据单元向量. 
	那么得到的转换后 的 输入数据 self.l_in_y 的 shape 是 [(batch, n_steps, cell_size)]
这里的 RNN 只用一层 lstm, 就是说只有一个 lstm单元
	初始化 lstm 单元:
		lstm_cell = tf.contrib.rnn.BasicLSTMCell(self.cell_size, forget_bias=1.0, state_is_tuple=True)
		BasicLSTMCell参数:
		self.cell_size 就是 C状态值维数,h状态值维数,输入数据单元向量维数,都是同一个数量
		forget_bias=1.0 如果忘记门是有偏移的 ft = sigmod{ (Wxf)(xt) + (Whf)(h(t-1)) + bias }
		这个就是那个偏移值参数
		state_is_tuple=True 意味着 每次 lstm单元 输出的状态值 是 [C状态值,h状态值] 的组合数组  
	初始化 lstm 单元的C状态值 和 h状态值 这里初始化都为零:           
		self.cell_init_state = lstm_cell.zero_state(self.batch_size, dtype=tf.float32)
		self.cell_init_state 模型状态值初始值,因为 state_is_tuple=True 的原因
			shape是 [batch_size,[C状态值维数+h状态值维数=]]
	设置 rnn 的训练过程:
		self.cell_outputs, self.cell_final_state = tf.nn.dynamic_rnn(
            		lstm_cell, self.l_in_y, initial_state=self.cell_init_state, time_major=False)
		这里 rnn 网络 是 单层的 lstm,
		rnn输入数据是 self.l_in_y
		rnn 状态的初始值,这里指的是lstm的状态初始值 self.cell_init_state
		time_major 跟 input数据有关系,其实是跟训练方式有关系:
			当 self.l_in_y的shape是 [(batch, n_steps, cell_size)],time_major=False
			当 self.l_in_y的shape是 [(n_steps, batch, cell_size)],time_major=True
		self.cell_outputs 是训练后得到的输出, shape是 (batch * n_steps, cell_size)
			也就说,每刻(step)喂一数据单元数组到rnn网络, 就有 cell_size维输出向量
			喂完一段数据,就是前后喂完 n_steps , 得到 n_steps个cell_size维输出向量
			喂了 batch 段数据, 就有 batch 个 (n_steps个cell_size维输出向量)
		self.cell_final_state 是得到最后模型状态值, shape与self.cell_init_state的一样
rnn网络输出数据处理:
	self.cell_outputs 的 shape是 (batch * n_steps, cell_size)
	我们要对 cell_size维输出向量 转成 我们处理的输出数据单元
	最后得到输出数据 self.pred , shape为: (batch * steps, 输出数据单元)
误差处理:
	经过训练得到的输出数据 和 真实输出数据 的误差
	    def compute_cost(self):
	        losses = tf.contrib.legacy_seq2seq.sequence_loss_by_example(
	            [tf.reshape(self.pred, [-1], name='reshape_pred')],
	            [tf.reshape(self.ys, [-1], name='reshape_target')],
	            [tf.ones([self.batch_size * self.n_steps], dtype=tf.float32)],
	            average_across_timesteps=True,
	            softmax_loss_function=self.ms_error,
	            name='losses'
	        )
	        with tf.name_scope('average_cost'):
	            self.cost = tf.div(
	                tf.reduce_sum(losses, name='losses_sum'),
	                self.batch_size,
	                name='average_cost')
	            tf.summary.scalar('cost', self.cost)
	    @staticmethod
	    def ms_error(labels, logits):
	        return tf.square(tf.subtract(labels, logits))
	tf.contrib.legacy_seq2seq.sequence_loss_by_example 是计算误差的一种方法,
	现在仅仅就具体例子分析,未打算系统说明这个函数的内容:
		参考:https://tensorflow.google.cn/api_docs/python/tf/contrib/seq2seq/sequence_loss
		    :https://blog.csdn.net/liuchonge/article/details/71424432
		第一个参数 logits, 一般是shape为 [batch,nsteps]
			特别要讨论下这里的shape:
			譬如,输入本来的shape 是 [batch, steps, 数据单元向量]:[50, 30, 20],
			必须reshape成二维数组: [batch, steps*数据单元向量]:[50, 600],才能作为logits输入,
			这时的 nsteps 就是 600 了!!
			现在,我们输入的 self.pred 的shape 是 (batch * steps, 输出数据单元):[50*20,1]
			对应着 logits 的 shape [batch,nsteps]: [50*20,1]
			而我们先把 self.pred reshape 成 一维数组 [batch]:[50*20*1] 再输入到 logits
			显然 logits 把输入的 self.pred:[batch]:[50*20*1]看成self.pred:[batch,nsteps]:[50*20,1]
			也就说 一维数组 [batch], 和二维数组 [batch,1] 并没有区别
		第一个参数 targets, 一般是shape为 [batch,nsteps]
			同样我们先把 self.ys reshape 成 一维数组 [batch]:[50*20*1] 再输入到 targets
			即,显然 targets 把输入的 self.ys:[batch]:[50*20*1]看成self.ys:[batch,nsteps]:[50*20,1]
		第三参数 权重 Weight 表示要对不同loss,的重视程度: 一般是shape为 [batch,nsteps]
			这里要求每个loss重视程度都一样,所以 weights 都为1
		第四参数:average_across_timesteps=True,表示求 timesteps 平均,后续解释
		第五参数:average_across_batch=True,表示求 batch 平均,注意:这里没有填入,没有使用,所以默认False后续解释
		第六参数:softmax_loss_function=None,如果使用默认loss单元函数,填None
		第七参数:name=None给这个 定义的计算误差的方法 命名,也可以不命名,为none
			图例解释: logits,target,weights 的shape是一样的!!!
			假如
			self.pred:[batch,n_step,数据单元向量]:[2,3,4]
				|[p111,p112,p113,p114]| ,|[p211,p212,p213,p214]|
				|[p121,p122,p123,p124]|  |[p221,p222,p223,p224]|			
				|[p131,p132,p133,p134]|  |[p231,p232,p233,p234]|
			self.ys:[batch,n_step,数据单元向量] 也一定是 [2,3,4]
				|[y111,y112,y113,y114]| ,|[y211,y212,y213,y214]|
				|[y121,y122,y123,y124]|  |[y221,y222,y223,y224]|			
				|[y131,y132,y133,y134]|  |[y231,y232,y233,y234]|
			假如被reshape成以下样子:
			self.pred 成 logits:[batch,nsteps]:[2,3*4]
				|p111,p112,p113,p114,p121,p122,p123,p124,p131,p132,p133,p134|
				,
 				|p211,p212,p213,p214,p221,p222,p223,p224,p231,p232,p233,p234|
			self.ys 成 targets:[batch,nsteps]:[2,3*4]
				|y111,y112,y113,y114,y121,y122,y123,y124,y131,y132,y133,y134| 
				,
				|y211,y212,y213,y214,y221,y222,y223,y224,y231,y232,y233,y234|
			那么我们要求 weight 也是这样子:
				|w11,w12,w13,w14,w15,w16,w17,w18,w19,w110,w111,w112|
				,	
				|w21,w22,w23,w24,w25,w26,w27,w28,w29,w210,w211,w212|
			如果 softmax_loss_function=None ,就使用default-loss单元函数,
				如果不想使用 default-loss单元函数,就得设置 softmax_loss_function
				假如我们使用的误差是 均方差:mean squared error(MSE), 
				有必要说明 均方误差 MSE = E( (y-y')^2 ) =  ( (y1-y'1)^2 + .. + (yn-y'n)^2 ) /n
				我们设置的 softmax_loss_function = ms_error(labels, logits) 
				而函数 ms_error(labels, logits) 只实现了 差的平方 (y-y')^2,
				即只有 tf.square(tf.subtract(labels, logits))
				而E()部分,"即(..+..+..+..)/n)" 并不在 ms_error 里实现.
				而是 tf.contrib.legacy_seq2seq.sequence_loss_by_example 透过
				average_across_batch 或者 average_across_timesteps 条件实现
				一般 average_across_timesteps,  average_across_batch 只能选其中一个为True
			假如 average_across_timesteps=True 表示平均 nsteps 这维
			那么:
			这个定义了的误差方法得到的 结果
			A =  ( w11*(p111-y111)^2 + w12*(p112-y112)^2 + ... + w112*(p134-y134)^2 ) / (3*4)
			B =  ( w21*(p211-y211)^2 + w22*(p212-y212)^2 + ... + w212*(p234,y234)^2 ) / (3*4)
			当weights数组的值 w?? 都为 1 时 , 表示所有 差的平方 都重视
			这样就实现了在 nsteps维上 的 均方差 MSE = E( (y-y')^2 )
			当weights数组的值 w?? 不一样时 , 表示部分 差的平方 得到重视 
			这样就实现了在 nsteps维上 的 配置了权重的均方差 MSE = E( (y-y')^2 )
			最后得到 一个 二维向量(一维数组) [A,B]
			假如 average_across_batch=True 表示平均 batch 这维
			那么: 
			这个定义了的误差方法得到的 结果
			a =  ( alo(p111,y111)*w11 + alo(p211,y211)*w21 ) / 2
			b =  ( alo(p112,y112)*w12 + alo(p212,y212)*w22 ) / 2
			c =  ( alo(p113,y113)*w13 + alo(p213,y213)*w23 ) / 2
			b =  ( alo(p114,y114)*w14 + alo(p214,y214)*w24 ) / 2
			....
			l =  ( alo(p134,y134)*w112 + alo(p234,y234)*w212 ) / 2
			当weights数组的值 w?? 都为 1 时 , 表示所有 差的平方 都重视
			这样就实现了在 batch维上 的 均方差 MSE = E( (y-y')^2 )
			当weights数组的值 w?? 不一样时 , 表示部分 差的平方 得到重视 
			这样就实现了在 batch维上 的 配置了权重的均方差 MSE = E( w(y-y')^2 )
			最后得到 一个 十二维向量(一维数组) [a,b,c,d,e,f,g,h,i,j,k,l]
	回到 losses = tf.contrib.legacy_seq2seq.sequence_loss_by_example()
		self.pred:[batch]:[50*20*1] -> logits :[batch,nsteps]:[50*20*1,1]
		self.ys  :[batch]:[50*20*1] -> targets:[batch,nsteps]:[50*20*1,1]
		weights  :[batch]:[50*20*1] -> weights:[batch,nsteps]:[50*20*1,1]
		即 |p1,p2,p3,,,,p100|
		   |y1,y2,y3,,,,y100|
		   |w1,w2,w3,,,,w100|
		而且 w?? 的值都为 1
		我们斌不打算使用 tf.contrib.legacy_seq2seq.sequence_loss_by_example 默认方式求误差,
		同时 我们通过 均方差的方式 求出,误差, 并不是 交叉熵的方式,
		所以我们设置 softmax_loss_function=self.ms_error,只实现了 差的平方 (y-y')^2,
		我们设置了 average_across_timesteps=True,平均 nsteps 这维
		因为 nsteps 为 1,即只有一个成员
		于是,这个定义了的误差方法得到的 结果
		l1 = ( (p1-y1)^2 ) / 1
		l2 = ( (p2-y2)^2 ) / 1
		l3 = ( (p3-y3)^2 ) / 1
		...
		l1000 = ( (p100-y100)^2 ) / 1
		即得到一个 100维向量 [l1,l2,l3,,,l100]
		tf.contrib.legacy_seq2seq.sequence_loss_by_example 输出的 losses:[l1..l1000],
		是针对这个函数意义上的batch:1000 ,
		实际数据只有 50个batch, 每个batch有20个step,
		我们使用这个函数就预处理为把每个step都当成batch处理
	计算cost:  self.cost = tf.div(
	                tf.reduce_sum(losses, name='losses_sum'),
	                self.batch_size,
	                name='average_cost')
	            tf.summary.scalar('cost', self.cost)
		tf.reduce_sum: 把losses的1000个成员都加起来,
		tf.div: 然后除以 self.batch_size:50,
		最后得到的就是 平均 cost ,是一个值, 
		相当于每个 batch 的 20个Time_steps的loss加起来得到一个cost
		把50个cost加起来,再平均!!!
最后分析训练过程!!
	第一次:	feed_dict = {
	                    model.xs: seq,
	                    model.ys: res,
	                    # create initial state
	            }
		_, cost, state, pred = sess.run(
        	    [model.train_op, model.cost, model.cell_final_state, model.pred],
        	    feed_dict=feed_dict)
		先执行model.train_op,
			执行 tf.train.AdamOptimizer(LR).minimize(self.cost)
			需要 self.cost,
			所以得先执行 tf.div(.. losses, ,,self.batch_size),
			需要 losses 和 self.batch_size, self.batch_size已知,losses,未知
			所以得先执行 tf.contrib.legacy_seq2seq.sequence_loss_by_example
			需要 self.pred, self.ys, self.batch_size, self.n_steps	 
			self.batch_size, self.n_steps 已知,
			self.ys 是占位符号: 传入了 model.ys: res
			要取得self.pred,所以得先执行tf.matmul,
			需要 l_out_x, Ws_out, bs_out
			Ws_out, bs_out 是 被定义为要 训练 variable
			要取得 l_out_x,即取得 self.cell_outputs,
			所以得先执行 tf.nn.dynamic_rnn
			需要 lstm_cell, self.l_in_y, self.cell_init_state,
			lstm_cell, self.cell_init_state, 已知
			要取得 self.l_in_y, 即取得 l_in_x ,即取得 self.xs
			self.xs 是占位符号: 传入了 model.xs: seq
			最后,再最后一步步往前执行,
			回到 tf.train.AdamOptimizer(LR).minimize(self.cost) 执行反向传导更新参数
			最后执行完毕,返回 数据 放入 _
		然后执行model.cost,
			得先执行 tf.div(.. losses, ,,self.batch_size),
			需要 losses 和 self.batch_size, self.batch_size已知,losses,未知
			所以得先执行 tf.contrib.legacy_seq2seq.sequence_loss_by_example
			需要 self.pred, self.ys, self.batch_size, self.n_steps	 
			self.batch_size, self.n_steps 已知,
			self.ys 是占位符号: 传入了 model.ys: res
			要取得self.pred,所以得先执行tf.matmul,
			需要 l_out_x, Ws_out, bs_out
			Ws_out, bs_out 是 被定义为要 训练 variable
			要取得 l_out_x,即取得 self.cell_outputs,
			所以得先执行 tf.nn.dynamic_rnn
			需要 lstm_cell, self.l_in_y, self.cell_init_state,
			lstm_cell, self.cell_init_state, 已知
			要取得 self.l_in_y, 即取得 l_in_x ,即取得 self.xs
			self.xs 是占位符号: 传入了 model.xs: seq
			最后,再最后一步步往前执行,直到完毕,返回 数据 放入 cost
		然后执行 model.cell_final_state,
			得先执行 tf.nn.dynamic_rnn
			需要 lstm_cell, self.l_in_y, self.cell_init_state,
			lstm_cell, self.cell_init_state, 已知
			要取得 self.l_in_y, 即取得 l_in_x ,即取得 self.xs
			self.xs 是占位符号: 传入了 model.xs: seq
			最后,再最后一步步往前执行,
			tf.nn.dynamic_rnn 会得到 self.cell_outputs, self.cell_final_state
			但是 sess.run 只想得到 self.cell_final_state
			完毕后只返回 self.cell_final_state 数据 放入 state
		最后执行 model.pred
			得先执行tf.matmul,
			需要 l_out_x, Ws_out, bs_out
			Ws_out, bs_out 是 被定义为要 训练 variable
			要取得 l_out_x,即取得 self.cell_outputs,
			所以得先执行 tf.nn.dynamic_rnn
			需要 lstm_cell, self.l_in_y, self.cell_init_state,
			lstm_cell, self.cell_init_state, 已知
			要取得 self.l_in_y, 即取得 l_in_x ,即取得 self.xs
			self.xs 是占位符号: 传入了 model.xs: seq
			最后,再最后一步步往前执行,直到完毕,返回 数据 放入 pred
	第一次以后的:feed_dict = {
	                model.xs: seq,
	                model.ys: res,
	                model.cell_init_state: state    # use last state as the initial state for this run
	            }
		_, cost, state, pred = sess.run(
        	    [model.train_op, model.cost, model.cell_final_state, model.pred],
        	    feed_dict=feed_dict)
		这时 特别注意到 model.cell_init_state: state
		因为,model.cell_init_state 不像 xs,ys 被定义为 placeholder 占位符号, 
		也不像 weight, bias,那种 被定义为 要被训练的 variable
		初始化时,是这样子的
			self.cell_init_state = lstm_cell.zero_state(self.batch_size, dtype=tf.float32)
		应该像 self.cell_size 那样的变量, model里内部初始化,内部赋值的
		但现在, model.cell_init_state: state 的操作,
		相当于, self.cell_init_state  被外部赋值了, 有占位符号那样的作用.
		也就说模型里 非 variable 和 placeholder 变量, 其实也有 placeholder的作用
		可以 ,被外部赋值,取代原有值 !!

深入分析code10.py
	和code9.py的区别是, 输入的是一张28*28图片
	一张图片 分成 28 条数据,即分28次喂入模型, 每条数据 28个像素点
	每次喂 1 条数据, 每条数据 转换成 128维向量 再喂入单层 lstm 模型
	只取 最后一次,即第28次喂入数据后 得到的 128维输出向量
	再 转化 得到 10维向量 与 图片原本指向的 10维向量做比较
cell 状态向量 和hidden_unit 状态向量 初始值问题:
	由于每张图片输入rnn 模型过程中,都与另一张输入不一样
	所以前一张的图片训练得到 的 cell 状态向量 和hidden_unit 状态向量
	不需要传导到 下一张图片的识别,所以每次训练一张图片用到的 
		 cell 状态向量 和hidden_unit 状态向量的初始值都为 0 !!
只获取第28次喂入数据后 得到的 128维输出向量 做比较的问题 
	这里的重点是 如何取第28次得到 128维输出向量:
    	outputs, final_state = tf.nn.dynamic_rnn(cell, X_in, initial_state=init_state, time_major=False)
	输出的 outputs 的shape [batch,steps,输出向量维数]:[128,28,128]
		即包含 128张图片 同时喂入模型 得到的128个结果,
		每个结果有28条 128维输出向量, 
		每条输出向量 对应 每次(step) 喂入的一个输入向量
		现在我们要 每个结果 的 最后一条 128维输出向量, 即第28次喂入输出向量 得到的 128维输出向量
		outputs = tf.unstack(tf.transpose(outputs, [1,0,2]))
		outputs 是一个 三维数组 即shape [batch,steps,输出向量维数]
		batch 这一维的 标记为0,
		steps 这一维的 标记为1,
		输出向量维数 这一维的 标记为2,
		tf.transpose(outputs, [1,0,2]:表示把 batch steps 两维转换
		即outputs 变成了 [steps,batch,输出向量维数]
		图例直观解析
			假如 outputs本来是这样的 shape:[3,2,5]:
			[
	
				[
					[1,1,1,1,1,1]
					[2,2,2,2,2,2]		
				]
				,
	
				[
					[3,3,3,3,3,3]
					[4,4,4,4,4,4]		
				]
				,
	
				[
					[5,5,5,5,5,5]
					[6,6,6,6,6,6]		
				]
				,
			]
			transpose(outputs, [1,0,2] -> outputs shape:[2,3,5]:
			只换第 batch steps 维, 
			输出向量维数 这维内容不变, 这维可以简单标记:
				A = [1,1,1,1,1,1]
				B = [3,3,3,3,3,3]
				C = [5,5,5,5,5,5]
				D = [2,2,2,2,2,2]		
				E = [4,4,4,4,4,4]		
				F = [6,6,6,6,6,6]
	    		outputs 简单记为:
			[[A,B]
			 [C,D]
			 [E,F]]
			换第 batch steps 后:
			[[A,C,E]
			 [B,D,F]]
			把标记 ABCDEF换换回去就得:
			[
	
				[
					[1,1,1,1,1,1]
					[3,3,3,3,3,3]
					[5,5,5,5,5,5]
	
				]
				,
	
				[
					[2,2,2,2,2,2]		
					[4,4,4,4,4,4]		
					[6,6,6,6,6,6]		
				]
				,
			]		
	
		解构数组:tf.unstack()
		没有其他参数,默认解构最前的一维,即 steps维,即第0维
		解构图示:
			假如 outputs是这样子的:
			outputs = [
		
					[
						[1,1,1,1,1,1]
						[2,2,2,2,2,2]		
					]
					,
	
					[
						[3,3,3,3,3,3]
						[4,4,4,4,4,4]		
					]
					,
		
					[
						[5,5,5,5,5,5]
						[6,6,6,6,6,6]		
					]
	
				]
			tf.unstack(outputs) 后,得到
			outputs[0] = [ [1,1,1,1,1,1]
				       [2,2,2,2,2,2] ]
			outputs[1] = [ [3,3,3,3,3,3]
				       [4,4,4,4,4,4] ]
			outputs[2] = [ [5,5,5,5,5,5]
				       [6,6,6,6,6,6] ]
			output 从 变量 变成了 数组变量 !!
			另外 outputs[-1] 等于 outputs[2], outputs[-1]表示数组的变量的最后一个成员!!
		因此 真正outputs tf.transpose转换后 得 shape:[steps,batch,输出向量维数]
		说明最后一steps 的 [batch,输出向量维数]:[128,128]
		放着是 128 条 第28次喂入输出向量 得到的 128维输出向量 
		我们只要最后一steps 的 [batch,输出向量维数]:[128,128] 
		所以执行了 tf.unstack 解构,
		outputs变成了 数组变量,我们只有这个数组变量最后一个,即 outputs[-1]
		results = tf.matmul(outputs[-1], weights['out']) + biases['out']    # shape = (128, 10)
		然后 每条最后的128维输出向量 经过同一个 [128,10]的转换矩阵 得到 一条 10维one-hot向量
		得到的 results 是含有 128 条 10维one-hot向量,
		即128张图片 经网络后得到 128条  one-hot向量
		最后对比 one-hot向量 与图片实际的 标记向量, 得到误差,然后反向传导更新网络参数
	由于我们只需要 最后一条 128维输出向量, 即第28次喂入输出向量 得到的 128维输出向量
	而这个内容 跟 final_state 这个数组变量的 第2个成员 final_state[1] 是一样的!
	results = tf.matmul(final_state[1], weights['out']) + biases['out']
	同样得到含有 128 条 10维one-hot向量, 的 results
final_state 这个内容 是lstm训练后得到的 c状态值 和 h状态值!!  	
	final_state[0] 是 c状态值
	final_state[1] 是 h状态值,也等于当前的输出向量	

		







20180415
讨论无监督学习问题:
并没有什么特别难的内容
图片 128维向量 得到一个 128维向量 (称为编码过程)
128维向量 128维向量 自己组成一幅图片 (称为解码码过程)
图片本身,与网络生成的图片对比,得到 误差,
通过误差,反向传导,训练更新编码网络和解码网络








20180415
batch nomalizeion (BN)
与 优化器 和 激活函数 的概念都不一样!
有效 避免 梯度消失爆炸的问题.
有效 加速迭代,减少训练次数,减少计算负担,提高效率
跟 白化 这样的预处理 差不多,但意义性质又不太一样!!
https://blog.csdn.net/whitesilence/article/details/75667002
https://blog.csdn.net/intelligence1994/article/details/53888270
https://blog.csdn.net/happynear/article/details/44238541
http://ufldl.stanford.edu/wiki/index.php/%E7%99%BD%E5%8C%96

BN跟学习过的 向量正规化,归一化 矩阵归一化什么的,没有任何数学关系,或者借鉴意义.
假想我们有一 batch 样本,他们很多时都不可避免可能会有些比较固执的地方,
比如,某段时间,样本值,不是无限接近A,就是无限接近B,
然而 我们知道真实情况 样本值是平均分布在 A-Z 之间的.
那么这样导致的结果就是,这段时间内的多次模型学习训练,梯度收敛都特别小,甚至没有收敛. 
这种情况下, 我们使用BN 就是使这些样本的近似部分弱化,差异部分放大.
这样子,学习训练时,梯度下降特别快,模型的学习效率提高!!
BN操作,不仅仅在数据开始时操作,而且也要在 模型层里加入处理
输入数据的BN处理!!
	一个batch有 m 个样本向量 x1,x2,...xm
	             m
	uB = (1/m) * ∑ xi	//求均值
		    i=1

	             m
	oB = (1/m) * ∑ (xi-uB)^2  //求均方差
		    i=1

	      	 xi-uB
	xi' = ____________	  //标准化, e是一个很小的定值, 比如0.001, 是为了防止分母为0的情况
	      √(oB^2 + e )

		
	yi = Y * xi' + B	//反标准化, Y是scalc, B是shift, Y和B是要学习的参数,
				//这里的作用是, 配置标准化的程度
	最后大得到的 y1,y2,,,ym 就是 BN 后得到的 样本,
	简记为 y = BN(X)
学习模型層的BN处理!!
	我们知道数据经过一层处理后得到的数据,作为下一层的输出例如:
		某层的输入 为 x, 经过一层神经元处理,得到输出 wx_plus_b
		而 wx_plus_b 会经过激活函数例如relu处理后得到 relu(wx_plus_b)
		relu(wx_plus_b) 就是下一层的输入!!
	而添加BN处理时,一般是这样的:
		某层的输入 为 x, 经过一层神经元处理,得到输出 wx_plus_b
		输出先做BN处理得 BN(wx_plus_b)
		然后 BN(wx_plus_b) 经过激活函数例如relu处理后得到 relu(BN(wx_plus_b))
		relu(BN(wx_plus_b)) 就是下一层的输入!!
	注意,如果未先BN处理, 最后得到的 relu 后 内容可能大部分分布在小于0的地方,而直接被等于0处理
		这样就没有更多有效信息传到下一层,而导致模型接下来的模型层没有明显梯度传导
		最终传导的学习梯度下降缓慢,
		BN处理后,最后得到的 relu 后 内容,会相对减少分布在小于0的地方,
		可以把更多有用信息传到下一层.最终传导的加速学习梯度下降,
	更直观的来说,经过 BN 处理后的内容被缩小到处于 [-1,1] 之间, 
		这样, 再经过激活函数得到的内容 大多数据 不会处于 几乎或过分饱和值域!!
		这样就有充分信息 向下一级传导!!
		有效避免梯度爆炸和消失
解析了这么多,但仍然觉得并不了解 BN 的实质!!,日后再深化吧

code12.py
	np.random.seed(1)	//用来产生相同的随机数,生成随机数操作前,都seed(同一个数),生成的随机数相同
	这个例子的样品是 2次曲线 散布点,
	学习散布点分布规律
通过 code12.py
深化认识 python 的 一些语法原理 特别是函数运算空间
	认识 python 的全局变量和 局部变量 , 局部函数引入全局变量要加 global
	每执行一次 局部函数 都是重新建立一个空间存储 当前局部环境和变量
函数执行环境分析
	train_op, cost, layers_inputs = built_net(xs, ys, norm=False)   # without BN 
	train_op_norm, cost_norm, layers_inputs_norm = built_net(xs, ys, norm=True) # with BN
	这时,其实是建立了两个分别不一样的 built_net 函数环境
A环境:
train_op, cost, layers_inputs: 
___________________________________________________
|build_net (A)                                	  |					
|                                                 |
|  train_op: 	//仅属于这里的train_op            |
|  _____________________________                  |
|  |Gradient.minimize (A)      |                  |
|  |                           |                  |
|  |___________________________|                  |
|                                                 |
|    ^                                            |
|    |                                            |
|  cost:	//仅属于这里的cost                |
|  _____________________________                  |
|  |reduce_mean (A)            |                  |
|  |                           |                  |
|  |___________________________|                  |
|                                                 |
|    ^                                            |
|    |                                            |
|  prediction:     //仅属于这里的prediction       |
|  __________________________________________     |
|  |add_layer  (A,n+1)                      |     |
|  |    这个add_layer操作 自己的的局部变量有 |     |
|  |    weights,biases                      |     |
|  |    Wx_plus_b,                          |     |
|  |    outputs                             |     |
|  |________________________________________|     |
|                                                 |
|    ^                                            |
|    |                                            |
|  layers_inputs[n]:     //仅属于这里的prediction |
|  __________________________________________     |
|  |add_layer  (A,n)                        |     |
|  |    这个add_layer操作 自己的的局部变量有 |     |
|  |    weights,biases                      |     |
|  |    Wx_plus_b,                          |     |
|  |    outputs                             |     |
|  |________________________________________|     |
|                                                 |
|    ^                                            |
|    |                                            |
|  layers_inputs[n-1]:   //仅属于这里的prediction |
|  __________________________________________     |
|  |add_layer  (A,n-1)                      |     |
|  |    这个add_layer操作 自己的的局部变量有 |     |
|  |    weights,biases                      |     |
|  |    Wx_plus_b,                          |     |
|  |    outputs                             |     |
|  |________________________________________|     |
|                                                 |
|  ..........                                     |   
|                                                 |
|  layers_inputs[0]:     //仅属于这里的prediction |
|  __________________________________________
|  |add_layer  (A,0)                        |     |
|  |    这个add_layer操作 自己的的局部变量有 |     |
|  |    weights,biases                      |     |
|  |    Wx_plus_b,                          |     |
|  |    outputs                             |     |
|  |________________________________________|     |
|                                                 |
|    ^                                            |
|    |                                            |
|    xs                                           |
|_________________________________________________|

B环境:
train_op, cost, layers_inputs: 
___________________________________________________
|build_net (A)                                	  |					
|                                                 |
|  train_op: 	//仅属于这里的train_op            |
|  _____________________________                  |
|  |Gradient.minimize (A)      |                  |
|  |                           |                  |
|  |___________________________|                  |
|                                                 |
|    ^                                            |
|    |                                            |
|  cost:	//仅属于这里的cost                |
|  _____________________________                  |
|  |reduce_mean (A)            |                  |
|  |                           |                  |
|  |___________________________|                  |
|                                                 |
|    ^                                            |
|    |                                            |
|  prediction:     //仅属于这里的prediction       |
|  __________________________________________     |
|  |add_layer  (A,n+1)                      |     |
|  |    这个add_layer操作 自己的的局部变量有 |     |
|  |    weights,biases                      |     |
|  |    Wx_plus_b,                          |     |
|  |    Wx_plus_b = f(Wx_plus_b)            |     |
|  |    outputs                             |     |
|  |________________________________________|     |
|                                                 |
|    ^                                            |
|    |                                            |
|  layers_inputs[n]:     //仅属于这里的prediction |
|  __________________________________________     |
|  |add_layer  (A,n)                        |     |
|  |    这个add_layer操作 自己的的局部变量有 |     |
|  |    weights,biases                      |     |
|  |    Wx_plus_b,                          |     |
|  |    Wx_plus_b = f(Wx_plus_b)            |     |
|  |    outputs                             |     |
|  |________________________________________|     |
|                                                 |
|    ^                                            |
|    |                                            |
|  layers_inputs[n-1]:   //仅属于这里的prediction |
|  __________________________________________     |
|  |add_layer  (A,n-1)                      |     |
|  |    这个add_layer操作 自己的的局部变量有 |     |
|  |    weights,biases                      |     |
|  |    Wx_plus_b,                          |     |
|  |    Wx_plus_b = f(Wx_plus_b)            |     |
|  |    outputs                             |     |
|  |________________________________________|     |
|                                                 |
|  ..........                                     |   
|                                                 |
|  layers_inputs[0]:     //仅属于这里的prediction |
|  __________________________________________
|  |add_layer  (A,0)                        |     |
|  |    这个add_layer操作 自己的的局部变量有 |     |
|  |    weights,biases                      |     |
|  |    Wx_plus_b,                          |     |
|  |    Wx_plus_b = f(Wx_plus_b)            |     |
|  |    outputs                             |     |
|  |________________________________________|     |
|                                                 |
|    ^                                            |
|    |                                            |
|    b(xs)  //b()是做了batch-noralization (bN)    |
|    ^      //f()是先BN,最做激活函数处理           |
|    |                                            |
|    xs                                           |
|_________________________________________________|

	然后sess.run 的使用,就是重新利用这些已存在的函数空间,
	并不是再重新建立新空间!!
	虽然for循环 调用了多个 add_layer,
	但是,每一个 add_layer 都不一样.都有自己的weights等变量
	然而比较困惑在于, 每个 不一样的 add_layer 都把结果都给同一个 output变量
	那么gradient-minimize时,就追溯到 output 时 就不知道会发生什么事情.
	还有一个是 Wx_plus_b = f(Wx_plus_b) 的问题
	同样追溯到 Wx_plus_b 时, 又不知道要如何反向传导下去了,
	因为他们都请求自己,不过是请求之前的自己
	反向传导时应该会分析到 同个Wx_plus_b, output 变量被赋值的先后次序吧.
以上是值得思考的代码细节问题!!
	但先不再深究了








20180421
开始学习 强化学习!
学习分析 Q-learning 算法!!
	以地图游戏,寻找出口例子分析:
		先建立 一张 q 表, 记录每一位置点 s 的 所有行为 a 的权值 例如:
		       left     right        up      down
		0  0.000000  0.004320  0.000000  0.004320
		1  0.000000  0.025005  0.000400  0.004320
		2  0.000030  0.111241  0.000002  0.004320
		3  0.000000  0.368750  0.000000  0.004320
		4  0.027621  0.745813  0.000099  0.004320
		5  0.000000  0.000000  0.000000  0.004320
		Q(s,a)函数是得出 s 点,a 行为的 的 权值,
		例如 Q(4,right) = 0.745813, right:0.745813比其他行为的权值高,
		所以 在 4 点 采取 right行为 是最为推荐的选择.
		每个学习回合(episode)初,先确定好 q 表, 
		如果是开始回合,就初始化q表,否则沿用上回合最终更新的q表
	每个学习回合开的每一步工作内容如下:
		在当前点 s , 通过行为选取算法选取 一个行为 a
			其中 有 e_greedy 概率通过q表权值操作来选取 行为 a, 一般选最大权值的行为
			有 1-e_greedy 概率 随机选取行为!!
		执行操作 a , 根据操作到达下个点 s_, 并根据奖励算法得到行为应有的奖励 r
		询问q表得到 下个点 s_ 的 最大权值的行为 max_a_
		取得 Q(s_,max_a_) 为我们的参考权值
		执行学习更新 当前点s,当前行为a的 权值 Q(s,a):
			Q(s,a) = Q(s,a) + Alpha * [ r + gama * Q(s_,max_a_) - Q(s,a) ]
			Alpha是学习率
			GAMMA 是 discount factor
	重复一步又一步,直到第某步,执行操作 a 到达的下一点 s_ 是目标点,或者是陷阱点
		时,才正式结束结束这一回合.准备开始下一回合.				
	这里 奖励算法和行为选取算法都是自定义的,需要根据实际情况谨慎设计
	这里有必要详细讨论学习原理:
		假如当前 s:3 选出的 a:up 是 s 里最大权值行为, 
		我们也知道 a:up 后 到达的 S_:1 的最大权值行为是 max_a_:left
		如果 r + gama * Q(s_,max_a_) - Q(s,a) 很小,
		意味着 s:3 执行 a:up 后到 s_:1 然后执行 max_a_:left 是当前认为最正确操作流程
		如果 r + gama * Q(s_,max_a_) - Q(s,a) 很大时,
		s:3 执行 a:up 后到 s_:1 然后执行 max_a_:left 这串操作流并不合适
		于是, 把 r + gama * Q(s_,max_a_) - Q(s,a) 反馈更新 s:3 中 a:up 的权值,
		使得以后有机会重新回到 s:3 时,选出行为 a:up 的概率变小,
	逆向思考学习原理:
		假如有一个充分学习后的模型.q值表是这样的:
		       left     right        up      down
		0  0.999999  0.004320  0.000000  0.004320
		1  0.000000  0.025005  0.000400  0.004320
		2  0.000030  0.999999  0.000002  0.004320
		3  0.000000  0.368750  0.000000  0.004320
		4  0.027621  0.745813  0.999999  0.004320
		5  0.000000  0.000000  0.000000  0.004320
		6  0.000000  0.999999  0.000000  0.004320
		7  0.000000  0.025005  0.000400  0.004320
		8  0.000030  0.111241  0.999999  0.004320
		9  0.000000  0.000000  0.000000  0.000000
		10 0.027621  0.745813  0.000099  0.004320
		11 0.033030  0.000000  0.564363  0.004320
		先从地点 0 开始:
			0:left -> 4:up -> 2:right -> 6:right -> 8:up -> 9终点
			以后的回会没有意外都会沿着这条路径跑
		因为 s:0:a:left -> s_4:max_a_:up 的  r + gama * Q(s_,max_a_) - Q(s,a) 很小
		Q(0,left)就几乎变化,所以下次到达地点0时,还会很大机会选择left行为到 地点4
		同样道理, Q(4,up),Q(2,right),Q(6,right),Q(8,right) 都几乎不变化
		所以 0:left -> 4:up -> 2:right -> 6:right -> 8:up -> 9终点 
		就是一条当前最为正确 操作流
	Q-learning 也称 off-policy, 是因为选取 s_ 参考权值行为只选取最大权值的,
		因为只去看 s_ 哪个行为权值最大 ,就选他 (max_a_) 
		意味着,下一步最有可能执行这个行为
		下一步执行这个最有可能的行为 max_a_ 后,
		并无法知道 会在下下步到达 的地方 S__ 会不会是陷阱或禁止区域什么的
		这样子,前进就比较冒进,勇敢, 被认为是缺乏策略性的学习行为
		
学习分析 Sarsa 算法!!
	同样以地图游戏,寻找出口例子分析:
		先建立 一张 q 表, 记录每一位置点 s 的 所有行为 a 的权值 例如:
		       left     right        up      down
		0  0.000000  0.004320  0.000000  0.004320
		1  0.000000  0.025005  0.000400  0.004320
		2  0.000030  0.111241  0.000002  0.004320
		3  0.000000  0.368750  0.000000  0.004320
		4  0.027621  0.745813  0.000099  0.004320
		5  0.000000  0.000000  0.000000  0.004320
		Q(s,a)函数是得出 s 点,a 行为的 的 权值,
		例如 Q(4,right) = 0.745813, right:0.745813比其他行为的权值高,
		所以 在 4 点 采取 right行为 是最为推荐的选择.
		每个学习回合(episode)初,先确定好 q 表, 
		如果是开始回合,就初始化q表,否则沿用上回合最终更新的q表
	每个学习回合开的每一步工作内容如下:
		当前点 s , 有一个确认的行为 a
			如果不是执行在第一个回合 当前点s 是上一步的 s_, 
			行为 a 是上一步 s_ 在上一步通过 行为选取算法 得到的 a_
			如果是执行在第一个回合 当前点 s , 通过行为选取算法得到行为 a 
			行为选取算法典型有以下操作:
			有 e_greedy 概率通过q表权值操作来选取 行为 a_, 一般选最大权值的行为
			也有 1-e_greedy 概率 随机选取行为 a_!!
		执行操作 a , 根据操作会到达下个点 s_, 并根据奖励算法得到行为应有的奖励 r
		Q-learning 和 sarsa 算法 区别在于 参考权值 选取方式不一样
		再一次通过 行为选取算法 给S_ 选取 一个行为 a_
		取得 Q(s_,a_) 为我们的参考权值
		还有的是,下一步的这个行为a_,在下一步 s_ 点 一定会执行的!! 
		执行学习更新 当前点s,当前行为a的 权值 Q(s,a):
			Q(s,a) = Q(s,a) + Alpha * [ r + gama * Q(s_,a_) - Q(s,a) ]
			Alpha是学习率
			gama 是 discount factor
	重复一步又一步,直到第某步,执行操作 a 到达的下一点 s_ 是目标点,或者是陷阱点
		时,才正式结束结束这一回合.准备开始下一回合.
		然而一般可以很好地避开陷阱点.				
	这里 奖励算法和行为选取算法都是自定义的,需要根据实际情况谨慎设计
	Sarsa 也称 on-policy, 因为是通过 行为选取算法 选取 s_ 的 下一步行为 a_作为参考权值行为
		下一步一定会执行这个行为
		下一步一定会执行的 a_ 是通过 行为选取算法选出的,
		必须强调的是,s与s_使用相同,行为选取算法,而这个算法的设计还是有一定讲究的
		开始好几个回合, s 选取的 a 或者是 s_ 选取的 a_ 都有可能使下一步掉入陷阱区,
		多次回合后,q表有初步的更新,然后行为选取算法选出的行为 一般都是能避开陷阱区的行为
		所以,多个回合后,后面学习 执行的行为 一般都可以很好避开陷阱,比较谨慎的前进.
		就好象会察觉到危险而不会贸然跌入陷阱那样有策略地前进.

学习分析 Sarsa(lamda) 算法!!
	同样以地图游戏,寻找出口例子分析:
		先建立 一张 q 表, 记录每一位置点 s 的 所有行为 a 的权值 例如:
		       left     right        up      down
		0  0.000000  0.004320  0.000000  0.004320
		1  0.000000  0.025005  0.000400  0.004320
		2  0.000030  0.111241  0.000002  0.004320
		3  0.000000  0.368750  0.000000  0.004320
		4  0.027621  0.745813  0.000099  0.004320
		5  0.000000  0.000000  0.000000  0.004320
		Q(s,a)函数是得出 s 点,a 行为的 的 权值,
		例如 Q(4,right) = 0.745813, right:0.745813比其他行为的权值高,
		所以 在 4 点 采取 right行为 是最为推荐的选择.
		每个学习回合(episode)初,先确定好 q 表, 
		如果是开始回合,就初始化q表,否则沿用上回合最终更新的q表
		此外,一张 e 表,
		e表结构与q表一致,但是e表的成员意义 不是 行为权值, 而是行为相关度 例如:
		       left     right        up      down
		0         5         1         0         0
  		1         0         7         1         3
		2         2         4         1         3
		3        10         0         0         1   
		4         0         0         0         0 
		5         3         6         1         9
		回合的每一步都会根据当前步更新了的 e 表, 整体更新 q 表所有行为权值		
		假设此刻为回合的最后一步,已经到达终点,
			此刻的 e 表 记录的相关度可以这么说说明一个事实:
			要到达终点,在地点0 left行为比较相关,在地点0,更应该执行 left 行为,同理
				在地点1,更应该执行 right 行为
				在地点2,更应该执行 right 行为
				在地点3,更应该执行  left 行为
				在地点5,更应该执行  down 行为
				在地点4,是终点.
			因此这一步,更新 q 表权值时,
			更新 0:left, 1:right, 2:right, 3:left, 5:down 位置的权值的幅度比较大
			这些位置的 权值有比较显著的提高.
		假设此刻为并非回合的最后一步.
			此刻的 e 表 记录的相关度可以这么说说明一个事实:
			如果要到达像这一刻所处的位置,或者状态:
				在地点0,更应该执行 left 行为
				在地点1,更应该执行 right 行为
				在地点2,更应该执行 right 行为
				在地点3,更应该执行  left 行为
				在地点5,更应该执行  down 行为
				由于还没有到达过地点4,
				所以地点4,并未采取过行为,
				所以并没有行为相关度统计
				所以并不知道地点4更应该执行啥,
				也不知道 地点4 是终点还是其他
			这一步,更新 q 表权值时,同样地,
			更新 0:left, 1:right, 2:right, 3:left, 5:down 位置的权值的幅度比较大
			这些位置的 权值有比较显著的提高.
			并不更新 地点4 的权值
		e表 相关度 的统计方式可以有很多种,上述表是每当执行一次对应行为,就在e表对应项加1
		还有其他的统计方式,比如也有像以下一样的 e表
		       left     right        up      down
		0       0.5      0.91       0.1      0.22
  		1       0.3      0,43      0.87     0.334
		2         0         0      0.13      0.34
		3       0.1     0.465     0,112       0.3  
		4     0.445         0     0.345       0.9
		5      0.13      0.56      0.22         0		
		每个回合开始前, e 表所有项 都先被赋0										 
	每个学习回合开的每一步工作内容如下:
		当前点 s , 有一个确认的行为 a
			如果不是执行在第一个回合 当前点s 是上一步的 s_, 
			行为 a 是上一步 s_ 在上一步通过 行为选取算法 得到的 a_
			如果是执行在第一个回合 当前点 s , 通过行为选取算法得到行为 a 
			行为选取算法典型有以下操作:
			有 e_greedy 概率通过q表权值操作来选取 行为 a_, 一般选最大权值的行为
			也有 1-e_greedy 概率 随机选取行为 a_!!
		执行操作 a , 根据操作会到达下个点 s_, 并根据奖励算法得到行为应有的奖励 r
		类似 sarsa 算法,再一次通过 行为选取算法 给S_ 选取 一个行为 a_
		取得 Q(s_,a_) 为我们的参考权值
		然后得到误差值 error = [ r + gama * Q(s_,a_) - Q(s,a) ]
			                gama 是 discount factor
		注意,跟sarsa差不多,下一步的这个行为a_,在下一步 s_ 点 一定会执行的!! 
		接着讨论 e 表, E(s,a) 表示 e表, 地点s,行为a 的相关值,比如当前e表为
			       left     right        up      down
			0       0.5      0.91       0.1      0.22
	  		1       0.3      0,43      0.87     0.334
			2         0         0      0.13      0.34
			3       0.1     0.165     0,112       0.3  
			4     0.445         0     0.345       0.9
			5      0.13      0.56      0.22         0
		通过相关值统计算法 更新 e表中 地点s,行为a 的相关值, 
		比如加 0.3 算法, 比如 s:3, a:right
			E(s,a) = E(s,a) + 0.3
		即 e 表变为:
		接着讨论 e 表, E(s,a) 表示 e表, 地点s,行为a 的相关值,比如当前e表为
			       left     right        up      down
			0       0.5      0.91       0.1      0.22
	  		1       0.3      0,43      0.87     0.334
			2         0         0      0.13      0.34
			3       0.1     0.465     0,112       0.3  
			4     0.445         0     0.345       0.9
			5      0.13      0.56      0.22         0
		然后 就 通过 e 表整体更新 q表: 譬如当前 e 表 q 表为:
			       left     right        up      down
			0  0.000000  0.004320  0.000000  0.004320
			1  0.000000  0.025005  0.000400  0.004320
			2  0.000030  0.111241  0.000002  0.004320
			3  0.000000  0.368750  0.000000  0.004320
			4  0.027621  0.745813  0.000099  0.004320
			5  0.000000  0.000000  0.000000  0.004320	
		更新后的 q 表: (注意是按位算法,不是矩阵算法)
	    left     right        up      down                     left  right    up  down
	|0.000000  0.004320  0.000000  0.004320|                 |  0.5   0.91   0.1  0.22|
	|0.000000  0.025005  0.000400  0.004320|                 |  0.3   0,43  0.87 0.334|
	|0.000030  0.111241  0.000002  0.004320|                 |    0      0  0.13  0.34|
	|0.000000  0.368750  0.000000  0.004320| + Alpha * error |  0.1  0.465 0,112   0.3|
	|0.027621  0.745813  0.000099  0.004320|                 |0.445      0 0.345   0.9|
	|0.000000  0.000000  0.000000  0.004320|                 | 0.13   0.56  0.22     0|
						   Alpha是学习率
		然后整体更新 e 表:
                                  left  right    up  down
                                |  0.5   0.91   0.1  0.22|
                                |  0.3   0,43  0.87 0.334|
                 gama * lamda * |    0      0  0.13  0.34|
                                |  0.1  0.465 0,112   0.3|
                                |0.445      0 0.345   0.9|
                                | 0.13   0.56  0.22     0|
		gama 还是上述的那个 gama
		lamda 就是 Sarsa(lamda) 提到的 lamda
	到这里,这一步的内容就算是完结了,下一步同样重复这样的内容!!
		直到第某步,执行操作 a 到达的下一点 s_ 是目标点,或者是陷阱点时,
		才正式结束结束这一回合.准备开始下一回合.
	这种算法如 sarsa 一般可以很好地避开陷阱点, 而且比 sarsa 的 学习速度更快.
	这种算法其实就是 sarsa 变种,下一步的这个行为a_,在下一步 s_ 点 一定会执行的!!	
	现在 讨论 lamda 的作用意义:
	如果 lamda 设定为0, 
		那么可以看到,每一步执行前, e 表所有项都是 0
		经过 E(s,a) = E(s,a) + 0.3 后,只有 地点s,行为a 那项目有相关值 E(s,a)
		然后更新 q 表的时候,其实就只是更新 地点s,行为a 的权值.
		本来其他操作都跟 sarsa 一样, 
		加上这里:每一步更新权值,只更新 地点s,行为a 的权值
		这样就跟 sarsa 一模一样了. 这样有单步更新的样子
	如果 lamda 设定为1, 	
		那么, 在这回合里, 每一步的统计行为 E(s,a) = E(s,a) + 0.3 都被记录
		如果这一步还没到达 终点,
			那么从回合开始到现在,所执行的所有行为都被记录
			其中那些多次被执行的行为, 相关值比较高!
			被认为是能到达当前地点所 相对必要执行的行为
			所以,这一步更新 q 表时,更要大幅度更新那些多次被执行的行为的权值.
		如果这一步到达 终点,
			那么从回合开始到回合结束,所执行的所有行为都被记录
			那些多次被执行的行为, 相关值比较高!
			被认为是能到达终点所 相对必要执行的行为
			所以,这一步更新 q 表时,更要大幅度更新那些多次被执行的行为的权值.
			这样就有 我们所认识 的回合更新 的样子
	如果 lamda 设定为 0~1 之间,
		那么, 在这回合里, 每一步的统计行为 E(s,a) = E(s,a) + 0.3 
		随着新一步的到来,被弱化一次.
		比如 A时刻,当前处于地点0,
			这一步执行了left,统计了一次行为 E(0,left) = E(0,left) + 0.3
		 	然后更新 q 表.
			然后更新 e 表,
			更新 e 表的时候,乘上了 lamda ,即整体弱化了 e 表 所相关度
		经过n步后,到达b时刻 ,处于 地点5,
			这段时间未回到 地点0 执行left,
			那么 e表 地点0 所有行为包括left 的相关度 就被弱化了n次.变得非常低
			那么 说明,要到达地点5, 与在地点0不管执行什么操作,并没有什么关系
		这样就有一个这样子的推论.
			回合开始,经过 m 步到达 终点,回合结束
			开始点的行为是什么,并不重要.
			但慢慢的,越靠近终点的 地点,可能两三步就到终点了,
			他们的 行为 的相关度 显得相当重要.
			就是说要到达终点,开始点的行为并不重要
			越靠近终点,的地点的行为,相关度受到重视,
			对应的 q 表权值的 更新幅度也就比较大
			就好像越靠近终点,就越能看到到终点的路一样
		这样看起来 有一种介于 单步更新 和 回合更新 之间的样子		   				
	奖励算法,行为选取算法,相关值统计算法 都是自定义的,需要根据实际情况谨慎设计
	Sarsa(lamda) 是变种 sarsa,所以也称 on-policy, 
		因为也是通过 行为选取算法 选取 s_ 的 下一步行为 a_作为参考权值行为
		下一步一定会执行这个行为
		下一步一定会执行的 a_ 是通过 行为选取算法选出的,
		再三强调,s与s_使用相同,行为选取算法,而这个算法的设计还是有一定讲究的
		开始好几个回合, s 选取的 a 或者是 s_ 选取的 a_ 都有可能使下一步掉入陷阱区,
		多次回合后,q表有初步的更新,然后行为选取算法选出的行为 一般都是能避开陷阱区的行为
		所以,多个回合后,后面学习 执行的行为 一般都可以很好避开陷阱,比较谨慎的前进.
		就好象会察觉到危险而不会贸然跌入陷阱那样有策略地前进.
		而且 e 表 和 lamda 的 补充 , 学习速度更快!!!
	关键小结:
		注意理解 lamda=0 相当于 sarsa算法 相当于但不更新
			lamda = 1 相当于 回合更新
			lamda = [0,1]之间时 相当于越靠近终点,越有把握选择正确步数	
		每一个新回合开始前,都必须 重置 相关度表E 全为0.
		每一步都 更新 q表和 E 表全部内容
		每一回合到达终点时, E表都记录了这个回合里 那些地点哪些行为执行得比较多
		表示这些 地点的行为相关度比较高,将更大幅度更新对应权值

奖励算法同样重要和讲究:
注意理解分析 到达重点才给奖励的行为, 这样靠近终点的地点的 的最大权值行为的权值比较高,
	靠近起点的最大权值行为的权值相对低些.
	即越靠经终点越有把握	
还有一个是 每一步都给同样奖励的行为,可能导致并无法学习
	每一步都给同等奖励0的行为,相当于每一步都没有给奖励


code.py 是一个基于 Q-learning强化学习的小例子
这个例子是 角色o 寻找最佳路径到达T
这个例子奖励方式是,如果S_到达目的点时,当前点s就得到奖励

///////////////////以下的奖励算法策略思维有所保留,似乎不适合讨论在这个例子上
然而,比较不好说服的是,只要向右走就给奖励!! 
	因为我们知道 位置T在最右边,直观知道一直往右走就是了,
	但这样就很难分辨是Q-learning算法的可靠性,
	意义上不是寻找最佳路径,而是推荐不断往右跑	
	这样就没有学习的意义了
推荐合理的给奖励方法之一是,距离近了一点就给奖励!!
推荐最合理的给给奖励方法之二:
	统计每一回合的移动数, 当前回合比前一回合移动步数少的时候,执行以下操作,
	地图上有6个位置, 对应Q表有6行
	统计每个位置当前回合,执行最多次数的操作,被选为 加奖励操作
	然后,下一回合,
	每个位置,当前操作是否加奖励,根据上回合的决定!!
////////////////////////////////////////








































北京大学 沈阳 性侵犯女学生
西安交大 导师周筠施压杨宝德至其跳楼
上海西南模范中学包庇 市三好学生李明泰猥琐女生
武汉理工大学包庇 王攀教授奴役陶崇明逼跳楼,大学威逼其胞姐道歉











??????????????????






20180310:
更换高速的ubuntu16源:
	sudo gedit /etc/apt/sources.list
	注释其他源，添加：
		# 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释
		deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial main restricted universe multiverse
		# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial main restricted universe multiverse
		deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse
		# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse
		deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse
		# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse
		deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-security main restricted universe multiverse
		# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-security main restricted universe multiverse		
		# 预发布软件源，不建议启用
		# deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse
		# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse
	sudo apt-get update

更换高速的pip3源：
	1、在用户目录下（/home/XXX）创建.pip文件夹，并创建pip.conf文件
	2、在pip.conf下输入：（注意：这里更换的是阿里云镜像源）
		[global]
		trusted-host = mirrors.aliyun.com
		index-url = http://mirrors.aliyun.com/pypi/simple
	3、 sudo apt-get update

python 虚拟环境：
	pip3 install virtualenv
	sudo pip3 install virtualenvwrapper	//虚拟环境管理模块
	mkdir $HOME/.local/virtualenvs	  //创建虚拟环境管理目录 (不要加sudo)
	sudo gedit ~/.bashrc 	//末尾添加:
		# by william
		# setting about virtualenvwrapper
		export VIRTUALENVWRAPPER_PYTHON=/usr/bin/python3
		export VIRTUALENV_USE_DISTRIBUTE=1        #  总是使用 pip/distribute                                        
		export WORKON_HOME=$HOME/.local/virtualenvs       # 所有虚拟环境存储的目录
		if [ -e $HOME/.local/bin/virtualenvwrapper.sh ];then
		   source $HOME/.local/bin/virtualenvwrapper.sh                                                
		else if [ -e /usr/local/bin/virtualenvwrapper.sh ];then
		         source /usr/local/bin/virtualenvwrapper.sh
		     fi
		fi
		export PIP_VIRTUALENV_BASE=$WORKON_HOME
		export PIP_RESPECT_VIRTUALENV=true
	source ~/.bashrc	//启动 virtualenvwrapper

	简单创建虚拟环境:
		virtualenv aaa  	//创建一个独立环境空间aaa,在当前文件夹建立一个aaa文件夹,
					//这种默认情况下,会把默认的解释机,和对应的默认软件库加入环境aaa
		virtualenv --no-site-packages bbb //创建一个独立环境空间aaa,在当前文件夹建立一个aaa文件夹,
							  //这情况下,不会把默认的软件库加入环境bbb,
		virtualenv ccc --python=python2   //创建一个独立环境空间ccc,在当前文件夹建立一个ccc文件夹,
						  //这种默认情况下,会把默认的软件库,和默认的解释机加入环境ccc
		启用虚拟环境
		cd ccc	//进入环境文件夹
		source ./bin/activate
		cd ~ //进入要执行的项目的文件夹,例如~
		查看当前状态
		(ccc) kingders@kingders-ThinkPad-T420:~$ 	//先可以直观看到(ccc)前缀,就是说现在处于 ccc 的独立python 工作环境里下
		退出虚拟环境
		deactivate
	通过管理套件创建虚拟环境:
		mkvirtualenv aaa -p python3	//创建
		workon aaa		//进入
		workon			//查看
		deactivate		//退出
		




!@!



20180319
IndentationError: expected an indented block
	这个问题要注意缩进！！








20180401
Tensorflow 常用:
batch 机制 的功能作用的描述 !!
	整理下认识,一般我们求出实际y与训练结果Y'的误差来反向传导更新模型
	而误差,我们可以取,方差,标准差,交叉熵什么的,
	注意,这些是 一个样本下的误差,我们说的方差交叉熵什么的,是基于同一个样本得到的,
	例如,图片a实际标签向量y:[y1,y2,..y10],训练结果标签向量Y':[y'1,y'2,..y'10].
	图片a的方差,是:	{ (y1-y'1)^2 + ..+ (y10 - y'10)^2 } /10
	那么cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction),reduction_indices=[1]))如何理解,
	我们知道,在一次训练里,输入了100张图片,即100张图片通过同样的 weights,blase,得到各自的训练结果标签向量Y'
	tf.reduce_mean括号里算出的是 每张图片自己的交叉熵,
	交叉熵指,有10个特征值的标签向量,把每个特征值的混乱程度加到一块得到的一个值
	然后通过 tf.reduce_mean,把 100 个交叉熵 算出一个 平均值,
	我们最后才通过这个 交叉熵平均值 来反向传导更新一次学习模型的参数weights和biase
	即我们是通过100张图片得到100个交叉熵,然后取100个交叉熵的平均值
	并不是通过100张图片得到1个交叉熵,而且也不知道如何得到
	也不是100个交叉熵,每个交叉熵都反向传导更新模型一次,即不是在一次训练里更新100次模型!!!
	我们只是取 100个交叉熵的平均值来更新 1 次模型
	所以.模型中的 batch 设置为100,那个100,就是这种意思
参数:
tf.Variable.init(initial_value, trainable=True, collections=None, validate_shape=True, name=None)
	initial_value 	所有可以转换为Tensor的类型 	变量的初始值
	trainable 	bool 	如果为True，会把它加入到GraphKeys.TRAINABLE_VARIABLES，才能对它使用Optimizer
	collections 	list 	指定该图变量的类型、默认为[GraphKeys.GLOBAL_VARIABLES]
	validate_shape 	bool 	如果为False，则不进行类型和维度检查
	name 		string 	变量的名称，如果没有指定则系统会自动分配一个唯一的值
从正态分布中输出随机值:	
tf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None) 
	shape: 一维的张量，也是输出的张量。
	mean: 正态分布的均值。
	stddev: 正态分布的标准差。
	dtype: 输出的类型。
	seed: 一个整数，当设置之后，每次生成的随机数都一样。
	name: 操作的名字。
	例子:
	|2,6,7| = tf.random_normal([2.3])
	|9,1,4|
占位符号:
tf.placeholder(dtype, shape=None, name=None)
	此函数可以理解为形参，用于定义过程，在执行的时候再赋具体的值
	dtype：数据类型。常用的是tf.float32,tf.float64等数值类型
	shape：数据形状。默认是None，行不定，比如[2,3]表示列是3，行是2, [None, 3]表示列是3，行不定
	name：名称。
二维数组(二维矩阵)的叠加函数	
tf.reduce_sum()
	例子1:
	[2,2,2] = tf.reduce_sum(|1,1,1|, reduction_indices=[0] )
				|1,1,1|
	|3| = tf.reduce_sum(|1,1,1|, reduction_indices=[1] )
	|3|		    |1,1,1|	
	例子2:
	6 = tf.reduce_sum(|1,1,1|, reduction_indices=[0,1] )
			  |1,1,1|	
	就是先reduction_indices=[0]得到[2,2,2],再reduction_indices=[1] 得到 6 

二维数组的乘函数
tf.matmul()
	a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3]) => [[1. 2. 3.]
	                                                      [4. 5. 6.]]

	b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2]) => [[7. 8.]
	                                                         [9. 10.]
	                                                         [11. 12.]]
	c = tf.matmul(a, b) => [[58 64]
        	                [139 154]]
二维数组的 2次方
tf.square()
	|1,  4, 9| = tf.reduce_sum(|1,2,3|)
	|16,25,36|		   |4,5,6|
二维数组的 平均值:
tf.reduce_mean()
	  2.5 = tf.reduce_mean(|1,2|)
			       |3,4|
	|2,3| = tf.reduce_mean(|1,2|, 0)
			       |3,4|
	|1.5| = tf.reduce_mean(|1,2|, 1)
	|3.5|		       |3,4|

二维数组的 最大值位置:
tf.argmax(|1, 2, 3|,0)=[3,3,1]	//数组从选出 每列中最大值的位置 (从0数)
          |2, 3, 4|
          |5, 4, 3|
          |8, 7, 2|
tf.argmax(|1, 2, 3|,1)=[2, 2, 0, 0] //数组从选出 每行中最大值的位置 (从0数)
          |2, 3, 4|
          |5, 4, 3|
          |8, 7, 2|
tf.cast 类型转换 函数:
	tf.cast([2, 3, 4], tf.float32) //把一维数组的每个int值转换为float值

tf.reduce_sum
	# 'x' is [[1, 1, 1]
	#         [1, 1, 1]]
	tf.reduce_sum(x) ==> 6
	tf.reduce_sum(x, 0) ==> [2, 2, 2]
	tf.reduce_sum(x, 1) ==> [3, 3]
	tf.reduce_sum(x, 1, keep_dims=True) ==> [[3], [3]]
	tf.reduce_sum(x, [0, 1]) ==> 6

按正太分布随机生成 多维数组:
tf.truncated_normal(shape, mean, stddev)
	shape表示生成张量的维度，mean是均值，stddev是标准差。这个函数产生正太分布，均值和标准差自己设定。
	例子: 
	|1.95758033,-0.68666345,-1.83860338, 0.78213859|= tf.truncated_normal(shape=[2,4], mean=0, stddev=1) 
        |0.38035342, 0.57904619,-0.57145643,-1.22899497|
生成tensor：
	tf.zeros(shape, dtype=tf.float32, name=None)	//零矩阵
	tf.zeros_like(tensor, dtype=None, name=None)
	tf.constant(value, dtype=None, shape=None, name='Const') //值都为value的矩阵
	tf.fill(dims, value, name=None)
	tf.ones_like(tensor, dtype=None, name=None)
	tf.ones(shape, dtype=tf.float32, name=None)
生成序列
	tf.range(start, limit, delta=1, name='range')
	tf.linspace(start, stop, num, name=None)
生成随机数
	tf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)
	tf.truncated_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)
	tf.random_uniform(shape, minval=0.0, maxval=1.0, dtype=tf.float32, seed=None, name=None)
	tf.random_shuffle(value, seed=None, name=None)
卷积操作
tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, name=None)
	第一个参数input：指需要做卷积的输入图像，它要求是一个Tensor，
		具有[batch, in_height, in_width, in_channels]这样的shape，
		具体含义是[训练时一个batch的图片数量, 图片高度, 图片宽度, 图像通道数]，
		即这个参数是一个多维数组!!
		注意这是一个4维的Tensor，要求类型为float32和float64其中之一
		然而实际操作是 [batah,in_height*in_width*in_channels]二维数组(二维tensor)
	第二个参数filter：相当于CNN中的卷积核，它要求是一个Tensor，
		具有[filter_height, filter_width, in_channels, out_channels]这样的shape，
		即这个参数是一个多维数组!!
		具体含义是[卷积核的高度，卷积核的宽度，图像通道数，输出图像通道数]，
		要求类型与参数input相同，有一个地方需要注意，第三维in_channels，就是参数input的第四维
		注意,这个参数的含义
		譬如 input是一张28*28有32个通道的图片,即有32张特征图片, 
			有一个 shape=[2,2,32,64]的filter多维向量 ,即说明
			shape=[2,2,32,64]说明 输入的这张图片是有32个通道的,即有32张特征图片的,
			shape=[2,2,32,64]也要求卷积输出后的图片只有有64个通道的,即有64张特征图片的
			shape=[2,2,32,64]也说明,filter里共有 32x64 个独立 2x2 的卷积核
			所以按道理卷积出来的 通道应该有 32x64 个,而不是64 个
			所以这里卷积的过程与我们理论学习的过程有些详细的区别!!
			理论上,输出64通道的话,32个输入通道,每个分配两个卷积核就可以了!
			而这里是每个分配 64 个卷积核,一个通道就能卷积出64这个特征图片了,
			但是接着把每个通道卷积出的64个特征图片,求平均得出2个平均特征图片
			每个通道得出2个平均特征图片,32个就得出64个,
			这64个平均特征图片就凑成最后要输出的64个通道
	第三个参数strides：卷积时在图像每一维的步长，这是一个一维的向量，长度4
		由于对于图片,只有长宽,所以,只用到中间两个值表示步数,其他为1,
		如 strides=[1, 4, 4, 1],表示长宽步长都为4,
		即不在batch和channels上做卷积
	第四个参数padding：string类型的量，只能是"SAME","VALID"其中之一
		padding可'SAME' 可`VALID, SAME表示卷积所有像素,VALID则不是
		VALID:
		 1 2 3 4 5 6 7 8 9 10 11 12 13	多出的12,13不卷积
		|___________|
			  |_____________|
		SAME:
		0 |1 2 3 4 5 6 7 8 9 10 11 12 13| 0 0	卷积所有像素,不够的补0
		|___________|                   |
			  |____________|        |
	                            |________________|	
	第五个参数：use_cudnn_on_gpu:bool类型，是否使用cudnn加速，默认为true
	做后一个是当前卷积操作的名字
		结果返回一个Tensor，这个输出，
		就是我们常说的feature map，shape仍然是[batch, height, width, channels]这种形式。
		即是下一层的input,下一层的卷积的输入图像
池化操作:
tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')	
	建立一个池化操作
	池化输出的是邻近区域的概括统计量，一般是矩形区域。池化有最大池化、平均池化、滑动平均池化、L2范数池化等
	池化与卷积意义上不一样,虽然操作上有很多地方相同
	第一个参数value：需要池化的输入，一般池化层接在卷积层后面，
		所以输入通常是feature map，依然是[batch, height, width, channels]这样的shape
	第二个参数ksize：池化窗口的大小，取一个四维向量，一般是[1, height, width, 1]，
		因为我们不想在batch和channels上做池化，所以这两个维度设为了1
	第三个参数strides：和卷积类似，窗口在每一个维度上滑动的步长，一般也是[1, stride,stride, 1]
	第四个参数padding：和卷积类似，可以取'VALID' 或者'SAME'	









>>>>>>>>>>>>>>>>

20180401
回归:
/home/william/AI/machine learning/回归/code.py
通过散点数据训练一个模型,找到散点的落入规律,
这里找到的规律是,散点落入一个二次函数范畴的规律,也就说通过模型得到的散点建立的曲线越来越像二次曲线 
先人为制作一个 二次函数曲线 的散点图,作为样本参数 这里是建立 300 个散点
	x_data = np.linspace(-1, 1, 300, dtype=np.float32)[:, np.newaxis]
	noise = np.random.normal(0, 0.05, x_data.shape).astype(np.float32)
	y_data = np.square(x_data) - 0.5 + noise  
再通过 matplotlib.pyplot 显示散点图!! 
	plt.scatter(x_data, y_data)	
	plt.show()	//这里训练和显示散点图是冲突的,要训练,就要屏蔽显示散点图
训练模型建立:
//定义如何建立层
def add_layer(inputs, in_size, out_size, activation_function=None): //定义如何建立层
	重点如何地定义变量定义:例如:
	Weights = tf.Variable(tf.random_normal([in_size, out_size]))
	tf.random_normal 得随机变量值,这里出来是随机二维数组 in_size*out_size,其他参数默认,
	tf.Variable 把这个随机二位数组值变成 tensorflow变量
//定义训练模型的输入输出变量占位符
	xs = tf.placeholder(tf.float32, [None, 1])
	ys = tf.placeholder(tf.float32, [None, 1])
	//输入的xs.输出的ys是一维数组,而[None, 1]表示列是1，行不定的一维维数组,
	//注意是一维数组,不是一维向量,是有多个一维向量组成的一维数组
	//之所一维数组,是因为在这个例子里,样本是一个个的点坐标,而每一次训练是一次性输入XX个样本,统一计算
	//这堆样本的x分量会放入xs里,变成一个有XX个一维向量的一维数组
	//这堆样本的y分量会放入ys里,变成一个有XX个一维向量的一维数组	
//构建多层模型
	这里只有两层: l1隐藏层,和 prediction预测输出层
	l1 = add_layer(xs, 1, 10, activation_function=tf.nn.relu)
	prediction = add_layer(l1, 10, 1, activation_function=None)
	//li层,会对结果执行relu激活算法,使第一层的输出有10个变量的一维数组的变量值在0-1附近
	//使用激活函数,可以优化避免梯度消失和梯度爆炸的情况发生
//设置训练方式
	loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys-prediction), reduction_indices=[1]))
	//例子说明:例如一次训练直接输入5个样本点 [a,A][b,B][c,C][d,D][e,E]
	//那么 xs=|a| ,  ys=|A|  通过 xs 得到的 prediction=|Y|
	//	  |b|       |B|				  |H|
	//	  |c|       |C|				  |Z|
	//	  |d|       |D|				  |T|
	//	  |e|       |E|     			  |V|
	//那么 tf.square就得到 |(A-Y)^2|
	//		      |(B-H)^2|
	//		      |(C-Z)^2|
	//		      |(D-T)^2|
	//		      |(E-V)^2|
	//然后 tf.reduce_sum(..,reduction_indices=[1]) 得到:
	//	|(A-Y)^2|
	//	|(B-H)^2|
	//	|(C-Z)^2|
	//	|(D-T)^2|
	//	|(E-V)^2|
	//	因为是每行只有一个量,所 tf.reduce_sum 后并没有变化
	//然后 tf.reduce_mean()得到平均值:
	//	( |(A-Y)^2| + |(B-H)^2| + |(C-Z)^2| + |(D-T)^2| + |(E-V)^2| ) / 5 	
	train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)
	//使用 普通的梯度下降的优化方法(GradientDescent),来训练优化 loss, 学习率是 0.1
	//最终这次训练会更新所有的 weights 和 biases
//初始化tf训练环境
	sess = tf.Session(),	
//初始化tensorflow的所有变量
	init = tf.global_variables_initializer()
	sess.run(init)
//开始训练
	for i in range(1000):
    		sess.run(train_step, feed_dict={xs: x_data, ys: y_data})
	//执行1000次训练,每次训练都 读入 300 个散点,即 xs数组有300行, ys数组有300行
	//而这里我们就只有300个样本,所以每次训练都读入同一组数据
    		if i % 50 == 0:
        	print(sess.run(loss, feed_dict={xs: x_data, ys: y_data}))
	//每50次训练后,打印一次 loss

/home/william/AI/machine learning/回归/code.py
这个例子补充主要是图像监测部分!!
使用:import matplotlib.pyplot as plt
	fig = plt.figure()		//创建一个独立的视图窗口
	ax = fig.add_subplot(1,1,1)	//在窗口添加一个子视图ax
	ax.scatter(x_data, y_data)	//子视图的 x,y 轴对应 x_data, y_data
	plt.ion()			//使用交互形式,
	plt.show()			//一直显示图,(如果不开启交互模式,默认是阻塞模式,)
					//交互模式下一直显示图,图会一直显示,而程序也会继续plt.show()后的内容
					//阻塞模式下一直显示图,会一直卡在plt.show(),
	lines = ax.plot(x_data, prediction_value, 'r-', lw=5)
					//据 x_data, prediction_value 的一堆散点画出一条线
					//x值,y值,红色,宽度5
	plt.pause(1)		//暂停一秒
	ax.lines.remove(lines[0])//把刚刚画的线去掉,(这样就画下一条线,就不会挡住什么的)







20180401
学习使用tensorboard监视模型:/home/william/AI/machine learning/tensorboard
code3.py
重点是,在使用 tf.xxxx之前,先添加 with tf.name_scope('XXXX'):
例如:
with tf.name_scope('layer'):
    with tf.name_scope('weights'):
        Weights = tf.Variable(tf.random_normal([in_size, out_size]), name='W')
with tf.name_scope('inputs'):
    xs = tf.placeholder(tf.float32, [None, 1], name='x_input')
with tf.name_scope('loss'):
    loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction), reduction_indices=[1]))
with tf.name_scope('train'):
    train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)
然后,sess = tf.Session()后,init = tf.global_variables_initializer()前
    writer = tf.summary.FileWriter("logs/", sess.graph)
这样执行代码时,会把模型图加载到logs/里,
终端运行 tensorboard --logdir=logs后,就可以在浏览器http://127.0.0.1:6006/查看到模型内容

code4.py
重点添加训练参数的跟踪记录表
例如:
    Weights = tf.Variable(tf.random_normal([in_size, out_size]), name='W')
    tf.summary.histogram(layer_name + '/weights', Weights) 
    //在tensorboard的histogram和distribution栏添加Weights 的训练跟踪记录表
    loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction),reduction_indices=[1]))
    tf.summary.scalar('loss', loss)
    //在tensorboard的scalar栏添加loss 的训练跟踪记录表
然后,sess = tf.Session()后,init = tf.global_variables_initializer()前
    merged = tf.summary.merge_all()
    writer = tf.summary.FileWriter("logs2/", sess.graph)
这样执行代码时,会把模型图,还有训练跟踪表设置加载到logs2/里,
然后每隔n次训练后,给所有训练跟踪表添加新数据
    result = sess.run(merged, feed_dict={xs: x_data, ys: y_data})
    writer.add_summary(result, i)
最后终端运行 tensorboard --logdir=logs后,就可以在浏览器http://127.0.0.1:6006/查看到模型内容









20180401
分类:/home/william/AI/machine learning/分类
code5.py
注意,这里使用的交叉熵跟我之前分析的交叉熵有所区别
我之前分析的是 基于一个图像样品,得到的交叉熵再反向传导学习,
而这里却是 100个 样本的交叉熵,这里的交叉熵有点像平均值的意思,然后再反向传导学习
http://www.360doc.com/content/17/0118/20/10408243_623338635.shtml
经过慎重分析发现:
	注意不是 100个 样本的交叉熵,
	而是每个样本一个交叉熵,共100个,然后把他们都加起来除以100
	得到一个交叉熵的平均值,使用这个平均值反向传导训练模型
模型训练的思路分析:
每张图片有728个像素点: [x(1)1,x(1)2,x(1)3,...x(1)728],
	xs符合[None, 784],none=100行,即xs包含100张图片,即:
	xs = [x(1)1,x(1)2,x(1)3,x(1)4,...x(1)728],
	     [x(2)1,x(2)2,x(2)3,x(2)4,...x(2)728]
	     .......
	     [x(100)1,x(100)2,x(100)3,x(728)4,...x(100)728]
Weights符合[in_size, out_size],in_size=728,out_size=10,即
	Weights = [w(1)1,w(2)1,..w(10)1],
	          [w(1)2,w(2)2,..w(10)2]
		  [w(1)3,w(2)3,..w(10)3]
		  [w(1)4,w(2)4,..w(10)4]
		  .......
		  [w(1)728,w(2)728,..w(10)728]
bias符合[1,out_size],out_size=10,即
	bias = [b1,b2,b3,b4,..b10]
图片对应的标签向量: [y(1)1,y(1)2,..y(1)10]
	ys符合[None, 10],none=100行,即 ys 对应100张图片的 100 个标签向量:
	ys = [y(1)1,y(1)2,..y(1)10]
	     [y(2)1,y(2)2,..y(2)10]
	     [y(3)1,y(3)2,..y(3)10]
	     [y(4)1,y(4)2,..y(4)10]
	     ........
	     [y(100)1,y(100)2,..y(100)10]
那么,Wx_plus_b=y:
[x(1)1,x(1)2,x(1)3,x(1)4,...x(1)728]          * [w(1)1,w(2)1,..w(10)1]      + [b1,b2,b3,b4,..b10] = [y(1)1,y(1)2,..y(1)10]
[x(2)1,x(2)2,x(2)3,x(2)4,...x(2)728]            [w(1)2,w(2)2,..w(10)2]                              [y(2)1,y(2)2,..y(2)10]
 .......                                        [w(1)3,w(2)3,..w(10)3]                              [y(3)1,y(3)2,..y(3)10]
[x(100)1,x(100)2,x(100)3,x(728)4,...x(100)728]  [w(1)4,w(2)4,..w(10)4]                              [y(4)1,y(4)2,..y(4)10]
                                                .......						    ........
	                                        [w(1)728,w(2)728,..w(10)728]                        [y(100)1,y(100)2,..y(100)10]

batch 机制的功能作用的描述 !!
	整理下认识,一般我们求出实际y与训练结果Y'的误差来反向传导更新模型
	而误差,我们可以取,方差,标准差,交叉熵什么的,
	注意,这些是 一个样本下的误差,我们说的方差交叉熵什么的,是基于同一个样本得到的,
	例如,图片a实际标签向量y:[y1,y2,..y10],训练结果标签向量Y':[y'1,y'2,..y'10].
	图片a的方差,是:	{ (y1-y'1)^2 + ..+ (y10 - y'10)^2 } /10
	那么cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction),reduction_indices=[1]))如何理解,
	我们知道,在一次训练里,输入了100张图片,即100张图片通过同样的 weights,blase,得到各自的训练结果标签向量Y'
	tf.reduce_mean括号里算出的是 每张图片自己的交叉熵,
	交叉熵指,有10个特征值的标签向量,把每个特征值的混乱程度加到一块得到的一个值
	然后通过 tf.reduce_mean,把 100 个交叉熵 算出一个 平均值,
	我们最后才通过这个 交叉熵平均值 来反向传导更新一次学习模型的参数weights和biase
	即我们是通过100张图片得到100个交叉熵,然后取100个交叉熵的平均值
	并不是通过100张图片得到1个交叉熵,而且也不知道如何得到
	也不是100个交叉熵,每个交叉熵都反向传导更新模型一次,即不是在一次训练里更新100次模型!!!
	我们只是取 100个交叉熵的平均值来更新 1 次模型
	所以.模型中的 batch 设置为100,那个100,就是这种意思
其他重点内容:
	mnist = input_data.read_data_sets('MNIST_data', one_hot=True) //这里是导入官方训练样品库的方法
	batch_xs, batch_ys = mnist.train.next_batch(100)//从训练集,取出100个28*28图片样本和对应标签向量
	mnist.test.images, mnist.test.labels//检验集的图片样品,和对应标签向量
	
code6.py
重点是使用了sklearn 生成的样本,可以辅助我们学习使用tensorflow做很多模拟事情
	from sklearn.datasets import load_digits
	from sklearn.model_selection import train_test_split
	from sklearn.preprocessing import LabelBinarizer
	# load data
	digits = load_digits()
	X = digits.data
	y = digits.target
	y = LabelBinarizer().fit_transform(y)
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3)
还有的是,清晰告诉我们如何使用 dropout
	def add_layer(inputs, in_size, out_size, layer_name, activation_function=None, ):
	    # add one more layer and return the output of this layer
	    Weights = tf.Variable(tf.random_normal([in_size, out_size]))
	    biases = tf.Variable(tf.zeros([1, out_size]) + 0.1, )
	    Wx_plus_b = tf.matmul(inputs, Weights) + biases
	    # here to dropout
	    Wx_plus_b = tf.nn.dropout(Wx_plus_b, keep_prob)
	    if activation_function is None:
	        outputs = Wx_plus_b
	    else:
	        outputs = activation_function(Wx_plus_b, )
	    tf.summary.histogram(layer_name + '/outputs', outputs)
	    return outputs
	之前学习到:dropout不算是一个正规正矩的优化器，他的工作是，每次网络工作时，
	都随机抛弃一部分的神经元的作用，从而避免过度拟合
	从这个层建设定义中dropout的位置可看出,dropout不属于激活函数,
	同时也不能算作是一个优化器











20180403
CNN: code7.py
准确率计算:
def compute_accuracy(v_xs, v_ys):
    global prediction
    y_pre = sess.run(prediction, feed_dict={xs: v_xs, keep_prob: 1})
    correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(v_ys,1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
    result = sess.run(accuracy, feed_dict={xs: v_xs, ys: v_ys, keep_prob: 1})
    return result
	我们知道 图片对应的标签向量我们成为 one-hot 向量,即这样表示:
		数字图片0:[1,0,0,0,0,0,0,0,0,0]
		数字图片1:[0,1,0,0,0,0,0,0,0,0]
		...
		数字图片9:[0,0,0,0,0,0,0,0,0,1]
	而通过学习模型学习到的标签向量,往往不是整数的,例如:
		数字图片1:[0.01, 0.98, 0, 0.001, 0.1, 0.1, 0, 0, 0.02, 0.4]
		而标签向量 与 学习得到的标签向量, 位置1(从0数)的值都是最大的.
		这样就认为 模型准确学习识别出数字1的图片
	重点看: correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(v_ys,1))
		先tf.argmax算出 标签向量 与 学习得到的标签向量,的最大值位置
		比较这两个位置是否一样.
		注意 y_pre, v_ys 在这里是二维数值,即包含多个标签向量,
		tf.argmax后是一个表示位置意义的一维数组
		tf.equal后是一个表示正确与否意义的一维数组
	        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
		先把 correct_prediction 向量的每个值都转为 float类型,然后把加起来求平均
		这里,如果模型相当好,correct_prediction 向量的每个值基本为1,
		最后的平均值就接近1了
模型参数设置!!
权重weight
def weight_variable(shape):
    initial = tf.truncated_normal(shape, stddev=0.1)
    return tf.Variable(initial)
	按正太分布随机生成 张量的维度为shanpe 的权重数组 initial,
	然后计入 tf.Variable()
def bias_variable(shape):
    initial = tf.constant(0.1, shape=shape)
    return tf.Variable(initial)
	生成值为 0.1 ,张量的维度为shanpe 的 常量数组 inital
	然后计入 tf.Variable()
def conv2d(x, W):
    # stride [1, x_movement, y_movement, 1]
    # Must have strides[0] = strides[3] = 1
    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')
	建立一个卷积操作,注意strides=[1, 1, 1, 1]描述的是卷积核移动步长
	由于对于图片,只有长宽,所以,只用到中间两个值表示步数,其他为1,
	如 strides=[1, 4, 4, 1],表示长宽步长都为4
	padding可'SAME' 可`VALID, SAME表示卷积所有像素,VALID则不是
	VALID:
	 1 2 3 4 5 6 7 8 9 10 11 12 13	多出的12,13不卷积
	|___________|
		  |_____________|
	SAME:
	0 |1 2 3 4 5 6 7 8 9 10 11 12 13| 0 0	卷积所有像素,不够的补0
	|___________|                   |
		  |____________|        |
                            |________________|		

def max_pool_2x2(x):
    # stride [1, x_movement, y_movement, 1]
    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')
	建立一个池化操作
	池化输出的是邻近区域的概括统计量，一般是矩形区域。池化有最大池化、平均池化、滑动平均池化、L2范数池化等
	池化与卷积意义上不一样,虽然操作上有很多地方相同
	max_pool 是指最大池化的意思
最后补充内容:
	batch_xs, batch_ys = mnist.train.next_batch(100)
	sess.run(train_step, feed_dict={xs: batch_xs, ys: batch_ys, keep_prob: 0.5})
	先是,获取100张图片 的数据,
	然后加载到 占位符空间里,成为真正的参数
	compute_accuracy(mnist.test.images[:1000], mnist.test.labels[:1000]))
	输入1000用于mnist测试图片,即 batch=1000,使用定义的compute_accuracy函数检测准确率
code8.py是code7的后续
主要分析如何建设层模型!!
	# define placeholder for inputs to network
	xs = tf.placeholder(tf.float32, [None, 784])/255.   # 28x28	
	ys = tf.placeholder(tf.float32, [None, 10])
	keep_prob = tf.placeholder(tf.float32)
	x_image = tf.reshape(xs, [-1, 28, 28, 1])
	# print(x_image.shape)  # [n_samples, 28,28,1]
		占位符 xs 表示输入的每张图片是28X18=784个像素点,未知有多少图片输入所以none
		由于图片像素点值都是从值 0-255 来记录颜色的!!,增大后续计算量级,所以 除以255,
		把值域从 0-255 压缩到 0-1,只是值的比例缩小了,没有改变值记录的图像信息
		xs 是一个二维数组,一维表示图片,一维表示图片数量(即batch大小)
		所以需要转换传换成 4维数组, [batch,高,寬,深度(通道数)]
	## conv1 layer ##
	W_conv1 = weight_variable([5,5, 1,32]) # patch 5x5, in size 1, out size 32
	b_conv1 = bias_variable([32])
	h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1) # output size 28x28x32
	h_pool1 = max_pool_2x2(h_conv1)                                         # output size 14x14x32
		第一层CNN
		1*32 个 5x5 卷积核 卷积batch张,28x28的图片,最后得batch张 含32个通道的卷积后图片
		然后,卷积后图片 都经过池化得到  batch张 含32个通道的池化后图片	
	## conv2 layer ##
	W_conv2 = weight_variable([5,5, 32, 64]) # patch 5x5, in size 32, out size 64
	b_conv2 = bias_variable([64])
	h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2) # output size 14x14x64
	h_pool2 = max_pool_2x2(h_conv2)                                         # output size 7x7x64
		第二层CNN
		32*64 个 5x5 卷积核 卷积batch张,含32个通道的上一层池化后图片,
		先得到batch张,含 32*64 个通道的卷积后图片
		然后,平均压简成 batch张,含 64 个通道的卷积后图片
		然后,卷积后图片 都经过池化得到  batch张 含64个通道的池化后图片				
	## fc1 layer ##
	W_fc1 = weight_variable([7*7*64, 1024])
	b_fc1 = bias_variable([1024])
	# [n_samples, 7, 7, 64] ->> [n_samples, 7*7*64]
	h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])
	h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)
	h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)
		全连接层1
		第二层CNN 得到的  batch张 含64个通道的池化后图片 是一个[batch,高,寬,深度(通道数)]4维数组
		转换成 二维数组[batch,第二层池化后图片],
		然后换 Wx_plus_b 的层模型,使用relu激励函数 继续构建
		这里添加了 dropout 处理,是为了避免过拟合问题	
	## fc2 layer ##
	W_fc2 = weight_variable([1024, 10])
	b_fc2 = bias_variable([10])
	prediction = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)
		全连接层1
		同样使用 Wx_plus_b 的层模型 使用softmax激励函数 最后得到 学习的 one-hot 标签向量!!
	# the error between prediction and real data
	cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction),
                                              reduction_indices=[1]))       # loss
	train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)
		同样通过 -tf.reduce_sum(ys * tf.log(prediction),reduction_indices=[1])
		算出每张图片的 交叉熵,然后 tf.reduce_mean 求平均得到一个平均交叉熵
		通过 优化器 AdamOptimizer 优化器处理平均交叉熵 来执行反向传导,
		更新一次所有的学习参数(包括所有的卷积核,所有的weight和biase)










20180404
RNN 
code9.py 根据sin 画出cos
首先看 执行训练的部分:
	_, cost, state, pred = sess.run(
            [model.train_op, model.cost, model.cell_final_state, model.pred],
            feed_dict=feed_dict)
	sess.run先后执行了4个函数模块 model.train_op, model.cost, model.cell_final_state, model.pred
	sess.run执行 train_op时,会回溯执行嵌套操作
		当前的 cost,cell_final_state, pred,最后更新了一次学习模型
	执行完train_op后 ,sess.run继续执行 cost
		这时执行的 cost,得到的是 更新学习模型后的状态下得到的 cost
	sess.run继续执行 cell_final_state,得到的是 更新学习模型后的状态下得到的 cell_final_state
	sess.run继续执行 pred,得到的是 更新学习模型后的状态下得到的 pred
初步了解 (只考虑batch为1,即喂一堆段数据的情况)
	这里每次input是一段数据有 20 个数据单元(20个x数据得到的sin(x) ) ,
	然后每个数据单元通过 同一输入转化矩阵 wx_plus_b 扩成 10维输入向量:x1,x2...x10
	就是说输入转化矩阵有 10 个 W 和 B 需要学习
	然后 lstm 的cell 和hidden_unit 状态向量 都是10维向量 , 
	每次 给 lstm 喂 一个 10维输入向量 得到 一个 cell 和 hidden_unit 状态向量
	这个 cell 和 hidden_unit 状态向量,又作为下一次 喂 输入向量需要 的 cell 状态向量 和hidden_unit 状态向量
	有20个数据单元,所以一共喂 20次 10维输入向量
	收集 每次得到lstm 输出的 hidden_unit 状态向量,作为输出向量 即收集到 20个
	每个输出向量又通过 另一个 相同的 矩阵乘法 wx_plus_b 缩成一个输出数据
	输出转化矩阵有 10 个 W 和 B 需要学习
	得到20个输出数据(20个),
	再与20个真正的输出数据(20个x数据对应cos(x) 比较 得到一个 误差值(不是误差向量) 
	误差越小, 表示 从sin 推导出的 cos 越准确 !! 
	 
code10.py 同样先初步了解
这里是,一串一串地输入图片像数流数据,最后学习分辨出是什么图片!!!
初步了解
	这里每次input是一段数据有 28 条像素数据,
	每条像素数据 含28个像素点信息.
	每条像素数据 通过 同一个输入转化矩阵 wx_plus_b  扩成 128维向量x1,x2...x128
	输入转化矩阵有 128*28 个 W 和 128 个 B 需要学习
	然后 lstm 的 cell 和hidden_unit 状态向量 都是128维向量 , 
	每次 给 lstm 喂 一个 128维输入向量 得到 一个 cell 状态向量 和 hidden_unit 状态向量
	这个 cell 和 hidden_unit 状态向量,又作为下一次 喂 输入向量需要 的 cell 和hidden_unit 状态向量
	有 28 条像素数据,所以一共喂 28次 128维输入向量
	收集 每次得到lstm 输出的 hidden_unit 状态向量,作为输出向量 即收集到 20个
	把这 28 个 输出向量组成 一个 28维数组,即 28*128 矩阵,
	然而其实,我们只需要最后一个输出向量,(第28个),其他的丢弃
	后一个输出向量 乘以一个 转化矩阵 得到  一个 10维变量
	这个转化矩阵的 W 和 B 也需要学习
	这个10维变量 与 实际图片对应的 one-hot 变量比较 得到一个 误差值(不是误差向量)
	误差越小, 识别图片的准确率越高. 
参考:	
	http://dy.163.com/v2/article/detail/CTIPFRJF0511K58A.html
	https://www.zhihu.com/question/40819582
	https://blog.csdn.net/u014595019/article/details/52605693
	https://blog.csdn.net/u014595019/article/details/52759104
	https://www.jianshu.com/p/4e285112b988
lstm单元:
	t表示当前时间

                                     ht
		  ___________________|____
	C(t-1)----|                      |-----Ct
	          |                      |
		  |                      |
	          |                      |
	h(t-1)----|______________________|-----ht
	            |
		   xt

	xt     当前输入值,是一个向量!!
	h(t-1) 前一刻的 h 状态值,是一个向量!!  h 俗称 hidden_unit
	C(t-1) 前一刻的 C 状态值,也是一个向量!! C 俗称 Cell	
	ht     当前的 h 状态值,是一个向量!!  
	Ct     当前的 C 状态值,也是一个向量!! 
	注意: xt, h(t-1), ht, Ct, C(t-1) 向量维数相同,(一维数组 也称为向量)
	还有,xt是处理过的输入,比如,这代表一个句子中的一个词语,xt不是词语本身,而是对应处理过得到的向量
 	同样,yt是未处理的输入,yt这个向量需要做处理后才得到我们直接观察的结果
	三个门向量:
		输入门: it = sigmod{ (Wxi)(xt) + (Whi)(h(t-1)) }
		忘记门: ft = sigmod{ (Wxf)(xt) + (Whf)(h(t-1)) }
		输出门: ot = sigmod{ (Wxo)(xt) + (Who)(h(t-1)) }
	候选值向量:
		       ~
	               Ct  =  tanh{ (Wxc)(xt) + (Whc)(h(t-1)) }
 	当前的 C 状态值:
		                                     ~
	               Ct  =  ft ⊙  C(t-1)  +  it ⊙  Ct
	当前的 h 状态值:
		       ht  =  ot ⊙  Ct
	注意:⊙ 是 自定义乘法, 这里是按元素乘法 ,例如:门[0,1,1,0,0,1] X 向量[2,3,4,5,6,7] = [0,3,4,0,0,7]
	向量的按元素乘法也叫:Hadamard product (also known as the Schur product
	这里由于门向量的成员基本上不是0,就是1,所以就有所谓开关的意义,
	所以就可以让一部分内容向后传输,阻隔一部分输出,达到长短记忆的作用
	而且也说明为啥 xt, h(t-1), ht, Ct, C(t-1) 向量维数相同
	注意 sigmod 得到的值不是无限接近0就是无限接近1
	注意 tanh   得到的值是 -1到1 之间
	后向传播:(BPTT算法) 参考
		   :https://blog.csdn.net/dark_scope/article/details/47056361
		   :https://blog.csdn.net/u012319493/article/details/52802302
rnn-lstm输入例子图:	
	          ht        h(t+1)        h(t+2)        h(t+3)
	       ___|__    ___|__        ___|__        ___|__
	C(t-1)-|    |-Ct-|    |-C(t+1)-|    |-C(t+2)-|    |-C(t+3)-....
	       |    |    |    |        |    |        |    |
	       |  A |    |  A |        |  A |        |  A |
	       |    |    |    |        |    |        |    |
	h(t-1)-|____|-ht-|____|-h(t+1)-|____|-h(t+2)-|____|-h(t+3)-....
	        |         |             |             |
	        xt       x(t+1)        x(t+2)        x(t+3)
rnn-lstm多测层网络例子图:	
		           .         .              .              .
		           .         .              .              .
		           .         .              .              .
		          h3t      h3(t+1)        h3(t+2)        h3(t+3)
		        __|___     __|___         __|___         __|___
		C3(t-1)-|    |-C3t-|    |-C3(t+1)-|    |-C3(t+2)-|    |-C3(t+3)-....
		        |    |     |    |         |    |         |    |
	第三层	        |  C |     |  C |         |  C |         |  C |
		        |    |     |    |         |    |         |    |
		h3(t-1)-|____|-h3t-|____|-h3(t+1)-|____|-h3(t+2)-|____|-h3(t+3)-....
		          |          |              |              |
		          h2t      h2(t+1)        h2(t+2)        h2(t+3)
		        __|___     __|___         __|___         __|___
		C2(t-1)-|    |-C2t-|    |-C2(t+1)-|    |-C2(t+2)-|    |-C2(t+3)-....
		        |    |     |    |         |    |         |    |
	第二层	        |  B |     |  B |         |  B |         |  B |
		        |    |     |    |         |    |         |    |
		h2(t-1)-|____|-h2t-|____|-h2(t+1)-|____|-h2(t+2)-|____|-h2(t+3)-....
		          |          |              |              |
		          h1t      h1(t+1)        h1(t+2)        h1(t+3)
		        __|___     __|___         __|___         __|___
		C1(t-1)-|    |-C1t-|    |-C1(t+1)-|    |-C1(t+2)-|    |-C1(t+3)-....
		        |    |     |    |         |    |         |    |
	第一层	        |  A |     |  A |         |  A |         |  A |
		        |    |     |    |         |    |         |    |
		h1(t-1)-|____|-h1t-|____|-h1(t+1)-|____|-h1(t+2)-|____|-h1(t+3)-....
		          |          |              |              |
		         xt        x(t+1)         x(t+2)         x(t+3)
一次典型的训练过程:
	例如,每次给 rnn 喂一句话,然后反向传导训练一次网络.的过程
	首先,每句话后假设有 30 个单词,少于30个词语的,也假作有30个单词,剩余用"空白"代替单词位置
	于是可以把句子 分成 30 个steps, 又把每个单词通过 Wx_plus_b 转成 输入向量.
	向量成员数 与 rnn 的 cell/hidden_unit的成员数一样
	假如我们 使用的 是一个 3层lstm 网络,如上图, 那么只有三个lstm单元 A B C
	第1刻  输入第一个单词 向量 xt 到 A单元, 
	       A单元 输出的  h1t     输入到 B单元,
	       B单元 输出的  h2t     输入到 C单元,
	       最后  C单元输出  h3t
	       每个 lstm 单元都需要的 h?(t-1),C?(t-1)可能是最新一次训练的到的值,也可能是第一次训练,所以随机值
	第2刻  输入单词向量  x(t+1)  到A单元, 
	       A单元 输出的  h1(t+1) 输入到 B单元,
	       B单元 输出的  h2(t+1) 输入到 C单元,
	       最后  C单元输出  h3(t+1)
	....
	第30刻 输入单词向量  x(t+29) 到A单元,
	       A单元 输出的 h1(t+29) 输入到B单元,
	       B单元 输出的 h2(t+29) 输入到C单元,
	       最后  C单元输出  h3(t+29)
	根据目的选取结果: h3t,h3(t+1),..h3(t+29) 组成的就是一个回答向量组
		然后再对每个向量执行 另一套 Wx_plus_b 转成 单词	最后组成答句.
		对比本来的答句 得到 误差 然后就可以执行反向传导 
		更新 3个lstm A,B,C,单元的参数,还有输入输出转化矩阵的参数
		特别注意, 还会更新 输入单词 转 输入向量的 Wx_plus_b
			  还会更新 输出h3(t+?) 转 输出单词的 Wx_plus_b
	也就是对于 lstm 3个单元 A,B,C 会连续 喂 30 遍数据,算出最后得到的结果,
	才会执行一次反向传导更新 A,B,C 单元里的内容 !!	
	还有一种情况,就是 只选取 最后一个 h3(t+29) 作为输出
		其他时间点的 h3(t+?) 直接丢弃
		根据 h3(t+29) 经输出转化矩阵得到的结果 与真实结果比较 得到误差
		最后,反向传导,更新 更新 3个lstm A,B,C,单元的参数,还有输入输出转化矩阵的参数

深入分析code9.py:
首先获取 batch 段数据, 每段数据由 TIME_STEPS 个数据单元组成,一次训练 喂 batch段数据
	所以每次输入数据的 self.xs 的 shape 是[(batch, n_steps, 输入数据单元)]
	把每个数据单元 转成 cell_size 维数据单元向量. 
	那么得到的转换后 的 输入数据 self.l_in_y 的 shape 是 [(batch, n_steps, cell_size)]
这里的 RNN 只用一层 lstm, 就是说只有一个 lstm单元
	初始化 lstm 单元:
		lstm_cell = tf.contrib.rnn.BasicLSTMCell(self.cell_size, forget_bias=1.0, state_is_tuple=True)
		BasicLSTMCell参数:
		self.cell_size 就是 C状态值维数,h状态值维数,输入数据单元向量维数,都是同一个数量
		forget_bias=1.0 如果忘记门是有偏移的 ft = sigmod{ (Wxf)(xt) + (Whf)(h(t-1)) + bias }
		这个就是那个偏移值参数
		state_is_tuple=True 意味着 每次 lstm单元 输出的状态值 是 [C状态值,h状态值] 的组合数组  
	初始化 lstm 单元的C状态值 和 h状态值 这里初始化都为零:           
		self.cell_init_state = lstm_cell.zero_state(self.batch_size, dtype=tf.float32)
		self.cell_init_state 模型状态值初始值,因为 state_is_tuple=True 的原因
			shape是 [batch_size,[C状态值维数+h状态值维数=]]
	设置 rnn 的训练过程:
		self.cell_outputs, self.cell_final_state = tf.nn.dynamic_rnn(
            		lstm_cell, self.l_in_y, initial_state=self.cell_init_state, time_major=False)
		这里 rnn 网络 是 单层的 lstm,
		rnn输入数据是 self.l_in_y
		rnn 状态的初始值,这里指的是lstm的状态初始值 self.cell_init_state
		time_major 跟 input数据有关系,其实是跟训练方式有关系:
			当 self.l_in_y的shape是 [(batch, n_steps, cell_size)],time_major=False
			当 self.l_in_y的shape是 [(n_steps, batch, cell_size)],time_major=True
		self.cell_outputs 是训练后得到的输出, shape是 (batch * n_steps, cell_size)
			也就说,每刻(step)喂一数据单元数组到rnn网络, 就有 cell_size维输出向量
			喂完一段数据,就是前后喂完 n_steps , 得到 n_steps个cell_size维输出向量
			喂了 batch 段数据, 就有 batch 个 (n_steps个cell_size维输出向量)
		self.cell_final_state 是得到最后模型状态值, shape与self.cell_init_state的一样
rnn网络输出数据处理:
	self.cell_outputs 的 shape是 (batch * n_steps, cell_size)
	我们要对 cell_size维输出向量 转成 我们处理的输出数据单元
	最后得到输出数据 self.pred , shape为: (batch * steps, 输出数据单元)
误差处理:
	经过训练得到的输出数据 和 真实输出数据 的误差
	    def compute_cost(self):
	        losses = tf.contrib.legacy_seq2seq.sequence_loss_by_example(
	            [tf.reshape(self.pred, [-1], name='reshape_pred')],
	            [tf.reshape(self.ys, [-1], name='reshape_target')],
	            [tf.ones([self.batch_size * self.n_steps], dtype=tf.float32)],
	            average_across_timesteps=True,
	            softmax_loss_function=self.ms_error,
	            name='losses'
	        )
	        with tf.name_scope('average_cost'):
	            self.cost = tf.div(
	                tf.reduce_sum(losses, name='losses_sum'),
	                self.batch_size,
	                name='average_cost')
	            tf.summary.scalar('cost', self.cost)
	    @staticmethod
	    def ms_error(labels, logits):
	        return tf.square(tf.subtract(labels, logits))
	tf.contrib.legacy_seq2seq.sequence_loss_by_example 是计算误差的一种方法,
	现在仅仅就具体例子分析,未打算系统说明这个函数的内容:
		参考:https://tensorflow.google.cn/api_docs/python/tf/contrib/seq2seq/sequence_loss
		    :https://blog.csdn.net/liuchonge/article/details/71424432
		第一个参数 logits, 一般是shape为 [batch,nsteps]
			特别要讨论下这里的shape:
			譬如,输入本来的shape 是 [batch, steps, 数据单元向量]:[50, 30, 20],
			必须reshape成二维数组: [batch, steps*数据单元向量]:[50, 600],才能作为logits输入,
			这时的 nsteps 就是 600 了!!
			现在,我们输入的 self.pred 的shape 是 (batch * steps, 输出数据单元):[50*20,1]
			对应着 logits 的 shape [batch,nsteps]: [50*20,1]
			而我们先把 self.pred reshape 成 一维数组 [batch]:[50*20*1] 再输入到 logits
			显然 logits 把输入的 self.pred:[batch]:[50*20*1]看成self.pred:[batch,nsteps]:[50*20,1]
			也就说 一维数组 [batch], 和二维数组 [batch,1] 并没有区别
		第一个参数 targets, 一般是shape为 [batch,nsteps]
			同样我们先把 self.ys reshape 成 一维数组 [batch]:[50*20*1] 再输入到 targets
			即,显然 targets 把输入的 self.ys:[batch]:[50*20*1]看成self.ys:[batch,nsteps]:[50*20,1]
		第三参数 权重 Weight 表示要对不同loss,的重视程度: 一般是shape为 [batch,nsteps]
			这里要求每个loss重视程度都一样,所以 weights 都为1
		第四参数:average_across_timesteps=True,表示求 timesteps 平均,后续解释
		第五参数:average_across_batch=True,表示求 batch 平均,注意:这里没有填入,没有使用,所以默认False后续解释
		第六参数:softmax_loss_function=None,如果使用默认loss单元函数,填None
		第七参数:name=None给这个 定义的计算误差的方法 命名,也可以不命名,为none
			图例解释: logits,target,weights 的shape是一样的!!!
			假如
			self.pred:[batch,n_step,数据单元向量]:[2,3,4]
				|[p111,p112,p113,p114]| ,|[p211,p212,p213,p214]|
				|[p121,p122,p123,p124]|  |[p221,p222,p223,p224]|			
				|[p131,p132,p133,p134]|  |[p231,p232,p233,p234]|
			self.ys:[batch,n_step,数据单元向量] 也一定是 [2,3,4]
				|[y111,y112,y113,y114]| ,|[y211,y212,y213,y214]|
				|[y121,y122,y123,y124]|  |[y221,y222,y223,y224]|			
				|[y131,y132,y133,y134]|  |[y231,y232,y233,y234]|
			假如被reshape成以下样子:
			self.pred 成 logits:[batch,nsteps]:[2,3*4]
				|p111,p112,p113,p114,p121,p122,p123,p124,p131,p132,p133,p134|
				,
 				|p211,p212,p213,p214,p221,p222,p223,p224,p231,p232,p233,p234|
			self.ys 成 targets:[batch,nsteps]:[2,3*4]
				|y111,y112,y113,y114,y121,y122,y123,y124,y131,y132,y133,y134| 
				,
				|y211,y212,y213,y214,y221,y222,y223,y224,y231,y232,y233,y234|
			那么我们要求 weight 也是这样子:
				|w11,w12,w13,w14,w15,w16,w17,w18,w19,w110,w111,w112|
				,	
				|w21,w22,w23,w24,w25,w26,w27,w28,w29,w210,w211,w212|
			如果 softmax_loss_function=None ,就使用default-loss单元函数,
				如果不想使用 default-loss单元函数,就得设置 softmax_loss_function
				假如我们使用的误差是 均方差:mean squared error(MSE), 
				有必要说明 均方误差 MSE = E( (y-y')^2 ) =  ( (y1-y'1)^2 + .. + (yn-y'n)^2 ) /n
				我们设置的 softmax_loss_function = ms_error(labels, logits) 
				而函数 ms_error(labels, logits) 只实现了 差的平方 (y-y')^2,
				即只有 tf.square(tf.subtract(labels, logits))
				而E()部分,"即(..+..+..+..)/n)" 并不在 ms_error 里实现.
				而是 tf.contrib.legacy_seq2seq.sequence_loss_by_example 透过
				average_across_batch 或者 average_across_timesteps 条件实现
				一般 average_across_timesteps,  average_across_batch 只能选其中一个为True
			假如 average_across_timesteps=True 表示平均 nsteps 这维
			那么:
			这个定义了的误差方法得到的 结果
			A =  ( w11*(p111-y111)^2 + w12*(p112-y112)^2 + ... + w112*(p134-y134)^2 ) / (3*4)
			B =  ( w21*(p211-y211)^2 + w22*(p212-y212)^2 + ... + w212*(p234,y234)^2 ) / (3*4)
			当weights数组的值 w?? 都为 1 时 , 表示所有 差的平方 都重视
			这样就实现了在 nsteps维上 的 均方差 MSE = E( (y-y')^2 )
			当weights数组的值 w?? 不一样时 , 表示部分 差的平方 得到重视 
			这样就实现了在 nsteps维上 的 配置了权重的均方差 MSE = E( (y-y')^2 )
			最后得到 一个 二维向量(一维数组) [A,B]
			假如 average_across_batch=True 表示平均 batch 这维
			那么: 
			这个定义了的误差方法得到的 结果
			a =  ( alo(p111,y111)*w11 + alo(p211,y211)*w21 ) / 2
			b =  ( alo(p112,y112)*w12 + alo(p212,y212)*w22 ) / 2
			c =  ( alo(p113,y113)*w13 + alo(p213,y213)*w23 ) / 2
			b =  ( alo(p114,y114)*w14 + alo(p214,y214)*w24 ) / 2
			....
			l =  ( alo(p134,y134)*w112 + alo(p234,y234)*w212 ) / 2
			当weights数组的值 w?? 都为 1 时 , 表示所有 差的平方 都重视
			这样就实现了在 batch维上 的 均方差 MSE = E( (y-y')^2 )
			当weights数组的值 w?? 不一样时 , 表示部分 差的平方 得到重视 
			这样就实现了在 batch维上 的 配置了权重的均方差 MSE = E( w(y-y')^2 )
			最后得到 一个 十二维向量(一维数组) [a,b,c,d,e,f,g,h,i,j,k,l]
	回到 losses = tf.contrib.legacy_seq2seq.sequence_loss_by_example()
		self.pred:[batch]:[50*20*1] -> logits :[batch,nsteps]:[50*20*1,1]
		self.ys  :[batch]:[50*20*1] -> targets:[batch,nsteps]:[50*20*1,1]
		weights  :[batch]:[50*20*1] -> weights:[batch,nsteps]:[50*20*1,1]
		即 |p1,p2,p3,,,,p100|
		   |y1,y2,y3,,,,y100|
		   |w1,w2,w3,,,,w100|
		而且 w?? 的值都为 1
		我们斌不打算使用 tf.contrib.legacy_seq2seq.sequence_loss_by_example 默认方式求误差,
		同时 我们通过 均方差的方式 求出,误差, 并不是 交叉熵的方式,
		所以我们设置 softmax_loss_function=self.ms_error,只实现了 差的平方 (y-y')^2,
		我们设置了 average_across_timesteps=True,平均 nsteps 这维
		因为 nsteps 为 1,即只有一个成员
		于是,这个定义了的误差方法得到的 结果
		l1 = ( (p1-y1)^2 ) / 1
		l2 = ( (p2-y2)^2 ) / 1
		l3 = ( (p3-y3)^2 ) / 1
		...
		l1000 = ( (p100-y100)^2 ) / 1
		即得到一个 100维向量 [l1,l2,l3,,,l100]
		tf.contrib.legacy_seq2seq.sequence_loss_by_example 输出的 losses:[l1..l1000],
		是针对这个函数意义上的batch:1000 ,
		实际数据只有 50个batch, 每个batch有20个step,
		我们使用这个函数就预处理为把每个step都当成batch处理
	计算cost:  self.cost = tf.div(
	                tf.reduce_sum(losses, name='losses_sum'),
	                self.batch_size,
	                name='average_cost')
	            tf.summary.scalar('cost', self.cost)
		tf.reduce_sum: 把losses的1000个成员都加起来,
		tf.div: 然后除以 self.batch_size:50,
		最后得到的就是 平均 cost ,是一个值, 
		相当于每个 batch 的 20个Time_steps的loss加起来得到一个cost
		把50个cost加起来,再平均!!!
最后分析训练过程!!
	第一次:	feed_dict = {
	                    model.xs: seq,
	                    model.ys: res,
	                    # create initial state
	            }
		_, cost, state, pred = sess.run(
        	    [model.train_op, model.cost, model.cell_final_state, model.pred],
        	    feed_dict=feed_dict)
		先执行model.train_op,
			执行 tf.train.AdamOptimizer(LR).minimize(self.cost)
			需要 self.cost,
			所以得先执行 tf.div(.. losses, ,,self.batch_size),
			需要 losses 和 self.batch_size, self.batch_size已知,losses,未知
			所以得先执行 tf.contrib.legacy_seq2seq.sequence_loss_by_example
			需要 self.pred, self.ys, self.batch_size, self.n_steps	 
			self.batch_size, self.n_steps 已知,
			self.ys 是占位符号: 传入了 model.ys: res
			要取得self.pred,所以得先执行tf.matmul,
			需要 l_out_x, Ws_out, bs_out
			Ws_out, bs_out 是 被定义为要 训练 variable
			要取得 l_out_x,即取得 self.cell_outputs,
			所以得先执行 tf.nn.dynamic_rnn
			需要 lstm_cell, self.l_in_y, self.cell_init_state,
			lstm_cell, self.cell_init_state, 已知
			要取得 self.l_in_y, 即取得 l_in_x ,即取得 self.xs
			self.xs 是占位符号: 传入了 model.xs: seq
			最后,再最后一步步往前执行,
			回到 tf.train.AdamOptimizer(LR).minimize(self.cost) 执行反向传导更新参数
			最后执行完毕,返回 数据 放入 _
		然后执行model.cost,
			得先执行 tf.div(.. losses, ,,self.batch_size),
			需要 losses 和 self.batch_size, self.batch_size已知,losses,未知
			所以得先执行 tf.contrib.legacy_seq2seq.sequence_loss_by_example
			需要 self.pred, self.ys, self.batch_size, self.n_steps	 
			self.batch_size, self.n_steps 已知,
			self.ys 是占位符号: 传入了 model.ys: res
			要取得self.pred,所以得先执行tf.matmul,
			需要 l_out_x, Ws_out, bs_out
			Ws_out, bs_out 是 被定义为要 训练 variable
			要取得 l_out_x,即取得 self.cell_outputs,
			所以得先执行 tf.nn.dynamic_rnn
			需要 lstm_cell, self.l_in_y, self.cell_init_state,
			lstm_cell, self.cell_init_state, 已知
			要取得 self.l_in_y, 即取得 l_in_x ,即取得 self.xs
			self.xs 是占位符号: 传入了 model.xs: seq
			最后,再最后一步步往前执行,直到完毕,返回 数据 放入 cost
		然后执行 model.cell_final_state,
			得先执行 tf.nn.dynamic_rnn
			需要 lstm_cell, self.l_in_y, self.cell_init_state,
			lstm_cell, self.cell_init_state, 已知
			要取得 self.l_in_y, 即取得 l_in_x ,即取得 self.xs
			self.xs 是占位符号: 传入了 model.xs: seq
			最后,再最后一步步往前执行,
			tf.nn.dynamic_rnn 会得到 self.cell_outputs, self.cell_final_state
			但是 sess.run 只想得到 self.cell_final_state
			完毕后只返回 self.cell_final_state 数据 放入 state
		最后执行 model.pred
			得先执行tf.matmul,
			需要 l_out_x, Ws_out, bs_out
			Ws_out, bs_out 是 被定义为要 训练 variable
			要取得 l_out_x,即取得 self.cell_outputs,
			所以得先执行 tf.nn.dynamic_rnn
			需要 lstm_cell, self.l_in_y, self.cell_init_state,
			lstm_cell, self.cell_init_state, 已知
			要取得 self.l_in_y, 即取得 l_in_x ,即取得 self.xs
			self.xs 是占位符号: 传入了 model.xs: seq
			最后,再最后一步步往前执行,直到完毕,返回 数据 放入 pred
	第一次以后的:feed_dict = {
	                model.xs: seq,
	                model.ys: res,
	                model.cell_init_state: state    # use last state as the initial state for this run
	            }
		_, cost, state, pred = sess.run(
        	    [model.train_op, model.cost, model.cell_final_state, model.pred],
        	    feed_dict=feed_dict)
		这时 特别注意到 model.cell_init_state: state
		因为,model.cell_init_state 不像 xs,ys 被定义为 placeholder 占位符号, 
		也不像 weight, bias,那种 被定义为 要被训练的 variable
		初始化时,是这样子的
			self.cell_init_state = lstm_cell.zero_state(self.batch_size, dtype=tf.float32)
		应该像 self.cell_size 那样的变量, model里内部初始化,内部赋值的
		但现在, model.cell_init_state: state 的操作,
		相当于, self.cell_init_state  被外部赋值了, 有占位符号那样的作用.
		也就说模型里 非 variable 和 placeholder 变量, 其实也有 placeholder的作用
		可以 ,被外部赋值,取代原有值 !!

深入分析code10.py
	和code9.py的区别是, 输入的是一张28*28图片
	一张图片 分成 28 条数据,即分28次喂入模型, 每条数据 28个像素点
	每次喂 1 条数据, 每条数据 转换成 128维向量 再喂入单层 lstm 模型
	只取 最后一次,即第28次喂入数据后 得到的 128维输出向量
	再 转化 得到 10维向量 与 图片原本指向的 10维向量做比较
cell 状态向量 和hidden_unit 状态向量 初始值问题:
	由于每张图片输入rnn 模型过程中,都与另一张输入不一样
	所以前一张的图片训练得到 的 cell 状态向量 和hidden_unit 状态向量
	不需要传导到 下一张图片的识别,所以每次训练一张图片用到的 
		 cell 状态向量 和hidden_unit 状态向量的初始值都为 0 !!
只获取第28次喂入数据后 得到的 128维输出向量 做比较的问题 
	这里的重点是 如何取第28次得到 128维输出向量:
    	outputs, final_state = tf.nn.dynamic_rnn(cell, X_in, initial_state=init_state, time_major=False)
	输出的 outputs 的shape [batch,steps,输出向量维数]:[128,28,128]
		即包含 128张图片 同时喂入模型 得到的128个结果,
		每个结果有28条 128维输出向量, 
		每条输出向量 对应 每次(step) 喂入的一个输入向量
		现在我们要 每个结果 的 最后一条 128维输出向量, 即第28次喂入输出向量 得到的 128维输出向量
		outputs = tf.unstack(tf.transpose(outputs, [1,0,2]))
		outputs 是一个 三维数组 即shape [batch,steps,输出向量维数]
		batch 这一维的 标记为0,
		steps 这一维的 标记为1,
		输出向量维数 这一维的 标记为2,
		tf.transpose(outputs, [1,0,2]:表示把 batch steps 两维转换
		即outputs 变成了 [steps,batch,输出向量维数]
		图例直观解析
			假如 outputs本来是这样的 shape:[3,2,5]:
			[
	
				[
					[1,1,1,1,1,1]
					[2,2,2,2,2,2]		
				]
				,
	
				[
					[3,3,3,3,3,3]
					[4,4,4,4,4,4]		
				]
				,
	
				[
					[5,5,5,5,5,5]
					[6,6,6,6,6,6]		
				]
				,
			]
			transpose(outputs, [1,0,2] -> outputs shape:[2,3,5]:
			只换第 batch steps 维, 
			输出向量维数 这维内容不变, 这维可以简单标记:
				A = [1,1,1,1,1,1]
				B = [3,3,3,3,3,3]
				C = [5,5,5,5,5,5]
				D = [2,2,2,2,2,2]		
				E = [4,4,4,4,4,4]		
				F = [6,6,6,6,6,6]
	    		outputs 简单记为:
			[[A,B]
			 [C,D]
			 [E,F]]
			换第 batch steps 后:
			[[A,C,E]
			 [B,D,F]]
			把标记 ABCDEF换换回去就得:
			[
	
				[
					[1,1,1,1,1,1]
					[3,3,3,3,3,3]
					[5,5,5,5,5,5]
	
				]
				,
	
				[
					[2,2,2,2,2,2]		
					[4,4,4,4,4,4]		
					[6,6,6,6,6,6]		
				]
				,
			]		
	
		解构数组:tf.unstack()
		没有其他参数,默认解构最前的一维,即 steps维,即第0维
		解构图示:
			假如 outputs是这样子的:
			outputs = [
		
					[
						[1,1,1,1,1,1]
						[2,2,2,2,2,2]		
					]
					,
	
					[
						[3,3,3,3,3,3]
						[4,4,4,4,4,4]		
					]
					,
		
					[
						[5,5,5,5,5,5]
						[6,6,6,6,6,6]		
					]
	
				]
			tf.unstack(outputs) 后,得到
			outputs[0] = [ [1,1,1,1,1,1]
				       [2,2,2,2,2,2] ]
			outputs[1] = [ [3,3,3,3,3,3]
				       [4,4,4,4,4,4] ]
			outputs[2] = [ [5,5,5,5,5,5]
				       [6,6,6,6,6,6] ]
			output 从 变量 变成了 数组变量 !!
			另外 outputs[-1] 等于 outputs[2], outputs[-1]表示数组的变量的最后一个成员!!
		因此 真正outputs tf.transpose转换后 得 shape:[steps,batch,输出向量维数]
		说明最后一steps 的 [batch,输出向量维数]:[128,128]
		放着是 128 条 第28次喂入输出向量 得到的 128维输出向量 
		我们只要最后一steps 的 [batch,输出向量维数]:[128,128] 
		所以执行了 tf.unstack 解构,
		outputs变成了 数组变量,我们只有这个数组变量最后一个,即 outputs[-1]
		results = tf.matmul(outputs[-1], weights['out']) + biases['out']    # shape = (128, 10)
		然后 每条最后的128维输出向量 经过同一个 [128,10]的转换矩阵 得到 一条 10维one-hot向量
		得到的 results 是含有 128 条 10维one-hot向量,
		即128张图片 经网络后得到 128条  one-hot向量
		最后对比 one-hot向量 与图片实际的 标记向量, 得到误差,然后反向传导更新网络参数
	由于我们只需要 最后一条 128维输出向量, 即第28次喂入输出向量 得到的 128维输出向量
	而这个内容 跟 final_state 这个数组变量的 第2个成员 final_state[1] 是一样的!
	results = tf.matmul(final_state[1], weights['out']) + biases['out']
	同样得到含有 128 条 10维one-hot向量, 的 results
final_state 这个内容 是lstm训练后得到的 c状态值 和 h状态值!!  	
	final_state[0] 是 c状态值
	final_state[1] 是 h状态值,也等于当前的输出向量	

		












北京大学 沈阳 性侵犯女学生
西安交大 导师周筠施压杨宝德至其跳楼
上海西南模范中学包庇 市三好学生李明泰猥琐女生
武汉理工大学包庇 王攀教授奴役陶崇明逼跳楼,大学威逼其胞姐道歉











??????????????????






20180310:
更换高速的ubuntu16源:
	sudo gedit /etc/apt/sources.list
	注释其他源，添加：
		# 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释
		deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial main restricted universe multiverse
		# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial main restricted universe multiverse
		deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse
		# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse
		deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse
		# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse
		deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-security main restricted universe multiverse
		# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-security main restricted universe multiverse		
		# 预发布软件源，不建议启用
		# deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse
		# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse
	sudo apt-get update
	换源要谨慎,机子中途换源会容易与以前的软件冲突!!所以要么一开始就换源,要么不换

更换高速的pip3源：
	1、在用户目录下（/home/XXX）创建.pip文件夹，并创建pip.conf文件
	2、在pip.conf下输入：（注意：这里更换的是阿里云镜像源）
		[global]
		trusted-host = mirrors.aliyun.com
		index-url = http://mirrors.aliyun.com/pypi/simple
	3、 sudo apt-get update

python 虚拟环境：
	pip3 install virtualenv
	sudo pip3 install virtualenvwrapper	//虚拟环境管理模块
	mkdir $HOME/.local/virtualenvs	  //创建虚拟环境管理目录 (不要加sudo)
	sudo gedit ~/.bashrc 	//末尾添加:
		# by william
		# setting about virtualenvwrapper
		export VIRTUALENVWRAPPER_PYTHON=/usr/bin/python3
		export VIRTUALENV_USE_DISTRIBUTE=1        #  总是使用 pip/distribute                                        
		export WORKON_HOME=$HOME/.local/virtualenvs       # 所有虚拟环境存储的目录
		if [ -e $HOME/.local/bin/virtualenvwrapper.sh ];then
		   source $HOME/.local/bin/virtualenvwrapper.sh                                                
		else if [ -e /usr/local/bin/virtualenvwrapper.sh ];then
		         source /usr/local/bin/virtualenvwrapper.sh
		     fi
		fi
		export PIP_VIRTUALENV_BASE=$WORKON_HOME
		export PIP_RESPECT_VIRTUALENV=true
	source ~/.bashrc	//启动 virtualenvwrapper

	简单创建虚拟环境:
		virtualenv aaa  	//创建一个独立环境空间aaa,在当前文件夹建立一个aaa文件夹,
					//这种默认情况下,会把默认的解释机,和对应的默认软件库加入环境aaa
		virtualenv --no-site-packages bbb //创建一个独立环境空间aaa,在当前文件夹建立一个aaa文件夹,
							  //这情况下,不会把默认的软件库加入环境bbb,
		virtualenv ccc --python=python2   //创建一个独立环境空间ccc,在当前文件夹建立一个ccc文件夹,
						  //这种默认情况下,会把默认的软件库,和默认的解释机加入环境ccc
		启用虚拟环境
		cd ccc	//进入环境文件夹
		source ./bin/activate
		cd ~ //进入要执行的项目的文件夹,例如~
		查看当前状态
		(ccc) kingders@kingders-ThinkPad-T420:~$ 	//先可以直观看到(ccc)前缀,就是说现在处于 ccc 的独立python 工作环境里下
		退出虚拟环境
		deactivate
	通过管理套件创建虚拟环境:
		mkvirtualenv aaa -p python3	//创建
		workon aaa		//进入
		workon			//查看
		deactivate		//退出
		




!@!



20180319
IndentationError: expected an indented block
	这个问题要注意缩进！！






20xxx
python 基础:
np.newaxis 使用: (import numpy as np)
	a=np.array([1,2,3,4,5])
	print a.shape
	print a
	输出结果
		(5,)
		[1 2 3 4 5]

	a=np.array([1,2,3,4,5])
	b=a[np.newaxis,:]
	c=a[:np.newaxis]
	print (a.shape,b.shape,c.shape)
	print (b.shape[1])
	print (a)
	print (b)
	print (c)
	输出结果:
		(5,) (1, 5) (5, 1)
		5
		[1 2 3 4 5]
		[[1 2 3 4 5]]
		[[1]
		 [2]
		 [3]
		 [4]
		 [5]]
range()
	一个特殊函数.
	range(start, stop[, step])
	    start: 计数从 start 开始。默认是从 0 开始。例如range（5）等价于range（0， 5）;
	    stop: 计数到 stop 结束，但不包括 stop。例如：range（0， 5） 是[0, 1, 2, 3, 4]没有5
	    step：步长，默认为1。例如：range（0， 5） 等价于 range(0, 5, 1)
	>>>range(10)        # 从 0 开始到 10
	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
	>>> range(1, 11)     # 从 1 开始到 11
	[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
	>>> range(0, 30, 5)  # 步长为 5
	[0, 5, 10, 15, 20, 25]
	>>> range(0, 10, 3)  # 步长为 3
	[0, 3, 6, 9]
	>>> range(0, -10, -1) # 负数
	[0, -1, -2, -3, -4, -5, -6, -7, -8, -9]
	>>> range(0)
	[]
	>>> range(1, 0)
	[]
	以下是 range 在 for 中的使用，循环出runoob 的每个字母:
	>>>x = 'runoob'
	>>> for i in range(len(x)) :
	...     print(x[i])
	... 
	r
	u
	n
	o
	o
	b
	>>>
	Python3.x 中 range() 函数返回的结果是一个整数序列的对象，而不是列表。
	>>> type(range(10))
	<class 'range'>
	当你 help(range) 时会看到：
	Return an object...
	所以，不是列表，但是可以利用 list 函数返回列表，即：
	>>> list(range(10))
	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
	不是列表的证据
	a = range(9)
	print (a)
	输出结果
		range(0, 9)
np.random.choice
	import numpy as np
	# 参数意思分别 是从a 中以概率P，随机选择3个, p没有指定的时候相当于是一致的分布
	# a可以是一个数表示从0到a-1之间选,也可以是一个一维向量,从向量中选第几个成员
	a1 = np.random.choice(a=5, size=3, replace=False, p=None)
	print(a1)
	# 非一致的分布，会以多少的概率提出来
	a2 = np.random.choice(a=5, size=3, replace=False, p=[0.2, 0.1, 0.3, 0.4, 0.0])
	print(a2)
	# replacement 代表的意思是抽样之后还放不放回去，如果是False的话，那么出来的三个数都不一样，如果是	
	True的话， 有可能会出现重复的，因为前面的抽的放回去了。	
append()
	给数组添加一个新成员!
np.zeros_like
	W_update=np.zeros_like(W);
	构造一个矩阵W_update，其维度与矩阵W一致，并为其初始化为全0


20180401
Tensorflow 常用:
batch 机制 的功能作用的描述 !!
	整理下认识,一般我们求出实际y与训练结果Y'的误差来反向传导更新模型
	而误差,我们可以取,方差,标准差,交叉熵什么的,
	注意,这些是 一个样本下的误差,我们说的方差交叉熵什么的,是基于同一个样本得到的,
	例如,图片a实际标签向量y:[y1,y2,..y10],训练结果标签向量Y':[y'1,y'2,..y'10].
	图片a的方差,是:	{ (y1-y'1)^2 + ..+ (y10 - y'10)^2 } /10
	那么cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction),reduction_indices=[1]))如何理解,
	我们知道,在一次训练里,输入了100张图片,即100张图片通过同样的 weights,blase,得到各自的训练结果标签向量Y'
	tf.reduce_mean括号里算出的是 每张图片自己的交叉熵,
	交叉熵指,有10个特征值的标签向量,把每个特征值的混乱程度加到一块得到的一个值
	然后通过 tf.reduce_mean,把 100 个交叉熵 算出一个 平均值,
	我们最后才通过这个 交叉熵平均值 来反向传导更新一次学习模型的参数weights和biase
	即我们是通过100张图片得到100个交叉熵,然后取100个交叉熵的平均值
	并不是通过100张图片得到1个交叉熵,而且也不知道如何得到
	也不是100个交叉熵,每个交叉熵都反向传导更新模型一次,即不是在一次训练里更新100次模型!!!
	我们只是取 100个交叉熵的平均值来更新 1 次模型
	所以.模型中的 batch 设置为100,那个100,就是这种意思
参数:
tf.Variable.init(initial_value, trainable=True, collections=None, validate_shape=True, name=None)
	initial_value 	所有可以转换为Tensor的类型 	变量的初始值
	trainable 	bool 	如果为True，会把它加入到GraphKeys.TRAINABLE_VARIABLES，才能对它使用Optimizer
	collections 	list 	指定该图变量的类型、默认为[GraphKeys.GLOBAL_VARIABLES]
	validate_shape 	bool 	如果为False，则不进行类型和维度检查
	name 		string 	变量的名称，如果没有指定则系统会自动分配一个唯一的值
从正态分布中输出随机值:	
tf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None) 
	shape: 一维的张量，也是输出的张量。
	mean: 正态分布的均值。
	stddev: 正态分布的标准差。
	dtype: 输出的类型。
	seed: 一个整数，当设置之后，每次生成的随机数都一样。
	name: 操作的名字。
	例子:
	|2,6,7| = tf.random_normal([2.3])
	|9,1,4|
占位符号:
tf.placeholder(dtype, shape=None, name=None)
	此函数可以理解为形参，用于定义过程，在执行的时候再赋具体的值
	dtype：数据类型。常用的是tf.float32,tf.float64等数值类型
	shape：数据形状。默认是None，行不定，比如[2,3]表示列是3，行是2, [None, 3]表示列是3，行不定
	name：名称。
二维数组(二维矩阵)的叠加函数	
tf.reduce_sum()
	例子1:
	[2,2,2] = tf.reduce_sum(|1,1,1|, reduction_indices=[0] )
				|1,1,1|
	|3| = tf.reduce_sum(|1,1,1|, reduction_indices=[1] )
	|3|		    |1,1,1|	
	例子2:
	6 = tf.reduce_sum(|1,1,1|, reduction_indices=[0,1] )
			  |1,1,1|	
	就是先reduction_indices=[0]得到[2,2,2],再reduction_indices=[1] 得到 6 

二维数组的乘函数
tf.matmul()
	a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3]) => [[1. 2. 3.]
	                                                      [4. 5. 6.]]

	b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2]) => [[7. 8.]
	                                                         [9. 10.]
	                                                         [11. 12.]]
	c = tf.matmul(a, b) => [[58 64]
        	                [139 154]]
二维数组的 2次方
tf.square()
	|1,  4, 9| = tf.reduce_sum(|1,2,3|)
	|16,25,36|		   |4,5,6|
二维数组的 平均值:
tf.reduce_mean()
	  2.5 = tf.reduce_mean(|1,2|)
			       |3,4|
	|2,3| = tf.reduce_mean(|1,2|, 0)
			       |3,4|
	|1.5| = tf.reduce_mean(|1,2|, 1)
	|3.5|		       |3,4|

二维数组的 最大值位置:
tf.argmax(|1, 2, 3|,0)=[3,3,1]	//数组从选出 每列中最大值的位置 (从0数)
          |2, 3, 4|
          |5, 4, 3|
          |8, 7, 2|
tf.argmax(|1, 2, 3|,1)=[2, 2, 0, 0] //数组从选出 每行中最大值的位置 (从0数)
          |2, 3, 4|
          |5, 4, 3|
          |8, 7, 2|
tf.cast 类型转换 函数:
	tf.cast([2, 3, 4], tf.float32) //把一维数组的每个int值转换为float值

tf.reduce_sum
	# 'x' is [[1, 1, 1]
	#         [1, 1, 1]]
	tf.reduce_sum(x) ==> 6
	tf.reduce_sum(x, 0) ==> [2, 2, 2]
	tf.reduce_sum(x, 1) ==> [3, 3]
	tf.reduce_sum(x, 1, keep_dims=True) ==> [[3], [3]]
	tf.reduce_sum(x, [0, 1]) ==> 6

按正太分布随机生成 多维数组:
tf.truncated_normal(shape, mean, stddev)
	shape表示生成张量的维度，mean是均值，stddev是标准差。这个函数产生正太分布，均值和标准差自己设定。
	例子: 
	|1.95758033,-0.68666345,-1.83860338, 0.78213859|= tf.truncated_normal(shape=[2,4], mean=0, stddev=1) 
        |0.38035342, 0.57904619,-0.57145643,-1.22899497|
生成tensor：
	tf.zeros(shape, dtype=tf.float32, name=None)	//零矩阵
	tf.zeros_like(tensor, dtype=None, name=None)
	tf.constant(value, dtype=None, shape=None, name='Const') //值都为value的矩阵
	tf.fill(dims, value, name=None)
	tf.ones_like(tensor, dtype=None, name=None)
	tf.ones(shape, dtype=tf.float32, name=None)
生成序列
	tf.range(start, limit, delta=1, name='range')
	tf.linspace(start, stop, num, name=None)
生成随机数
	tf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)
	tf.truncated_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)
	tf.random_uniform(shape, minval=0.0, maxval=1.0, dtype=tf.float32, seed=None, name=None)
	tf.random_shuffle(value, seed=None, name=None)
卷积操作
tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, name=None)
	第一个参数input：指需要做卷积的输入图像，它要求是一个Tensor，
		具有[batch, in_height, in_width, in_channels]这样的shape，
		具体含义是[训练时一个batch的图片数量, 图片高度, 图片宽度, 图像通道数]，
		即这个参数是一个多维数组!!
		注意这是一个4维的Tensor，要求类型为float32和float64其中之一
		然而实际操作是 [batah,in_height*in_width*in_channels]二维数组(二维tensor)
	第二个参数filter：相当于CNN中的卷积核，它要求是一个Tensor，
		具有[filter_height, filter_width, in_channels, out_channels]这样的shape，
		即这个参数是一个多维数组!!
		具体含义是[卷积核的高度，卷积核的宽度，图像通道数，输出图像通道数]，
		要求类型与参数input相同，有一个地方需要注意，第三维in_channels，就是参数input的第四维
		注意,这个参数的含义
		譬如 input是一张28*28有32个通道的图片,即有32张特征图片, 
			有一个 shape=[2,2,32,64]的filter多维向量 ,即说明
			shape=[2,2,32,64]说明 输入的这张图片是有32个通道的,即有32张特征图片的,
			shape=[2,2,32,64]也要求卷积输出后的图片只有有64个通道的,即有64张特征图片的
			shape=[2,2,32,64]也说明,filter里共有 32x64 个独立 2x2 的卷积核
			所以按道理卷积出来的 通道应该有 32x64 个,而不是64 个
			所以这里卷积的过程与我们理论学习的过程有些详细的区别!!
			理论上,输出64通道的话,32个输入通道,每个分配两个卷积核就可以了!
			而这里是每个分配 64 个卷积核,一个通道就能卷积出64这个特征图片了,
			但是接着把每个通道卷积出的64个特征图片,求平均得出2个平均特征图片
			每个通道得出2个平均特征图片,32个就得出64个,
			这64个平均特征图片就凑成最后要输出的64个通道
	第三个参数strides：卷积时在图像每一维的步长，这是一个一维的向量，长度4
		由于对于图片,只有长宽,所以,只用到中间两个值表示步数,其他为1,
		如 strides=[1, 4, 4, 1],表示长宽步长都为4,
		即不在batch和channels上做卷积
	第四个参数padding：string类型的量，只能是"SAME","VALID"其中之一
		padding可'SAME' 可`VALID, SAME表示卷积所有像素,VALID则不是
		VALID:
		 1 2 3 4 5 6 7 8 9 10 11 12 13	多出的12,13不卷积
		|___________|
			  |_____________|
		SAME:
		0 |1 2 3 4 5 6 7 8 9 10 11 12 13| 0 0	卷积所有像素,不够的补0
		|___________|                   |
			  |____________|        |
	                            |________________|	
	第五个参数：use_cudnn_on_gpu:bool类型，是否使用cudnn加速，默认为true
	做后一个是当前卷积操作的名字
		结果返回一个Tensor，这个输出，
		就是我们常说的feature map，shape仍然是[batch, height, width, channels]这种形式。
		即是下一层的input,下一层的卷积的输入图像
池化操作:
tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')	
	建立一个池化操作
	池化输出的是邻近区域的概括统计量，一般是矩形区域。池化有最大池化、平均池化、滑动平均池化、L2范数池化等
	池化与卷积意义上不一样,虽然操作上有很多地方相同
	第一个参数value：需要池化的输入，一般池化层接在卷积层后面，
		所以输入通常是feature map，依然是[batch, height, width, channels]这样的shape
	第二个参数ksize：池化窗口的大小，取一个四维向量，一般是[1, height, width, 1]，
		因为我们不想在batch和channels上做池化，所以这两个维度设为了1
	第三个参数strides：和卷积类似，窗口在每一个维度上滑动的步长，一般也是[1, stride,stride, 1]
	第四个参数padding：和卷积类似，可以取'VALID' 或者'SAME'	

tf.transpose(outputs, [1,0,2]:表示把 batch steps 两维转换
	即outputs 变成了 [steps,batch,输出向量维数]
	图例直观解析
		假如 outputs本来是这样的 shape:[3,2,5]:
		[

			[
				[1,1,1,1,1,1]
				[2,2,2,2,2,2]		
			]
			,

			[
				[3,3,3,3,3,3]
				[4,4,4,4,4,4]		
			]
			,

			[
				[5,5,5,5,5,5]
				[6,6,6,6,6,6]		
			]
			,
		]
		transpose(outputs, [1,0,2] -> outputs shape:[2,3,5]:
		只换第 batch steps 维, 
		输出向量维数 这维内容不变, 这维可以简单标记:
			A = [1,1,1,1,1,1]
			B = [3,3,3,3,3,3]
			C = [5,5,5,5,5,5]
			D = [2,2,2,2,2,2]		
			E = [4,4,4,4,4,4]		
			F = [6,6,6,6,6,6]
    		outputs 简单记为:
		[[A,B]
		 [C,D]
		 [E,F]]
		换第 batch steps 后:
		[[A,C,E]
		 [B,D,F]]
		把标记 ABCDEF换换回去就得:
		[

			[
				[1,1,1,1,1,1]
				[3,3,3,3,3,3]
				[5,5,5,5,5,5]

			]
			,

			[
				[2,2,2,2,2,2]		
				[4,4,4,4,4,4]		
				[6,6,6,6,6,6]		
			]
			,
		]		

解构数组:tf.unstack()
	没有其他参数,默认解构最前的一维,即 steps维,即第0维
	解构图示:
		假如 outputs是这样子的:
		outputs = [
	
				[
					[1,1,1,1,1,1]
					[2,2,2,2,2,2]		
				]
				,

					[
					[3,3,3,3,3,3]
					[4,4,4,4,4,4]		
				]
				,

				[
					[5,5,5,5,5,5]
					[6,6,6,6,6,6]		
				]

			]
		tf.unstack(outputs) 后,得到
		outputs[0] = [ [1,1,1,1,1,1]
			       [2,2,2,2,2,2] ]
		outputs[1] = [ [3,3,3,3,3,3]
			       [4,4,4,4,4,4] ]
		outputs[2] = [ [5,5,5,5,5,5]
			       [6,6,6,6,6,6] ]
		output 从 变量 变成了 数组变量 !!



>>>>>>>>>>>>>>>>

20180401
回归:
/home/william/AI/machine learning/回归/code.py
通过散点数据训练一个模型,找到散点的落入规律,
这里找到的规律是,散点落入一个二次函数范畴的规律,也就说通过模型得到的散点建立的曲线越来越像二次曲线 
先人为制作一个 二次函数曲线 的散点图,作为样本参数 这里是建立 300 个散点
	x_data = np.linspace(-1, 1, 300, dtype=np.float32)[:, np.newaxis]
	noise = np.random.normal(0, 0.05, x_data.shape).astype(np.float32)
	y_data = np.square(x_data) - 0.5 + noise  
再通过 matplotlib.pyplot 显示散点图!! 
	plt.scatter(x_data, y_data)	
	plt.show()	//这里训练和显示散点图是冲突的,要训练,就要屏蔽显示散点图
训练模型建立:
//定义如何建立层
def add_layer(inputs, in_size, out_size, activation_function=None): //定义如何建立层
	重点如何地定义变量定义:例如:
	Weights = tf.Variable(tf.random_normal([in_size, out_size]))
	tf.random_normal 得随机变量值,这里出来是随机二维数组 in_size*out_size,其他参数默认,
	tf.Variable 把这个随机二位数组值变成 tensorflow变量
//定义训练模型的输入输出变量占位符
	xs = tf.placeholder(tf.float32, [None, 1])
	ys = tf.placeholder(tf.float32, [None, 1])
	//输入的xs.输出的ys是一维数组,而[None, 1]表示列是1，行不定的一维维数组,
	//注意是一维数组,不是一维向量,是有多个一维向量组成的一维数组
	//之所一维数组,是因为在这个例子里,样本是一个个的点坐标,而每一次训练是一次性输入XX个样本,统一计算
	//这堆样本的x分量会放入xs里,变成一个有XX个一维向量的一维数组
	//这堆样本的y分量会放入ys里,变成一个有XX个一维向量的一维数组	
//构建多层模型
	这里只有两层: l1隐藏层,和 prediction预测输出层
	l1 = add_layer(xs, 1, 10, activation_function=tf.nn.relu)
	prediction = add_layer(l1, 10, 1, activation_function=None)
	//li层,会对结果执行relu激活算法,使第一层的输出有10个变量的一维数组的变量值在0-1附近
	//使用激活函数,可以优化避免梯度消失和梯度爆炸的情况发生
//设置训练方式
	loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys-prediction), reduction_indices=[1]))
	//例子说明:例如一次训练直接输入5个样本点 [a,A][b,B][c,C][d,D][e,E]
	//那么 xs=|a| ,  ys=|A|  通过 xs 得到的 prediction=|Y|
	//	  |b|       |B|				  |H|
	//	  |c|       |C|				  |Z|
	//	  |d|       |D|				  |T|
	//	  |e|       |E|     			  |V|
	//那么 tf.square就得到 |(A-Y)^2|
	//		      |(B-H)^2|
	//		      |(C-Z)^2|
	//		      |(D-T)^2|
	//		      |(E-V)^2|
	//然后 tf.reduce_sum(..,reduction_indices=[1]) 得到:
	//	|(A-Y)^2|
	//	|(B-H)^2|
	//	|(C-Z)^2|
	//	|(D-T)^2|
	//	|(E-V)^2|
	//	因为是每行只有一个量,所 tf.reduce_sum 后并没有变化
	//然后 tf.reduce_mean()得到平均值:
	//	( |(A-Y)^2| + |(B-H)^2| + |(C-Z)^2| + |(D-T)^2| + |(E-V)^2| ) / 5 	
	train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)
	//使用 普通的梯度下降的优化方法(GradientDescent),来训练优化 loss, 学习率是 0.1
	//最终这次训练会更新所有的 weights 和 biases
//初始化tf训练环境
	sess = tf.Session(),	
//初始化tensorflow的所有变量
	init = tf.global_variables_initializer()
	sess.run(init)
//开始训练
	for i in range(1000):
    		sess.run(train_step, feed_dict={xs: x_data, ys: y_data})
	//执行1000次训练,每次训练都 读入 300 个散点,即 xs数组有300行, ys数组有300行
	//而这里我们就只有300个样本,所以每次训练都读入同一组数据
    		if i % 50 == 0:
        	print(sess.run(loss, feed_dict={xs: x_data, ys: y_data}))
	//每50次训练后,打印一次 loss

/home/william/AI/machine learning/回归/code.py
这个例子补充主要是图像监测部分!!
使用:import matplotlib.pyplot as plt
	fig = plt.figure()		//创建一个独立的视图窗口
	ax = fig.add_subplot(1,1,1)	//在窗口添加一个子视图ax
	ax.scatter(x_data, y_data)	//子视图的 x,y 轴对应 x_data, y_data
	plt.ion()			//使用交互形式,
	plt.show()			//一直显示图,(如果不开启交互模式,默认是阻塞模式,)
					//交互模式下一直显示图,图会一直显示,而程序也会继续plt.show()后的内容
					//阻塞模式下一直显示图,会一直卡在plt.show(),
	lines = ax.plot(x_data, prediction_value, 'r-', lw=5)
					//据 x_data, prediction_value 的一堆散点画出一条线
					//x值,y值,红色,宽度5
	plt.pause(1)		//暂停一秒
	ax.lines.remove(lines[0])//把刚刚画的线去掉,(这样就画下一条线,就不会挡住什么的)







20180401
学习使用tensorboard监视模型:/home/william/AI/machine learning/tensorboard
code3.py
重点是,在使用 tf.xxxx之前,先添加 with tf.name_scope('XXXX'):
例如:
with tf.name_scope('layer'):
    with tf.name_scope('weights'):
        Weights = tf.Variable(tf.random_normal([in_size, out_size]), name='W')
with tf.name_scope('inputs'):
    xs = tf.placeholder(tf.float32, [None, 1], name='x_input')
with tf.name_scope('loss'):
    loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction), reduction_indices=[1]))
with tf.name_scope('train'):
    train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)
然后,sess = tf.Session()后,init = tf.global_variables_initializer()前
    writer = tf.summary.FileWriter("logs/", sess.graph)
这样执行代码时,会把模型图加载到logs/里,
终端运行 tensorboard --logdir=logs后,就可以在浏览器http://127.0.0.1:6006/查看到模型内容

code4.py
重点添加训练参数的跟踪记录表
例如:
    Weights = tf.Variable(tf.random_normal([in_size, out_size]), name='W')
    tf.summary.histogram(layer_name + '/weights', Weights) 
    //在tensorboard的histogram和distribution栏添加Weights 的训练跟踪记录表
    loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction),reduction_indices=[1]))
    tf.summary.scalar('loss', loss)
    //在tensorboard的scalar栏添加loss 的训练跟踪记录表
然后,sess = tf.Session()后,init = tf.global_variables_initializer()前
    merged = tf.summary.merge_all()
    writer = tf.summary.FileWriter("logs2/", sess.graph)
这样执行代码时,会把模型图,还有训练跟踪表设置加载到logs2/里,
然后每隔n次训练后,给所有训练跟踪表添加新数据
    result = sess.run(merged, feed_dict={xs: x_data, ys: y_data})
    writer.add_summary(result, i)
最后终端运行 tensorboard --logdir=logs后,就可以在浏览器http://127.0.0.1:6006/查看到模型内容









20180401
分类:/home/william/AI/machine learning/分类
code5.py
注意,这里使用的交叉熵跟我之前分析的交叉熵有所区别
我之前分析的是 基于一个图像样品,得到的交叉熵再反向传导学习,
而这里却是 100个 样本的交叉熵,这里的交叉熵有点像平均值的意思,然后再反向传导学习
http://www.360doc.com/content/17/0118/20/10408243_623338635.shtml
经过慎重分析发现:
	注意不是 100个 样本的交叉熵,
	而是每个样本一个交叉熵,共100个,然后把他们都加起来除以100
	得到一个交叉熵的平均值,使用这个平均值反向传导训练模型
模型训练的思路分析:
每张图片有728个像素点: [x(1)1,x(1)2,x(1)3,...x(1)728],
	xs符合[None, 784],none=100行,即xs包含100张图片,即:
	xs = [x(1)1,x(1)2,x(1)3,x(1)4,...x(1)728],
	     [x(2)1,x(2)2,x(2)3,x(2)4,...x(2)728]
	     .......
	     [x(100)1,x(100)2,x(100)3,x(728)4,...x(100)728]
Weights符合[in_size, out_size],in_size=728,out_size=10,即
	Weights = [w(1)1,w(2)1,..w(10)1],
	          [w(1)2,w(2)2,..w(10)2]
		  [w(1)3,w(2)3,..w(10)3]
		  [w(1)4,w(2)4,..w(10)4]
		  .......
		  [w(1)728,w(2)728,..w(10)728]
bias符合[1,out_size],out_size=10,即
	bias = [b1,b2,b3,b4,..b10]
图片对应的标签向量: [y(1)1,y(1)2,..y(1)10]
	ys符合[None, 10],none=100行,即 ys 对应100张图片的 100 个标签向量:
	ys = [y(1)1,y(1)2,..y(1)10]
	     [y(2)1,y(2)2,..y(2)10]
	     [y(3)1,y(3)2,..y(3)10]
	     [y(4)1,y(4)2,..y(4)10]
	     ........
	     [y(100)1,y(100)2,..y(100)10]
那么,Wx_plus_b=y:
[x(1)1,x(1)2,x(1)3,x(1)4,...x(1)728]          * [w(1)1,w(2)1,..w(10)1]      + [b1,b2,b3,b4,..b10] = [y(1)1,y(1)2,..y(1)10]
[x(2)1,x(2)2,x(2)3,x(2)4,...x(2)728]            [w(1)2,w(2)2,..w(10)2]                              [y(2)1,y(2)2,..y(2)10]
 .......                                        [w(1)3,w(2)3,..w(10)3]                              [y(3)1,y(3)2,..y(3)10]
[x(100)1,x(100)2,x(100)3,x(728)4,...x(100)728]  [w(1)4,w(2)4,..w(10)4]                              [y(4)1,y(4)2,..y(4)10]
                                                .......						    ........
	                                        [w(1)728,w(2)728,..w(10)728]                        [y(100)1,y(100)2,..y(100)10]

batch 机制的功能作用的描述 !!
	整理下认识,一般我们求出实际y与训练结果Y'的误差来反向传导更新模型
	而误差,我们可以取,方差,标准差,交叉熵什么的,
	注意,这些是 一个样本下的误差,我们说的方差交叉熵什么的,是基于同一个样本得到的,
	例如,图片a实际标签向量y:[y1,y2,..y10],训练结果标签向量Y':[y'1,y'2,..y'10].
	图片a的方差,是:	{ (y1-y'1)^2 + ..+ (y10 - y'10)^2 } /10
	那么cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction),reduction_indices=[1]))如何理解,
	我们知道,在一次训练里,输入了100张图片,即100张图片通过同样的 weights,blase,得到各自的训练结果标签向量Y'
	tf.reduce_mean括号里算出的是 每张图片自己的交叉熵,
	交叉熵指,有10个特征值的标签向量,把每个特征值的混乱程度加到一块得到的一个值
	然后通过 tf.reduce_mean,把 100 个交叉熵 算出一个 平均值,
	我们最后才通过这个 交叉熵平均值 来反向传导更新一次学习模型的参数weights和biase
	即我们是通过100张图片得到100个交叉熵,然后取100个交叉熵的平均值
	并不是通过100张图片得到1个交叉熵,而且也不知道如何得到
	也不是100个交叉熵,每个交叉熵都反向传导更新模型一次,即不是在一次训练里更新100次模型!!!
	我们只是取 100个交叉熵的平均值来更新 1 次模型
	所以.模型中的 batch 设置为100,那个100,就是这种意思
其他重点内容:
	mnist = input_data.read_data_sets('MNIST_data', one_hot=True) //这里是导入官方训练样品库的方法
	batch_xs, batch_ys = mnist.train.next_batch(100)//从训练集,取出100个28*28图片样本和对应标签向量
	mnist.test.images, mnist.test.labels//检验集的图片样品,和对应标签向量
	
code6.py
重点是使用了sklearn 生成的样本,可以辅助我们学习使用tensorflow做很多模拟事情
	from sklearn.datasets import load_digits
	from sklearn.model_selection import train_test_split
	from sklearn.preprocessing import LabelBinarizer
	# load data
	digits = load_digits()
	X = digits.data
	y = digits.target
	y = LabelBinarizer().fit_transform(y)
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3)
还有的是,清晰告诉我们如何使用 dropout
	def add_layer(inputs, in_size, out_size, layer_name, activation_function=None, ):
	    # add one more layer and return the output of this layer
	    Weights = tf.Variable(tf.random_normal([in_size, out_size]))
	    biases = tf.Variable(tf.zeros([1, out_size]) + 0.1, )
	    Wx_plus_b = tf.matmul(inputs, Weights) + biases
	    # here to dropout
	    Wx_plus_b = tf.nn.dropout(Wx_plus_b, keep_prob)
	    if activation_function is None:
	        outputs = Wx_plus_b
	    else:
	        outputs = activation_function(Wx_plus_b, )
	    tf.summary.histogram(layer_name + '/outputs', outputs)
	    return outputs
	之前学习到:dropout不算是一个正规正矩的优化器，他的工作是，每次网络工作时，
	都随机抛弃一部分的神经元的作用，从而避免过度拟合
	从这个层建设定义中dropout的位置可看出,dropout不属于激活函数,
	同时也不能算作是一个优化器











20180403
CNN: code7.py
准确率计算:
def compute_accuracy(v_xs, v_ys):
    global prediction
    y_pre = sess.run(prediction, feed_dict={xs: v_xs, keep_prob: 1})
    correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(v_ys,1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
    result = sess.run(accuracy, feed_dict={xs: v_xs, ys: v_ys, keep_prob: 1})
    return result
	我们知道 图片对应的标签向量我们成为 one-hot 向量,即这样表示:
		数字图片0:[1,0,0,0,0,0,0,0,0,0]
		数字图片1:[0,1,0,0,0,0,0,0,0,0]
		...
		数字图片9:[0,0,0,0,0,0,0,0,0,1]
	而通过学习模型学习到的标签向量,往往不是整数的,例如:
		数字图片1:[0.01, 0.98, 0, 0.001, 0.1, 0.1, 0, 0, 0.02, 0.4]
		而标签向量 与 学习得到的标签向量, 位置1(从0数)的值都是最大的.
		这样就认为 模型准确学习识别出数字1的图片
	重点看: correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(v_ys,1))
		先tf.argmax算出 标签向量 与 学习得到的标签向量,的最大值位置
		比较这两个位置是否一样.
		注意 y_pre, v_ys 在这里是二维数值,即包含多个标签向量,
		tf.argmax后是一个表示位置意义的一维数组
		tf.equal后是一个表示正确与否意义的一维数组
	        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
		先把 correct_prediction 向量的每个值都转为 float类型,然后把加起来求平均
		这里,如果模型相当好,correct_prediction 向量的每个值基本为1,
		最后的平均值就接近1了
模型参数设置!!
权重weight
def weight_variable(shape):
    initial = tf.truncated_normal(shape, stddev=0.1)
    return tf.Variable(initial)
	按正太分布随机生成 张量的维度为shanpe 的权重数组 initial,
	然后计入 tf.Variable()
def bias_variable(shape):
    initial = tf.constant(0.1, shape=shape)
    return tf.Variable(initial)
	生成值为 0.1 ,张量的维度为shanpe 的 常量数组 inital
	然后计入 tf.Variable()
def conv2d(x, W):
    # stride [1, x_movement, y_movement, 1]
    # Must have strides[0] = strides[3] = 1
    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')
	建立一个卷积操作,注意strides=[1, 1, 1, 1]描述的是卷积核移动步长
	由于对于图片,只有长宽,所以,只用到中间两个值表示步数,其他为1,
	如 strides=[1, 4, 4, 1],表示长宽步长都为4
	padding可'SAME' 可`VALID, SAME表示卷积所有像素,VALID则不是
	VALID:
	 1 2 3 4 5 6 7 8 9 10 11 12 13	多出的12,13不卷积
	|___________|
		  |_____________|
	SAME:
	0 |1 2 3 4 5 6 7 8 9 10 11 12 13| 0 0	卷积所有像素,不够的补0
	|___________|                   |
		  |____________|        |
                            |________________|		

def max_pool_2x2(x):
    # stride [1, x_movement, y_movement, 1]
    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')
	建立一个池化操作
	池化输出的是邻近区域的概括统计量，一般是矩形区域。池化有最大池化、平均池化、滑动平均池化、L2范数池化等
	池化与卷积意义上不一样,虽然操作上有很多地方相同
	max_pool 是指最大池化的意思
最后补充内容:
	batch_xs, batch_ys = mnist.train.next_batch(100)
	sess.run(train_step, feed_dict={xs: batch_xs, ys: batch_ys, keep_prob: 0.5})
	先是,获取100张图片 的数据,
	然后加载到 占位符空间里,成为真正的参数
	compute_accuracy(mnist.test.images[:1000], mnist.test.labels[:1000]))
	输入1000用于mnist测试图片,即 batch=1000,使用定义的compute_accuracy函数检测准确率
code8.py是code7的后续
主要分析如何建设层模型!!
	# define placeholder for inputs to network
	xs = tf.placeholder(tf.float32, [None, 784])/255.   # 28x28	
	ys = tf.placeholder(tf.float32, [None, 10])
	keep_prob = tf.placeholder(tf.float32)
	x_image = tf.reshape(xs, [-1, 28, 28, 1])
	# print(x_image.shape)  # [n_samples, 28,28,1]
		占位符 xs 表示输入的每张图片是28X18=784个像素点,未知有多少图片输入所以none
		由于图片像素点值都是从值 0-255 来记录颜色的!!,增大后续计算量级,所以 除以255,
		把值域从 0-255 压缩到 0-1,只是值的比例缩小了,没有改变值记录的图像信息
		xs 是一个二维数组,一维表示图片,一维表示图片数量(即batch大小)
		所以需要转换传换成 4维数组, [batch,高,寬,深度(通道数)]
	## conv1 layer ##
	W_conv1 = weight_variable([5,5, 1,32]) # patch 5x5, in size 1, out size 32
	b_conv1 = bias_variable([32])
	h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1) # output size 28x28x32
	h_pool1 = max_pool_2x2(h_conv1)                                         # output size 14x14x32
		第一层CNN
		1*32 个 5x5 卷积核 卷积batch张,28x28的图片,最后得batch张 含32个通道的卷积后图片
		然后,卷积后图片 都经过池化得到  batch张 含32个通道的池化后图片	
	## conv2 layer ##
	W_conv2 = weight_variable([5,5, 32, 64]) # patch 5x5, in size 32, out size 64
	b_conv2 = bias_variable([64])
	h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2) # output size 14x14x64
	h_pool2 = max_pool_2x2(h_conv2)                                         # output size 7x7x64
		第二层CNN
		32*64 个 5x5 卷积核 卷积batch张,含32个通道的上一层池化后图片,
		先得到batch张,含 32*64 个通道的卷积后图片
		然后,平均压简成 batch张,含 64 个通道的卷积后图片
		然后,卷积后图片 都经过池化得到  batch张 含64个通道的池化后图片				
	## fc1 layer ##
	W_fc1 = weight_variable([7*7*64, 1024])
	b_fc1 = bias_variable([1024])
	# [n_samples, 7, 7, 64] ->> [n_samples, 7*7*64]
	h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])
	h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)
	h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)
		全连接层1
		第二层CNN 得到的  batch张 含64个通道的池化后图片 是一个[batch,高,寬,深度(通道数)]4维数组
		转换成 二维数组[batch,第二层池化后图片],
		然后换 Wx_plus_b 的层模型,使用relu激励函数 继续构建
		这里添加了 dropout 处理,是为了避免过拟合问题	
	## fc2 layer ##
	W_fc2 = weight_variable([1024, 10])
	b_fc2 = bias_variable([10])
	prediction = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)
		全连接层1
		同样使用 Wx_plus_b 的层模型 使用softmax激励函数 最后得到 学习的 one-hot 标签向量!!
	# the error between prediction and real data
	cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction),
                                              reduction_indices=[1]))       # loss
	train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)
		同样通过 -tf.reduce_sum(ys * tf.log(prediction),reduction_indices=[1])
		算出每张图片的 交叉熵,然后 tf.reduce_mean 求平均得到一个平均交叉熵
		通过 优化器 AdamOptimizer 优化器处理平均交叉熵 来执行反向传导,
		更新一次所有的学习参数(包括所有的卷积核,所有的weight和biase)










20180404
RNN 
code9.py 根据sin 画出cos
首先看 执行训练的部分:
	_, cost, state, pred = sess.run(
            [model.train_op, model.cost, model.cell_final_state, model.pred],
            feed_dict=feed_dict)
	sess.run先后执行了4个函数模块 model.train_op, model.cost, model.cell_final_state, model.pred
	sess.run执行 train_op时,会回溯执行嵌套操作
		当前的 cost,cell_final_state, pred,最后更新了一次学习模型
	执行完train_op后 ,sess.run继续执行 cost
		这时执行的 cost,得到的是 更新学习模型后的状态下得到的 cost
	sess.run继续执行 cell_final_state,得到的是 更新学习模型后的状态下得到的 cell_final_state
	sess.run继续执行 pred,得到的是 更新学习模型后的状态下得到的 pred
初步了解 (只考虑batch为1,即喂一堆段数据的情况)
	这里每次input是一段数据有 20 个数据单元(20个x数据得到的sin(x) ) ,
	然后每个数据单元通过 同一输入转化矩阵 wx_plus_b 扩成 10维输入向量:x1,x2...x10
	就是说输入转化矩阵有 10 个 W 和 B 需要学习
	然后 lstm 的cell 和hidden_unit 状态向量 都是10维向量 , 
	每次 给 lstm 喂 一个 10维输入向量 得到 一个 cell 和 hidden_unit 状态向量
	这个 cell 和 hidden_unit 状态向量,又作为下一次 喂 输入向量需要 的 cell 状态向量 和hidden_unit 状态向量
	有20个数据单元,所以一共喂 20次 10维输入向量
	收集 每次得到lstm 输出的 hidden_unit 状态向量,作为输出向量 即收集到 20个
	每个输出向量又通过 另一个 相同的 矩阵乘法 wx_plus_b 缩成一个输出数据
	输出转化矩阵有 10 个 W 和 B 需要学习
	得到20个输出数据(20个),
	再与20个真正的输出数据(20个x数据对应cos(x) 比较 得到一个 误差值(不是误差向量) 
	误差越小, 表示 从sin 推导出的 cos 越准确 !! 
	 
code10.py 同样先初步了解
这里是,一串一串地输入图片像数流数据,最后学习分辨出是什么图片!!!
初步了解
	这里每次input是一段数据有 28 条像素数据,
	每条像素数据 含28个像素点信息.
	每条像素数据 通过 同一个输入转化矩阵 wx_plus_b  扩成 128维向量x1,x2...x128
	输入转化矩阵有 128*28 个 W 和 128 个 B 需要学习
	然后 lstm 的 cell 和hidden_unit 状态向量 都是128维向量 , 
	每次 给 lstm 喂 一个 128维输入向量 得到 一个 cell 状态向量 和 hidden_unit 状态向量
	这个 cell 和 hidden_unit 状态向量,又作为下一次 喂 输入向量需要 的 cell 和hidden_unit 状态向量
	有 28 条像素数据,所以一共喂 28次 128维输入向量
	收集 每次得到lstm 输出的 hidden_unit 状态向量,作为输出向量 即收集到 20个
	把这 28 个 输出向量组成 一个 28维数组,即 28*128 矩阵,
	然而其实,我们只需要最后一个输出向量,(第28个),其他的丢弃
	后一个输出向量 乘以一个 转化矩阵 得到  一个 10维变量
	这个转化矩阵的 W 和 B 也需要学习
	这个10维变量 与 实际图片对应的 one-hot 变量比较 得到一个 误差值(不是误差向量)
	误差越小, 识别图片的准确率越高. 
参考:	
	http://dy.163.com/v2/article/detail/CTIPFRJF0511K58A.html
	https://www.zhihu.com/question/40819582
	https://blog.csdn.net/u014595019/article/details/52605693
	https://blog.csdn.net/u014595019/article/details/52759104
	https://www.jianshu.com/p/4e285112b988
lstm单元:
	t表示当前时间

                                     ht
		  ___________________|____
	C(t-1)----|                      |-----Ct
	          |                      |
		  |                      |
	          |                      |
	h(t-1)----|______________________|-----ht
	            |
		   xt

	xt     当前输入值,是一个向量!!
	h(t-1) 前一刻的 h 状态值,是一个向量!!  h 俗称 hidden_unit
	C(t-1) 前一刻的 C 状态值,也是一个向量!! C 俗称 Cell	
	ht     当前的 h 状态值,是一个向量!!  
	Ct     当前的 C 状态值,也是一个向量!! 
	注意: xt, h(t-1), ht, Ct, C(t-1) 向量维数相同,(一维数组 也称为向量)
	还有,xt是处理过的输入,比如,这代表一个句子中的一个词语,xt不是词语本身,而是对应处理过得到的向量
 	同样,yt是未处理的输入,yt这个向量需要做处理后才得到我们直接观察的结果
	三个门向量:
		输入门: it = sigmod{ (Wxi)(xt) + (Whi)(h(t-1)) }
		忘记门: ft = sigmod{ (Wxf)(xt) + (Whf)(h(t-1)) }
		输出门: ot = sigmod{ (Wxo)(xt) + (Who)(h(t-1)) }
	候选值向量:
		       ~
	               Ct  =  tanh{ (Wxc)(xt) + (Whc)(h(t-1)) }
 	当前的 C 状态值:
		                                     ~
	               Ct  =  ft ⊙  C(t-1)  +  it ⊙  Ct
	当前的 h 状态值:
		       ht  =  ot ⊙  Ct
	注意:⊙ 是 自定义乘法, 这里是按元素乘法 ,例如:门[0,1,1,0,0,1] X 向量[2,3,4,5,6,7] = [0,3,4,0,0,7]
	向量的按元素乘法也叫:Hadamard product (also known as the Schur product
	这里由于门向量的成员基本上不是0,就是1,所以就有所谓开关的意义,
	所以就可以让一部分内容向后传输,阻隔一部分输出,达到长短记忆的作用
	而且也说明为啥 xt, h(t-1), ht, Ct, C(t-1) 向量维数相同
	注意 sigmod 得到的值不是无限接近0就是无限接近1
	注意 tanh   得到的值是 -1到1 之间
	后向传播:(BPTT算法) 参考
		   :https://blog.csdn.net/dark_scope/article/details/47056361
		   :https://blog.csdn.net/u012319493/article/details/52802302
rnn-lstm输入例子图:	
	          ht        h(t+1)        h(t+2)        h(t+3)
	       ___|__    ___|__        ___|__        ___|__
	C(t-1)-|    |-Ct-|    |-C(t+1)-|    |-C(t+2)-|    |-C(t+3)-....
	       |    |    |    |        |    |        |    |
	       |  A |    |  A |        |  A |        |  A |
	       |    |    |    |        |    |        |    |
	h(t-1)-|____|-ht-|____|-h(t+1)-|____|-h(t+2)-|____|-h(t+3)-....
	        |         |             |             |
	        xt       x(t+1)        x(t+2)        x(t+3)
rnn-lstm多测层网络例子图:	
		           .         .              .              .
		           .         .              .              .
		           .         .              .              .
		          h3t      h3(t+1)        h3(t+2)        h3(t+3)
		        __|___     __|___         __|___         __|___
		C3(t-1)-|    |-C3t-|    |-C3(t+1)-|    |-C3(t+2)-|    |-C3(t+3)-....
		        |    |     |    |         |    |         |    |
	第三层	        |  C |     |  C |         |  C |         |  C |
		        |    |     |    |         |    |         |    |
		h3(t-1)-|____|-h3t-|____|-h3(t+1)-|____|-h3(t+2)-|____|-h3(t+3)-....
		          |          |              |              |
		          h2t      h2(t+1)        h2(t+2)        h2(t+3)
		        __|___     __|___         __|___         __|___
		C2(t-1)-|    |-C2t-|    |-C2(t+1)-|    |-C2(t+2)-|    |-C2(t+3)-....
		        |    |     |    |         |    |         |    |
	第二层	        |  B |     |  B |         |  B |         |  B |
		        |    |     |    |         |    |         |    |
		h2(t-1)-|____|-h2t-|____|-h2(t+1)-|____|-h2(t+2)-|____|-h2(t+3)-....
		          |          |              |              |
		          h1t      h1(t+1)        h1(t+2)        h1(t+3)
		        __|___     __|___         __|___         __|___
		C1(t-1)-|    |-C1t-|    |-C1(t+1)-|    |-C1(t+2)-|    |-C1(t+3)-....
		        |    |     |    |         |    |         |    |
	第一层	        |  A |     |  A |         |  A |         |  A |
		        |    |     |    |         |    |         |    |
		h1(t-1)-|____|-h1t-|____|-h1(t+1)-|____|-h1(t+2)-|____|-h1(t+3)-....
		          |          |              |              |
		         xt        x(t+1)         x(t+2)         x(t+3)
一次典型的训练过程:
	例如,每次给 rnn 喂一句话,然后反向传导训练一次网络.的过程
	首先,每句话后假设有 30 个单词,少于30个词语的,也假作有30个单词,剩余用"空白"代替单词位置
	于是可以把句子 分成 30 个steps, 又把每个单词通过 Wx_plus_b 转成 输入向量.
	向量成员数 与 rnn 的 cell/hidden_unit的成员数一样
	假如我们 使用的 是一个 3层lstm 网络,如上图, 那么只有三个lstm单元 A B C
	第1刻  输入第一个单词 向量 xt 到 A单元, 
	       A单元 输出的  h1t     输入到 B单元,
	       B单元 输出的  h2t     输入到 C单元,
	       最后  C单元输出  h3t
	       每个 lstm 单元都需要的 h?(t-1),C?(t-1)可能是最新一次训练的到的值,也可能是第一次训练,所以随机值
	第2刻  输入单词向量  x(t+1)  到A单元, 
	       A单元 输出的  h1(t+1) 输入到 B单元,
	       B单元 输出的  h2(t+1) 输入到 C单元,
	       最后  C单元输出  h3(t+1)
	....
	第30刻 输入单词向量  x(t+29) 到A单元,
	       A单元 输出的 h1(t+29) 输入到B单元,
	       B单元 输出的 h2(t+29) 输入到C单元,
	       最后  C单元输出  h3(t+29)
	根据目的选取结果: h3t,h3(t+1),..h3(t+29) 组成的就是一个回答向量组
		然后再对每个向量执行 另一套 Wx_plus_b 转成 单词	最后组成答句.
		对比本来的答句 得到 误差 然后就可以执行反向传导 
		更新 3个lstm A,B,C,单元的参数,还有输入输出转化矩阵的参数
		特别注意, 还会更新 输入单词 转 输入向量的 Wx_plus_b
			  还会更新 输出h3(t+?) 转 输出单词的 Wx_plus_b
	也就是对于 lstm 3个单元 A,B,C 会连续 喂 30 遍数据,算出最后得到的结果,
	才会执行一次反向传导更新 A,B,C 单元里的内容 !!	
	还有一种情况,就是 只选取 最后一个 h3(t+29) 作为输出
		其他时间点的 h3(t+?) 直接丢弃
		根据 h3(t+29) 经输出转化矩阵得到的结果 与真实结果比较 得到误差
		最后,反向传导,更新 更新 3个lstm A,B,C,单元的参数,还有输入输出转化矩阵的参数

深入分析code9.py:
首先获取 batch 段数据, 每段数据由 TIME_STEPS 个数据单元组成,一次训练 喂 batch段数据
	所以每次输入数据的 self.xs 的 shape 是[(batch, n_steps, 输入数据单元)]
	把每个数据单元 转成 cell_size 维数据单元向量. 
	那么得到的转换后 的 输入数据 self.l_in_y 的 shape 是 [(batch, n_steps, cell_size)]
这里的 RNN 只用一层 lstm, 就是说只有一个 lstm单元
	初始化 lstm 单元:
		lstm_cell = tf.contrib.rnn.BasicLSTMCell(self.cell_size, forget_bias=1.0, state_is_tuple=True)
		BasicLSTMCell参数:
		self.cell_size 就是 C状态值维数,h状态值维数,输入数据单元向量维数,都是同一个数量
		forget_bias=1.0 如果忘记门是有偏移的 ft = sigmod{ (Wxf)(xt) + (Whf)(h(t-1)) + bias }
		这个就是那个偏移值参数
		state_is_tuple=True 意味着 每次 lstm单元 输出的状态值 是 [C状态值,h状态值] 的组合数组  
	初始化 lstm 单元的C状态值 和 h状态值 这里初始化都为零:           
		self.cell_init_state = lstm_cell.zero_state(self.batch_size, dtype=tf.float32)
		self.cell_init_state 模型状态值初始值,因为 state_is_tuple=True 的原因
			shape是 [batch_size,[C状态值维数+h状态值维数=]]
	设置 rnn 的训练过程:
		self.cell_outputs, self.cell_final_state = tf.nn.dynamic_rnn(
            		lstm_cell, self.l_in_y, initial_state=self.cell_init_state, time_major=False)
		这里 rnn 网络 是 单层的 lstm,
		rnn输入数据是 self.l_in_y
		rnn 状态的初始值,这里指的是lstm的状态初始值 self.cell_init_state
		time_major 跟 input数据有关系,其实是跟训练方式有关系:
			当 self.l_in_y的shape是 [(batch, n_steps, cell_size)],time_major=False
			当 self.l_in_y的shape是 [(n_steps, batch, cell_size)],time_major=True
		self.cell_outputs 是训练后得到的输出, shape是 (batch * n_steps, cell_size)
			也就说,每刻(step)喂一数据单元数组到rnn网络, 就有 cell_size维输出向量
			喂完一段数据,就是前后喂完 n_steps , 得到 n_steps个cell_size维输出向量
			喂了 batch 段数据, 就有 batch 个 (n_steps个cell_size维输出向量)
		self.cell_final_state 是得到最后模型状态值, shape与self.cell_init_state的一样
rnn网络输出数据处理:
	self.cell_outputs 的 shape是 (batch * n_steps, cell_size)
	我们要对 cell_size维输出向量 转成 我们处理的输出数据单元
	最后得到输出数据 self.pred , shape为: (batch * steps, 输出数据单元)
误差处理:
	经过训练得到的输出数据 和 真实输出数据 的误差
	    def compute_cost(self):
	        losses = tf.contrib.legacy_seq2seq.sequence_loss_by_example(
	            [tf.reshape(self.pred, [-1], name='reshape_pred')],
	            [tf.reshape(self.ys, [-1], name='reshape_target')],
	            [tf.ones([self.batch_size * self.n_steps], dtype=tf.float32)],
	            average_across_timesteps=True,
	            softmax_loss_function=self.ms_error,
	            name='losses'
	        )
	        with tf.name_scope('average_cost'):
	            self.cost = tf.div(
	                tf.reduce_sum(losses, name='losses_sum'),
	                self.batch_size,
	                name='average_cost')
	            tf.summary.scalar('cost', self.cost)
	    @staticmethod
	    def ms_error(labels, logits):
	        return tf.square(tf.subtract(labels, logits))
	tf.contrib.legacy_seq2seq.sequence_loss_by_example 是计算误差的一种方法,
	现在仅仅就具体例子分析,未打算系统说明这个函数的内容:
		参考:https://tensorflow.google.cn/api_docs/python/tf/contrib/seq2seq/sequence_loss
		    :https://blog.csdn.net/liuchonge/article/details/71424432
		第一个参数 logits, 一般是shape为 [batch,nsteps]
			特别要讨论下这里的shape:
			譬如,输入本来的shape 是 [batch, steps, 数据单元向量]:[50, 30, 20],
			必须reshape成二维数组: [batch, steps*数据单元向量]:[50, 600],才能作为logits输入,
			这时的 nsteps 就是 600 了!!
			现在,我们输入的 self.pred 的shape 是 (batch * steps, 输出数据单元):[50*20,1]
			对应着 logits 的 shape [batch,nsteps]: [50*20,1]
			而我们先把 self.pred reshape 成 一维数组 [batch]:[50*20*1] 再输入到 logits
			显然 logits 把输入的 self.pred:[batch]:[50*20*1]看成self.pred:[batch,nsteps]:[50*20,1]
			也就说 一维数组 [batch], 和二维数组 [batch,1] 并没有区别
		第一个参数 targets, 一般是shape为 [batch,nsteps]
			同样我们先把 self.ys reshape 成 一维数组 [batch]:[50*20*1] 再输入到 targets
			即,显然 targets 把输入的 self.ys:[batch]:[50*20*1]看成self.ys:[batch,nsteps]:[50*20,1]
		第三参数 权重 Weight 表示要对不同loss,的重视程度: 一般是shape为 [batch,nsteps]
			这里要求每个loss重视程度都一样,所以 weights 都为1
		第四参数:average_across_timesteps=True,表示求 timesteps 平均,后续解释
		第五参数:average_across_batch=True,表示求 batch 平均,注意:这里没有填入,没有使用,所以默认False后续解释
		第六参数:softmax_loss_function=None,如果使用默认loss单元函数,填None
		第七参数:name=None给这个 定义的计算误差的方法 命名,也可以不命名,为none
			图例解释: logits,target,weights 的shape是一样的!!!
			假如
			self.pred:[batch,n_step,数据单元向量]:[2,3,4]
				|[p111,p112,p113,p114]| ,|[p211,p212,p213,p214]|
				|[p121,p122,p123,p124]|  |[p221,p222,p223,p224]|			
				|[p131,p132,p133,p134]|  |[p231,p232,p233,p234]|
			self.ys:[batch,n_step,数据单元向量] 也一定是 [2,3,4]
				|[y111,y112,y113,y114]| ,|[y211,y212,y213,y214]|
				|[y121,y122,y123,y124]|  |[y221,y222,y223,y224]|			
				|[y131,y132,y133,y134]|  |[y231,y232,y233,y234]|
			假如被reshape成以下样子:
			self.pred 成 logits:[batch,nsteps]:[2,3*4]
				|p111,p112,p113,p114,p121,p122,p123,p124,p131,p132,p133,p134|
				,
 				|p211,p212,p213,p214,p221,p222,p223,p224,p231,p232,p233,p234|
			self.ys 成 targets:[batch,nsteps]:[2,3*4]
				|y111,y112,y113,y114,y121,y122,y123,y124,y131,y132,y133,y134| 
				,
				|y211,y212,y213,y214,y221,y222,y223,y224,y231,y232,y233,y234|
			那么我们要求 weight 也是这样子:
				|w11,w12,w13,w14,w15,w16,w17,w18,w19,w110,w111,w112|
				,	
				|w21,w22,w23,w24,w25,w26,w27,w28,w29,w210,w211,w212|
			如果 softmax_loss_function=None ,就使用default-loss单元函数,
				如果不想使用 default-loss单元函数,就得设置 softmax_loss_function
				假如我们使用的误差是 均方差:mean squared error(MSE), 
				有必要说明 均方误差 MSE = E( (y-y')^2 ) =  ( (y1-y'1)^2 + .. + (yn-y'n)^2 ) /n
				我们设置的 softmax_loss_function = ms_error(labels, logits) 
				而函数 ms_error(labels, logits) 只实现了 差的平方 (y-y')^2,
				即只有 tf.square(tf.subtract(labels, logits))
				而E()部分,"即(..+..+..+..)/n)" 并不在 ms_error 里实现.
				而是 tf.contrib.legacy_seq2seq.sequence_loss_by_example 透过
				average_across_batch 或者 average_across_timesteps 条件实现
				一般 average_across_timesteps,  average_across_batch 只能选其中一个为True
			假如 average_across_timesteps=True 表示平均 nsteps 这维
			那么:
			这个定义了的误差方法得到的 结果
			A =  ( w11*(p111-y111)^2 + w12*(p112-y112)^2 + ... + w112*(p134-y134)^2 ) / (3*4)
			B =  ( w21*(p211-y211)^2 + w22*(p212-y212)^2 + ... + w212*(p234,y234)^2 ) / (3*4)
			当weights数组的值 w?? 都为 1 时 , 表示所有 差的平方 都重视
			这样就实现了在 nsteps维上 的 均方差 MSE = E( (y-y')^2 )
			当weights数组的值 w?? 不一样时 , 表示部分 差的平方 得到重视 
			这样就实现了在 nsteps维上 的 配置了权重的均方差 MSE = E( (y-y')^2 )
			最后得到 一个 二维向量(一维数组) [A,B]
			假如 average_across_batch=True 表示平均 batch 这维
			那么: 
			这个定义了的误差方法得到的 结果
			a =  ( alo(p111,y111)*w11 + alo(p211,y211)*w21 ) / 2
			b =  ( alo(p112,y112)*w12 + alo(p212,y212)*w22 ) / 2
			c =  ( alo(p113,y113)*w13 + alo(p213,y213)*w23 ) / 2
			b =  ( alo(p114,y114)*w14 + alo(p214,y214)*w24 ) / 2
			....
			l =  ( alo(p134,y134)*w112 + alo(p234,y234)*w212 ) / 2
			当weights数组的值 w?? 都为 1 时 , 表示所有 差的平方 都重视
			这样就实现了在 batch维上 的 均方差 MSE = E( (y-y')^2 )
			当weights数组的值 w?? 不一样时 , 表示部分 差的平方 得到重视 
			这样就实现了在 batch维上 的 配置了权重的均方差 MSE = E( w(y-y')^2 )
			最后得到 一个 十二维向量(一维数组) [a,b,c,d,e,f,g,h,i,j,k,l]
	回到 losses = tf.contrib.legacy_seq2seq.sequence_loss_by_example()
		self.pred:[batch]:[50*20*1] -> logits :[batch,nsteps]:[50*20*1,1]
		self.ys  :[batch]:[50*20*1] -> targets:[batch,nsteps]:[50*20*1,1]
		weights  :[batch]:[50*20*1] -> weights:[batch,nsteps]:[50*20*1,1]
		即 |p1,p2,p3,,,,p100|
		   |y1,y2,y3,,,,y100|
		   |w1,w2,w3,,,,w100|
		而且 w?? 的值都为 1
		我们斌不打算使用 tf.contrib.legacy_seq2seq.sequence_loss_by_example 默认方式求误差,
		同时 我们通过 均方差的方式 求出,误差, 并不是 交叉熵的方式,
		所以我们设置 softmax_loss_function=self.ms_error,只实现了 差的平方 (y-y')^2,
		我们设置了 average_across_timesteps=True,平均 nsteps 这维
		因为 nsteps 为 1,即只有一个成员
		于是,这个定义了的误差方法得到的 结果
		l1 = ( (p1-y1)^2 ) / 1
		l2 = ( (p2-y2)^2 ) / 1
		l3 = ( (p3-y3)^2 ) / 1
		...
		l1000 = ( (p100-y100)^2 ) / 1
		即得到一个 100维向量 [l1,l2,l3,,,l100]
		tf.contrib.legacy_seq2seq.sequence_loss_by_example 输出的 losses:[l1..l1000],
		是针对这个函数意义上的batch:1000 ,
		实际数据只有 50个batch, 每个batch有20个step,
		我们使用这个函数就预处理为把每个step都当成batch处理
	计算cost:  self.cost = tf.div(
	                tf.reduce_sum(losses, name='losses_sum'),
	                self.batch_size,
	                name='average_cost')
	            tf.summary.scalar('cost', self.cost)
		tf.reduce_sum: 把losses的1000个成员都加起来,
		tf.div: 然后除以 self.batch_size:50,
		最后得到的就是 平均 cost ,是一个值, 
		相当于每个 batch 的 20个Time_steps的loss加起来得到一个cost
		把50个cost加起来,再平均!!!
最后分析训练过程!!
	第一次:	feed_dict = {
	                    model.xs: seq,
	                    model.ys: res,
	                    # create initial state
	            }
		_, cost, state, pred = sess.run(
        	    [model.train_op, model.cost, model.cell_final_state, model.pred],
        	    feed_dict=feed_dict)
		先执行model.train_op,
			执行 tf.train.AdamOptimizer(LR).minimize(self.cost)
			需要 self.cost,
			所以得先执行 tf.div(.. losses, ,,self.batch_size),
			需要 losses 和 self.batch_size, self.batch_size已知,losses,未知
			所以得先执行 tf.contrib.legacy_seq2seq.sequence_loss_by_example
			需要 self.pred, self.ys, self.batch_size, self.n_steps	 
			self.batch_size, self.n_steps 已知,
			self.ys 是占位符号: 传入了 model.ys: res
			要取得self.pred,所以得先执行tf.matmul,
			需要 l_out_x, Ws_out, bs_out
			Ws_out, bs_out 是 被定义为要 训练 variable
			要取得 l_out_x,即取得 self.cell_outputs,
			所以得先执行 tf.nn.dynamic_rnn
			需要 lstm_cell, self.l_in_y, self.cell_init_state,
			lstm_cell, self.cell_init_state, 已知
			要取得 self.l_in_y, 即取得 l_in_x ,即取得 self.xs
			self.xs 是占位符号: 传入了 model.xs: seq
			最后,再最后一步步往前执行,
			回到 tf.train.AdamOptimizer(LR).minimize(self.cost) 执行反向传导更新参数
			最后执行完毕,返回 数据 放入 _
		然后执行model.cost,
			得先执行 tf.div(.. losses, ,,self.batch_size),
			需要 losses 和 self.batch_size, self.batch_size已知,losses,未知
			所以得先执行 tf.contrib.legacy_seq2seq.sequence_loss_by_example
			需要 self.pred, self.ys, self.batch_size, self.n_steps	 
			self.batch_size, self.n_steps 已知,
			self.ys 是占位符号: 传入了 model.ys: res
			要取得self.pred,所以得先执行tf.matmul,
			需要 l_out_x, Ws_out, bs_out
			Ws_out, bs_out 是 被定义为要 训练 variable
			要取得 l_out_x,即取得 self.cell_outputs,
			所以得先执行 tf.nn.dynamic_rnn
			需要 lstm_cell, self.l_in_y, self.cell_init_state,
			lstm_cell, self.cell_init_state, 已知
			要取得 self.l_in_y, 即取得 l_in_x ,即取得 self.xs
			self.xs 是占位符号: 传入了 model.xs: seq
			最后,再最后一步步往前执行,直到完毕,返回 数据 放入 cost
		然后执行 model.cell_final_state,
			得先执行 tf.nn.dynamic_rnn
			需要 lstm_cell, self.l_in_y, self.cell_init_state,
			lstm_cell, self.cell_init_state, 已知
			要取得 self.l_in_y, 即取得 l_in_x ,即取得 self.xs
			self.xs 是占位符号: 传入了 model.xs: seq
			最后,再最后一步步往前执行,
			tf.nn.dynamic_rnn 会得到 self.cell_outputs, self.cell_final_state
			但是 sess.run 只想得到 self.cell_final_state
			完毕后只返回 self.cell_final_state 数据 放入 state
		最后执行 model.pred
			得先执行tf.matmul,
			需要 l_out_x, Ws_out, bs_out
			Ws_out, bs_out 是 被定义为要 训练 variable
			要取得 l_out_x,即取得 self.cell_outputs,
			所以得先执行 tf.nn.dynamic_rnn
			需要 lstm_cell, self.l_in_y, self.cell_init_state,
			lstm_cell, self.cell_init_state, 已知
			要取得 self.l_in_y, 即取得 l_in_x ,即取得 self.xs
			self.xs 是占位符号: 传入了 model.xs: seq
			最后,再最后一步步往前执行,直到完毕,返回 数据 放入 pred
	第一次以后的:feed_dict = {
	                model.xs: seq,
	                model.ys: res,
	                model.cell_init_state: state    # use last state as the initial state for this run
	            }
		_, cost, state, pred = sess.run(
        	    [model.train_op, model.cost, model.cell_final_state, model.pred],
        	    feed_dict=feed_dict)
		这时 特别注意到 model.cell_init_state: state
		因为,model.cell_init_state 不像 xs,ys 被定义为 placeholder 占位符号, 
		也不像 weight, bias,那种 被定义为 要被训练的 variable
		初始化时,是这样子的
			self.cell_init_state = lstm_cell.zero_state(self.batch_size, dtype=tf.float32)
		应该像 self.cell_size 那样的变量, model里内部初始化,内部赋值的
		但现在, model.cell_init_state: state 的操作,
		相当于, self.cell_init_state  被外部赋值了, 有占位符号那样的作用.
		也就说模型里 非 variable 和 placeholder 变量, 其实也有 placeholder的作用
		可以 ,被外部赋值,取代原有值 !!

深入分析code10.py
	和code9.py的区别是, 输入的是一张28*28图片
	一张图片 分成 28 条数据,即分28次喂入模型, 每条数据 28个像素点
	每次喂 1 条数据, 每条数据 转换成 128维向量 再喂入单层 lstm 模型
	只取 最后一次,即第28次喂入数据后 得到的 128维输出向量
	再 转化 得到 10维向量 与 图片原本指向的 10维向量做比较
cell 状态向量 和hidden_unit 状态向量 初始值问题:
	由于每张图片输入rnn 模型过程中,都与另一张输入不一样
	所以前一张的图片训练得到 的 cell 状态向量 和hidden_unit 状态向量
	不需要传导到 下一张图片的识别,所以每次训练一张图片用到的 
		 cell 状态向量 和hidden_unit 状态向量的初始值都为 0 !!
只获取第28次喂入数据后 得到的 128维输出向量 做比较的问题 
	这里的重点是 如何取第28次得到 128维输出向量:
    	outputs, final_state = tf.nn.dynamic_rnn(cell, X_in, initial_state=init_state, time_major=False)
	输出的 outputs 的shape [batch,steps,输出向量维数]:[128,28,128]
		即包含 128张图片 同时喂入模型 得到的128个结果,
		每个结果有28条 128维输出向量, 
		每条输出向量 对应 每次(step) 喂入的一个输入向量
		现在我们要 每个结果 的 最后一条 128维输出向量, 即第28次喂入输出向量 得到的 128维输出向量
		outputs = tf.unstack(tf.transpose(outputs, [1,0,2]))
		outputs 是一个 三维数组 即shape [batch,steps,输出向量维数]
		batch 这一维的 标记为0,
		steps 这一维的 标记为1,
		输出向量维数 这一维的 标记为2,
		tf.transpose(outputs, [1,0,2]:表示把 batch steps 两维转换
		即outputs 变成了 [steps,batch,输出向量维数]
		图例直观解析
			假如 outputs本来是这样的 shape:[3,2,5]:
			[
	
				[
					[1,1,1,1,1,1]
					[2,2,2,2,2,2]		
				]
				,
	
				[
					[3,3,3,3,3,3]
					[4,4,4,4,4,4]		
				]
				,
	
				[
					[5,5,5,5,5,5]
					[6,6,6,6,6,6]		
				]
				,
			]
			transpose(outputs, [1,0,2] -> outputs shape:[2,3,5]:
			只换第 batch steps 维, 
			输出向量维数 这维内容不变, 这维可以简单标记:
				A = [1,1,1,1,1,1]
				B = [3,3,3,3,3,3]
				C = [5,5,5,5,5,5]
				D = [2,2,2,2,2,2]		
				E = [4,4,4,4,4,4]		
				F = [6,6,6,6,6,6]
	    		outputs 简单记为:
			[[A,B]
			 [C,D]
			 [E,F]]
			换第 batch steps 后:
			[[A,C,E]
			 [B,D,F]]
			把标记 ABCDEF换换回去就得:
			[
	
				[
					[1,1,1,1,1,1]
					[3,3,3,3,3,3]
					[5,5,5,5,5,5]
	
				]
				,
	
				[
					[2,2,2,2,2,2]		
					[4,4,4,4,4,4]		
					[6,6,6,6,6,6]		
				]
				,
			]		
	
		解构数组:tf.unstack()
		没有其他参数,默认解构最前的一维,即 steps维,即第0维
		解构图示:
			假如 outputs是这样子的:
			outputs = [
		
					[
						[1,1,1,1,1,1]
						[2,2,2,2,2,2]		
					]
					,
	
					[
						[3,3,3,3,3,3]
						[4,4,4,4,4,4]		
					]
					,
		
					[
						[5,5,5,5,5,5]
						[6,6,6,6,6,6]		
					]
	
				]
			tf.unstack(outputs) 后,得到
			outputs[0] = [ [1,1,1,1,1,1]
				       [2,2,2,2,2,2] ]
			outputs[1] = [ [3,3,3,3,3,3]
				       [4,4,4,4,4,4] ]
			outputs[2] = [ [5,5,5,5,5,5]
				       [6,6,6,6,6,6] ]
			output 从 变量 变成了 数组变量 !!
			另外 outputs[-1] 等于 outputs[2], outputs[-1]表示数组的变量的最后一个成员!!
		因此 真正outputs tf.transpose转换后 得 shape:[steps,batch,输出向量维数]
		说明最后一steps 的 [batch,输出向量维数]:[128,128]
		放着是 128 条 第28次喂入输出向量 得到的 128维输出向量 
		我们只要最后一steps 的 [batch,输出向量维数]:[128,128] 
		所以执行了 tf.unstack 解构,
		outputs变成了 数组变量,我们只有这个数组变量最后一个,即 outputs[-1]
		results = tf.matmul(outputs[-1], weights['out']) + biases['out']    # shape = (128, 10)
		然后 每条最后的128维输出向量 经过同一个 [128,10]的转换矩阵 得到 一条 10维one-hot向量
		得到的 results 是含有 128 条 10维one-hot向量,
		即128张图片 经网络后得到 128条  one-hot向量
		最后对比 one-hot向量 与图片实际的 标记向量, 得到误差,然后反向传导更新网络参数
	由于我们只需要 最后一条 128维输出向量, 即第28次喂入输出向量 得到的 128维输出向量
	而这个内容 跟 final_state 这个数组变量的 第2个成员 final_state[1] 是一样的!
	results = tf.matmul(final_state[1], weights['out']) + biases['out']
	同样得到含有 128 条 10维one-hot向量, 的 results
final_state 这个内容 是lstm训练后得到的 c状态值 和 h状态值!!  	
	final_state[0] 是 c状态值
	final_state[1] 是 h状态值,也等于当前的输出向量	

		







20180415
讨论无监督学习问题:
并没有什么特别难的内容
图片 128维向量 得到一个 128维向量 (称为编码过程)
128维向量 128维向量 自己组成一幅图片 (称为解码码过程)
图片本身,与网络生成的图片对比,得到 误差,
通过误差,反向传导,训练更新编码网络和解码网络








20180415
batch nomalizeion (BN)
与 优化器 和 激活函数 的概念都不一样!
有效 避免 梯度消失爆炸的问题.
有效 加速迭代,减少训练次数,减少计算负担,提高效率
跟 白化 这样的预处理 差不多,但意义性质又不太一样!!
https://blog.csdn.net/whitesilence/article/details/75667002
https://blog.csdn.net/intelligence1994/article/details/53888270
https://blog.csdn.net/happynear/article/details/44238541
http://ufldl.stanford.edu/wiki/index.php/%E7%99%BD%E5%8C%96

BN跟学习过的 向量正规化,归一化 矩阵归一化什么的,没有任何数学关系,或者借鉴意义.
假想我们有一 batch 样本,他们很多时都不可避免可能会有些比较固执的地方,
比如,某段时间,样本值,不是无限接近A,就是无限接近B,
然而 我们知道真实情况 样本值是平均分布在 A-Z 之间的.
那么这样导致的结果就是,这段时间内的多次模型学习训练,梯度收敛都特别小,甚至没有收敛. 
这种情况下, 我们使用BN 就是使这些样本的近似部分弱化,差异部分放大.
这样子,学习训练时,梯度下降特别快,模型的学习效率提高!!
BN操作,不仅仅在数据开始时操作,而且也要在 模型层里加入处理
输入数据的BN处理!!
	一个batch有 m 个样本向量 x1,x2,...xm
	             m
	uB = (1/m) * ∑ xi	//求均值
		    i=1

	             m
	oB = (1/m) * ∑ (xi-uB)^2  //求均方差
		    i=1

	      	 xi-uB
	xi' = ____________	  //标准化, e是一个很小的定值, 比如0.001, 是为了防止分母为0的情况
	      √(oB^2 + e )

		
	yi = Y * xi' + B	//反标准化, Y是scalc, B是shift, Y和B是要学习的参数,
				//这里的作用是, 配置标准化的程度
	最后大得到的 y1,y2,,,ym 就是 BN 后得到的 样本,
	简记为 y = BN(X)
学习模型層的BN处理!!
	我们知道数据经过一层处理后得到的数据,作为下一层的输出例如:
		某层的输入 为 x, 经过一层神经元处理,得到输出 wx_plus_b
		而 wx_plus_b 会经过激活函数例如relu处理后得到 relu(wx_plus_b)
		relu(wx_plus_b) 就是下一层的输入!!
	而添加BN处理时,一般是这样的:
		某层的输入 为 x, 经过一层神经元处理,得到输出 wx_plus_b
		输出先做BN处理得 BN(wx_plus_b)
		然后 BN(wx_plus_b) 经过激活函数例如relu处理后得到 relu(BN(wx_plus_b))
		relu(BN(wx_plus_b)) 就是下一层的输入!!
	注意,如果未先BN处理, 最后得到的 relu 后 内容可能大部分分布在小于0的地方,而直接被等于0处理
		这样就没有更多有效信息传到下一层,而导致模型接下来的模型层没有明显梯度传导
		最终传导的学习梯度下降缓慢,
		BN处理后,最后得到的 relu 后 内容,会相对减少分布在小于0的地方,
		可以把更多有用信息传到下一层.最终传导的加速学习梯度下降,
	更直观的来说,经过 BN 处理后的内容被缩小到处于 [-1,1] 之间, 
		这样, 再经过激活函数得到的内容 大多数据 不会处于 几乎或过分饱和值域!!
		这样就有充分信息 向下一级传导!!
		有效避免梯度爆炸和消失
解析了这么多,但仍然觉得并不了解 BN 的实质!!,日后再深化吧

code12.py
	np.random.seed(1)	//用来产生相同的随机数,生成随机数操作前,都seed(同一个数),生成的随机数相同
	这个例子的样品是 2次曲线 散布点,
	学习散布点分布规律
通过 code12.py
深化认识 python 的 一些语法原理 特别是函数运算空间
	认识 python 的全局变量和 局部变量 , 局部函数引入全局变量要加 global
	每执行一次 局部函数 都是重新建立一个空间存储 当前局部环境和变量
函数执行环境分析
	train_op, cost, layers_inputs = built_net(xs, ys, norm=False)   # without BN 
	train_op_norm, cost_norm, layers_inputs_norm = built_net(xs, ys, norm=True) # with BN
	这时,其实是建立了两个分别不一样的 built_net 函数环境
A环境:
train_op, cost, layers_inputs: 
___________________________________________________
|build_net (A)                                	  |					
|                                                 |
|  train_op: 	//仅属于这里的train_op            |
|  _____________________________                  |
|  |Gradient.minimize (A)      |                  |
|  |                           |                  |
|  |___________________________|                  |
|                                                 |
|    ^                                            |
|    |                                            |
|  cost:	//仅属于这里的cost                |
|  _____________________________                  |
|  |reduce_mean (A)            |                  |
|  |                           |                  |
|  |___________________________|                  |
|                                                 |
|    ^                                            |
|    |                                            |
|  prediction:     //仅属于这里的prediction       |
|  __________________________________________     |
|  |add_layer  (A,n+1)                      |     |
|  |    这个add_layer操作 自己的的局部变量有 |     |
|  |    weights,biases                      |     |
|  |    Wx_plus_b,                          |     |
|  |    outputs                             |     |
|  |________________________________________|     |
|                                                 |
|    ^                                            |
|    |                                            |
|  layers_inputs[n]:     //仅属于这里的prediction |
|  __________________________________________     |
|  |add_layer  (A,n)                        |     |
|  |    这个add_layer操作 自己的的局部变量有 |     |
|  |    weights,biases                      |     |
|  |    Wx_plus_b,                          |     |
|  |    outputs                             |     |
|  |________________________________________|     |
|                                                 |
|    ^                                            |
|    |                                            |
|  layers_inputs[n-1]:   //仅属于这里的prediction |
|  __________________________________________     |
|  |add_layer  (A,n-1)                      |     |
|  |    这个add_layer操作 自己的的局部变量有 |     |
|  |    weights,biases                      |     |
|  |    Wx_plus_b,                          |     |
|  |    outputs                             |     |
|  |________________________________________|     |
|                                                 |
|  ..........                                     |   
|                                                 |
|  layers_inputs[0]:     //仅属于这里的prediction |
|  __________________________________________
|  |add_layer  (A,0)                        |     |
|  |    这个add_layer操作 自己的的局部变量有 |     |
|  |    weights,biases                      |     |
|  |    Wx_plus_b,                          |     |
|  |    outputs                             |     |
|  |________________________________________|     |
|                                                 |
|    ^                                            |
|    |                                            |
|    xs                                           |
|_________________________________________________|

B环境:
train_op, cost, layers_inputs: 
___________________________________________________
|build_net (A)                                	  |					
|                                                 |
|  train_op: 	//仅属于这里的train_op            |
|  _____________________________                  |
|  |Gradient.minimize (A)      |                  |
|  |                           |                  |
|  |___________________________|                  |
|                                                 |
|    ^                                            |
|    |                                            |
|  cost:	//仅属于这里的cost                |
|  _____________________________                  |
|  |reduce_mean (A)            |                  |
|  |                           |                  |
|  |___________________________|                  |
|                                                 |
|    ^                                            |
|    |                                            |
|  prediction:     //仅属于这里的prediction       |
|  __________________________________________     |
|  |add_layer  (A,n+1)                      |     |
|  |    这个add_layer操作 自己的的局部变量有 |     |
|  |    weights,biases                      |     |
|  |    Wx_plus_b,                          |     |
|  |    Wx_plus_b = f(Wx_plus_b)            |     |
|  |    outputs                             |     |
|  |________________________________________|     |
|                                                 |
|    ^                                            |
|    |                                            |
|  layers_inputs[n]:     //仅属于这里的prediction |
|  __________________________________________     |
|  |add_layer  (A,n)                        |     |
|  |    这个add_layer操作 自己的的局部变量有 |     |
|  |    weights,biases                      |     |
|  |    Wx_plus_b,                          |     |
|  |    Wx_plus_b = f(Wx_plus_b)            |     |
|  |    outputs                             |     |
|  |________________________________________|     |
|                                                 |
|    ^                                            |
|    |                                            |
|  layers_inputs[n-1]:   //仅属于这里的prediction |
|  __________________________________________     |
|  |add_layer  (A,n-1)                      |     |
|  |    这个add_layer操作 自己的的局部变量有 |     |
|  |    weights,biases                      |     |
|  |    Wx_plus_b,                          |     |
|  |    Wx_plus_b = f(Wx_plus_b)            |     |
|  |    outputs                             |     |
|  |________________________________________|     |
|                                                 |
|  ..........                                     |   
|                                                 |
|  layers_inputs[0]:     //仅属于这里的prediction |
|  __________________________________________
|  |add_layer  (A,0)                        |     |
|  |    这个add_layer操作 自己的的局部变量有 |     |
|  |    weights,biases                      |     |
|  |    Wx_plus_b,                          |     |
|  |    Wx_plus_b = f(Wx_plus_b)            |     |
|  |    outputs                             |     |
|  |________________________________________|     |
|                                                 |
|    ^                                            |
|    |                                            |
|    b(xs)  //b()是做了batch-noralization (bN)    |
|    ^      //f()是先BN,最做激活函数处理           |
|    |                                            |
|    xs                                           |
|_________________________________________________|

	然后sess.run 的使用,就是重新利用这些已存在的函数空间,
	并不是再重新建立新空间!!
	虽然for循环 调用了多个 add_layer,
	但是,每一个 add_layer 都不一样.都有自己的weights等变量
	然而比较困惑在于, 每个 不一样的 add_layer 都把结果都给同一个 output变量
	那么gradient-minimize时,就追溯到 output 时 就不知道会发生什么事情.
	还有一个是 Wx_plus_b = f(Wx_plus_b) 的问题
	同样追溯到 Wx_plus_b 时, 又不知道要如何反向传导下去了,
	因为他们都请求自己,不过是请求之前的自己
	反向传导时应该会分析到 同个Wx_plus_b, output 变量被赋值的先后次序吧.
以上是值得思考的代码细节问题!!
	但先不再深究了








20180421
开始学习 强化学习!
学习分析 Q-learning 算法!!
	以地图游戏,寻找出口例子分析:
		先建立 一张 q 表, 记录每一位置点 s 的 所有行为 a 的权值 例如:
		       left     right        up      down
		0  0.000000  0.004320  0.000000  0.004320
		1  0.000000  0.025005  0.000400  0.004320
		2  0.000030  0.111241  0.000002  0.004320
		3  0.000000  0.368750  0.000000  0.004320
		4  0.027621  0.745813  0.000099  0.004320
		5  0.000000  0.000000  0.000000  0.004320
		Q(s,a)函数是得出 s 点,a 行为的 的 权值,
		例如 Q(4,right) = 0.745813, right:0.745813比其他行为的权值高,
		所以 在 4 点 采取 right行为 是最为推荐的选择.
		每个学习回合(episode)初,先确定好 q 表, 
		如果是开始回合,就初始化q表,否则沿用上回合最终更新的q表
	每个学习回合开的每一步工作内容如下:
		在当前点 s , 通过行为选取算法选取 一个行为 a
			其中 有 e_greedy 概率通过q表权值操作来选取 行为 a, 一般选最大权值的行为
			有 1-e_greedy 概率 随机选取行为!!
		执行操作 a , 根据操作到达下个点 s_, 并根据奖励算法得到行为应有的奖励 r
		询问q表得到 下个点 s_ 的 最大权值的行为 max_a_
		取得 Q(s_,max_a_) 为我们的参考权值
		执行学习更新 当前点s,当前行为a的 权值 Q(s,a):
			Q(s,a) = Q(s,a) + Alpha * [ r + gama * Q(s_,max_a_) - Q(s,a) ]
			Alpha是学习率
			GAMMA 是 discount factor
	重复一步又一步,直到第某步,执行操作 a 到达的下一点 s_ 是目标点,或者是陷阱点
		时,才正式结束结束这一回合.准备开始下一回合.				
	这里 奖励算法和行为选取算法都是自定义的,需要根据实际情况谨慎设计
	这里有必要详细讨论学习原理:
		假如当前 s:3 选出的 a:up 是 s 里最大权值行为, 
		我们也知道 a:up 后 到达的 S_:1 的最大权值行为是 max_a_:left
		如果 r + gama * Q(s_,max_a_) - Q(s,a) 很小,
		意味着 s:3 执行 a:up 后到 s_:1 然后执行 max_a_:left 是当前认为最正确操作流程
		如果 r + gama * Q(s_,max_a_) - Q(s,a) 很大时,
		s:3 执行 a:up 后到 s_:1 然后执行 max_a_:left 这串操作流并不合适
		于是, 把 r + gama * Q(s_,max_a_) - Q(s,a) 反馈更新 s:3 中 a:up 的权值,
		使得以后有机会重新回到 s:3 时,选出行为 a:up 的概率变小,
	逆向思考学习原理:
		假如有一个充分学习后的模型.q值表是这样的:
		       left     right        up      down
		0  0.999999  0.004320  0.000000  0.004320
		1  0.000000  0.025005  0.000400  0.004320
		2  0.000030  0.999999  0.000002  0.004320
		3  0.000000  0.368750  0.000000  0.004320
		4  0.027621  0.745813  0.999999  0.004320
		5  0.000000  0.000000  0.000000  0.004320
		6  0.000000  0.999999  0.000000  0.004320
		7  0.000000  0.025005  0.000400  0.004320
		8  0.000030  0.111241  0.999999  0.004320
		9  0.000000  0.000000  0.000000  0.000000
		10 0.027621  0.745813  0.000099  0.004320
		11 0.033030  0.000000  0.564363  0.004320
		先从地点 0 开始:
			0:left -> 4:up -> 2:right -> 6:right -> 8:up -> 9终点
			以后的回会没有意外都会沿着这条路径跑
		因为 s:0:a:left -> s_4:max_a_:up 的  r + gama * Q(s_,max_a_) - Q(s,a) 很小
		Q(0,left)就几乎变化,所以下次到达地点0时,还会很大机会选择left行为到 地点4
		同样道理, Q(4,up),Q(2,right),Q(6,right),Q(8,right) 都几乎不变化
		所以 0:left -> 4:up -> 2:right -> 6:right -> 8:up -> 9终点 
		就是一条当前最为正确 操作流
	学习原理的假设并没有问题,但其实之所以 有 r + gama * Q(s_,max_a_) - Q(s,a) 
		这种算法方式 是基于基本强化学习模型 MDP 模型的的数学方程 简化后 
		才有的这种乍一看超级迷离,并不知道如何解释的这个算式
	Q-learning 也称 off-policy, 是因为选取 s_ 参考权值行为只选取最大权值的,
		因为只去看 s_ 哪个行为权值最大 ,就选他 (max_a_) 
		意味着,下一步最有可能执行这个行为
		下一步执行这个最有可能的行为 max_a_ 后,
		并无法知道 会在下下步到达 的地方 S__ 会不会是陷阱或禁止区域什么的
		这样子,前进就比较冒进,勇敢, 被认为是缺乏策略性的学习行为
		
学习分析 Sarsa 算法!!
	同样以地图游戏,寻找出口例子分析:
		先建立 一张 q 表, 记录每一位置点 s 的 所有行为 a 的权值 例如:
		       left     right        up      down
		0  0.000000  0.004320  0.000000  0.004320
		1  0.000000  0.025005  0.000400  0.004320
		2  0.000030  0.111241  0.000002  0.004320
		3  0.000000  0.368750  0.000000  0.004320
		4  0.027621  0.745813  0.000099  0.004320
		5  0.000000  0.000000  0.000000  0.004320
		Q(s,a)函数是得出 s 点,a 行为的 的 权值,
		例如 Q(4,right) = 0.745813, right:0.745813比其他行为的权值高,
		所以 在 4 点 采取 right行为 是最为推荐的选择.
		每个学习回合(episode)初,先确定好 q 表, 
		如果是开始回合,就初始化q表,否则沿用上回合最终更新的q表
	每个学习回合开的每一步工作内容如下:
		当前点 s , 有一个确认的行为 a
			如果不是执行在第一个回合 当前点s 是上一步的 s_, 
			行为 a 是上一步 s_ 在上一步通过 行为选取算法 得到的 a_
			如果是执行在第一个回合 当前点 s , 通过行为选取算法得到行为 a 
			行为选取算法典型有以下操作:
			有 e_greedy 概率通过q表权值操作来选取 行为 a_, 一般选最大权值的行为
			也有 1-e_greedy 概率 随机选取行为 a_!!
		执行操作 a , 根据操作会到达下个点 s_, 并根据奖励算法得到行为应有的奖励 r
		Q-learning 和 sarsa 算法 区别在于 参考权值 选取方式不一样
		再一次通过 行为选取算法 给S_ 选取 一个行为 a_
		取得 Q(s_,a_) 为我们的参考权值
		还有的是,下一步的这个行为a_,在下一步 s_ 点 一定会执行的!! 
		执行学习更新 当前点s,当前行为a的 权值 Q(s,a):
			Q(s,a) = Q(s,a) + Alpha * [ r + gama * Q(s_,a_) - Q(s,a) ]
			Alpha是学习率
			gama 是 discount factor
	重复一步又一步,直到第某步,执行操作 a 到达的下一点 s_ 是目标点,或者是陷阱点
		时,才正式结束结束这一回合.准备开始下一回合.
		然而一般可以很好地避开陷阱点.				
	这里 奖励算法和行为选取算法都是自定义的,需要根据实际情况谨慎设计
	Sarsa 也称 on-policy, 因为是通过 行为选取算法 选取 s_ 的 下一步行为 a_作为参考权值行为
		下一步一定会执行这个行为
		下一步一定会执行的 a_ 是通过 行为选取算法选出的,
		必须强调的是,s与s_使用相同,行为选取算法,而这个算法的设计还是有一定讲究的
		开始好几个回合, s 选取的 a 或者是 s_ 选取的 a_ 都有可能使下一步掉入陷阱区,
		多次回合后,q表有初步的更新,然后行为选取算法选出的行为 一般都是能避开陷阱区的行为
		所以,多个回合后,后面学习 执行的行为 一般都可以很好避开陷阱,比较谨慎的前进.
		就好象会察觉到危险而不会贸然跌入陷阱那样有策略地前进.

学习分析 Sarsa(lamda) 算法!!
	同样以地图游戏,寻找出口例子分析:
		先建立 一张 q 表, 记录每一位置点 s 的 所有行为 a 的权值 例如:
		       left     right        up      down
		0  0.000000  0.004320  0.000000  0.004320
		1  0.000000  0.025005  0.000400  0.004320
		2  0.000030  0.111241  0.000002  0.004320
		3  0.000000  0.368750  0.000000  0.004320
		4  0.027621  0.745813  0.000099  0.004320
		5  0.000000  0.000000  0.000000  0.004320
		Q(s,a)函数是得出 s 点,a 行为的 的 权值,
		例如 Q(4,right) = 0.745813, right:0.745813比其他行为的权值高,
		所以 在 4 点 采取 right行为 是最为推荐的选择.
		每个学习回合(episode)初,先确定好 q 表, 
		如果是开始回合,就初始化q表,否则沿用上回合最终更新的q表
		此外,一张 e 表,
		e表结构与q表一致,但是e表的成员意义 不是 行为权值, 而是行为相关度 例如:
		       left     right        up      down
		0         5         1         0         0
  		1         0         7         1         3
		2         2         4         1         3
		3        10         0         0         1   
		4         0         0         0         0 
		5         3         6         1         9
		回合的每一步都会根据当前步更新了的 e 表, 整体更新 q 表所有行为权值		
		假设此刻为回合的最后一步,已经到达终点,
			此刻的 e 表 记录的相关度可以这么说说明一个事实:
			要到达终点,在地点0 left行为比较相关,在地点0,更应该执行 left 行为,同理
				在地点1,更应该执行 right 行为
				在地点2,更应该执行 right 行为
				在地点3,更应该执行  left 行为
				在地点5,更应该执行  down 行为
				在地点4,是终点.
			因此这一步,更新 q 表权值时,
			更新 0:left, 1:right, 2:right, 3:left, 5:down 位置的权值的幅度比较大
			这些位置的 权值有比较显著的提高.
		假设此刻为并非回合的最后一步.
			此刻的 e 表 记录的相关度可以这么说说明一个事实:
			如果要到达像这一刻所处的位置,或者状态:
				在地点0,更应该执行 left 行为
				在地点1,更应该执行 right 行为
				在地点2,更应该执行 right 行为
				在地点3,更应该执行  left 行为
				在地点5,更应该执行  down 行为
				由于还没有到达过地点4,
				所以地点4,并未采取过行为,
				所以并没有行为相关度统计
				所以并不知道地点4更应该执行啥,
				也不知道 地点4 是终点还是其他
			这一步,更新 q 表权值时,同样地,
			更新 0:left, 1:right, 2:right, 3:left, 5:down 位置的权值的幅度比较大
			这些位置的 权值有比较显著的提高.
			并不更新 地点4 的权值
		e表 相关度 的统计方式可以有很多种,上述表是每当执行一次对应行为,就在e表对应项加1
		还有其他的统计方式,比如也有像以下一样的 e表
		       left     right        up      down
		0       0.5      0.91       0.1      0.22
  		1       0.3      0,43      0.87     0.334
		2         0         0      0.13      0.34
		3       0.1     0.465     0,112       0.3  
		4     0.445         0     0.345       0.9
		5      0.13      0.56      0.22         0		
		每个回合开始前, e 表所有项 都先被赋0										 
	每个学习回合开的每一步工作内容如下:
		当前点 s , 有一个确认的行为 a
			如果不是执行在第一个回合 当前点s 是上一步的 s_, 
			行为 a 是上一步 s_ 在上一步通过 行为选取算法 得到的 a_
			如果是执行在第一个回合 当前点 s , 通过行为选取算法得到行为 a 
			行为选取算法典型有以下操作:
			有 e_greedy 概率通过q表权值操作来选取 行为 a_, 一般选最大权值的行为
			也有 1-e_greedy 概率 随机选取行为 a_!!
		执行操作 a , 根据操作会到达下个点 s_, 并根据奖励算法得到行为应有的奖励 r
		类似 sarsa 算法,再一次通过 行为选取算法 给S_ 选取 一个行为 a_
		取得 Q(s_,a_) 为我们的参考权值
		然后得到误差值 error = [ r + gama * Q(s_,a_) - Q(s,a) ]
			                gama 是 discount factor
		注意,跟sarsa差不多,下一步的这个行为a_,在下一步 s_ 点 一定会执行的!! 
		接着讨论 e 表, E(s,a) 表示 e表, 地点s,行为a 的相关值,比如当前e表为
			       left     right        up      down
			0       0.5      0.91       0.1      0.22
	  		1       0.3      0,43      0.87     0.334
			2         0         0      0.13      0.34
			3       0.1     0.165     0,112       0.3  
			4     0.445         0     0.345       0.9
			5      0.13      0.56      0.22         0
		通过相关值统计算法 更新 e表中 地点s,行为a 的相关值, 
		比如加 0.3 算法, 比如 s:3, a:right
			E(s,a) = E(s,a) + 0.3
		即 e 表变为:
		接着讨论 e 表, E(s,a) 表示 e表, 地点s,行为a 的相关值,比如当前e表为
			       left     right        up      down
			0       0.5      0.91       0.1      0.22
	  		1       0.3      0,43      0.87     0.334
			2         0         0      0.13      0.34
			3       0.1     0.465     0,112       0.3  
			4     0.445         0     0.345       0.9
			5      0.13      0.56      0.22         0
		然后 就 通过 e 表整体更新 q表: 譬如当前 e 表 q 表为:
			       left     right        up      down
			0  0.000000  0.004320  0.000000  0.004320
			1  0.000000  0.025005  0.000400  0.004320
			2  0.000030  0.111241  0.000002  0.004320
			3  0.000000  0.368750  0.000000  0.004320
			4  0.027621  0.745813  0.000099  0.004320
			5  0.000000  0.000000  0.000000  0.004320	
		更新后的 q 表: (注意是按位算法,不是矩阵算法)
	    left     right        up      down                     left  right    up  down
	|0.000000  0.004320  0.000000  0.004320|                 |  0.5   0.91   0.1  0.22|
	|0.000000  0.025005  0.000400  0.004320|                 |  0.3   0,43  0.87 0.334|
	|0.000030  0.111241  0.000002  0.004320|                 |    0      0  0.13  0.34|
	|0.000000  0.368750  0.000000  0.004320| + Alpha * error |  0.1  0.465 0,112   0.3|
	|0.027621  0.745813  0.000099  0.004320|                 |0.445      0 0.345   0.9|
	|0.000000  0.000000  0.000000  0.004320|                 | 0.13   0.56  0.22     0|
						   Alpha是学习率
		然后整体更新 e 表:
                                  left  right    up  down
                                |  0.5   0.91   0.1  0.22|
                                |  0.3   0,43  0.87 0.334|
                 gama * lamda * |    0      0  0.13  0.34|
                                |  0.1  0.465 0,112   0.3|
                                |0.445      0 0.345   0.9|
                                | 0.13   0.56  0.22     0|
		gama 还是上述的那个 gama
		lamda 就是 Sarsa(lamda) 提到的 lamda
	到这里,这一步的内容就算是完结了,下一步同样重复这样的内容!!
		直到第某步,执行操作 a 到达的下一点 s_ 是目标点,或者是陷阱点时,
		才正式结束结束这一回合.准备开始下一回合.
	这种算法如 sarsa 一般可以很好地避开陷阱点, 而且比 sarsa 的 学习速度更快.
	这种算法其实就是 sarsa 变种,下一步的这个行为a_,在下一步 s_ 点 一定会执行的!!	
	现在 讨论 lamda 的作用意义:
	如果 lamda 设定为0, 
		那么可以看到,每一步执行前, e 表所有项都是 0
		经过 E(s,a) = E(s,a) + 0.3 后,只有 地点s,行为a 那项目有相关值 E(s,a)
		然后更新 q 表的时候,其实就只是更新 地点s,行为a 的权值.
		本来其他操作都跟 sarsa 一样, 
		加上这里:每一步更新权值,只更新 地点s,行为a 的权值
		这样就跟 sarsa 一模一样了. 这样有单步更新的样子
	如果 lamda 设定为1, 	
		那么, 在这回合里, 每一步的统计行为 E(s,a) = E(s,a) + 0.3 都被记录
		如果这一步还没到达 终点,
			那么从回合开始到现在,所执行的所有行为都被记录
			其中那些多次被执行的行为, 相关值比较高!
			被认为是能到达当前地点所 相对必要执行的行为
			所以,这一步更新 q 表时,更要大幅度更新那些多次被执行的行为的权值.
		如果这一步到达 终点,
			那么从回合开始到回合结束,所执行的所有行为都被记录
			那些多次被执行的行为, 相关值比较高!
			被认为是能到达终点所 相对必要执行的行为
			所以,这一步更新 q 表时,更要大幅度更新那些多次被执行的行为的权值.
			这样就有 我们所认识 的回合更新 的样子
	如果 lamda 设定为 0~1 之间,
		那么, 在这回合里, 每一步的统计行为 E(s,a) = E(s,a) + 0.3 
		随着新一步的到来,被弱化一次.
		比如 A时刻,当前处于地点0,
			这一步执行了left,统计了一次行为 E(0,left) = E(0,left) + 0.3
		 	然后更新 q 表.
			然后更新 e 表,
			更新 e 表的时候,乘上了 lamda ,即整体弱化了 e 表 所相关度
		经过n步后,到达b时刻 ,处于 地点5,
			这段时间未回到 地点0 执行left,
			那么 e表 地点0 所有行为包括left 的相关度 就被弱化了n次.变得非常低
			那么 说明,要到达地点5, 与在地点0不管执行什么操作,并没有什么关系
		这样就有一个这样子的推论.
			回合开始,经过 m 步到达 终点,回合结束
			开始点的行为是什么,并不重要.
			但慢慢的,越靠近终点的 地点,可能两三步就到终点了,
			他们的 行为 的相关度 显得相当重要.
			就是说要到达终点,开始点的行为并不重要
			越靠近终点,的地点的行为,相关度受到重视,
			对应的 q 表权值的 更新幅度也就比较大
			就好像越靠近终点,就越能看到到终点的路一样
		这样看起来 有一种介于 单步更新 和 回合更新 之间的样子		   				
	奖励算法,行为选取算法,相关值统计算法 都是自定义的,需要根据实际情况谨慎设计
	Sarsa(lamda) 是变种 sarsa,所以也称 on-policy, 
		因为也是通过 行为选取算法 选取 s_ 的 下一步行为 a_作为参考权值行为
		下一步一定会执行这个行为
		下一步一定会执行的 a_ 是通过 行为选取算法选出的,
		再三强调,s与s_使用相同,行为选取算法,而这个算法的设计还是有一定讲究的
		开始好几个回合, s 选取的 a 或者是 s_ 选取的 a_ 都有可能使下一步掉入陷阱区,
		多次回合后,q表有初步的更新,然后行为选取算法选出的行为 一般都是能避开陷阱区的行为
		所以,多个回合后,后面学习 执行的行为 一般都可以很好避开陷阱,比较谨慎的前进.
		就好象会察觉到危险而不会贸然跌入陷阱那样有策略地前进.
		而且 e 表 和 lamda 的 补充 , 学习速度更快!!!
	关键小结:
		注意理解 lamda=0 相当于 sarsa算法 相当于但不更新
			lamda = 1 相当于 回合更新
			lamda = [0,1]之间时 相当于越靠近终点,越有把握选择正确步数	
		每一个新回合开始前,都必须 重置 相关度表E 全为0.
		每一步都 更新 q表和 E 表全部内容
		每一回合到达终点时, E表都记录了这个回合里 那些地点哪些行为执行得比较多
		表示这些 地点的行为相关度比较高,将更大幅度更新对应权值

奖励算法同样重要和讲究:
注意理解分析 到达重点才给奖励的行为, 这样靠近终点的地点的 的最大权值行为的权值比较高,
	靠近起点的最大权值行为的权值相对低些.
	即越靠经终点越有把握	
还有一个是 每一步都给同样奖励的行为,可能导致并无法学习
	每一步都给同等奖励0的行为,相当于每一步都没有给奖励


code.py 是一个基于 Q-learning强化学习的小例子
这个例子是 角色o 寻找最佳路径到达T
这个例子奖励方式是,如果S_到达目的点时,当前点s就得到奖励

///////////////////以下的奖励算法策略思维有所保留,似乎不适合讨论在这个例子上
然而,比较不好说服的是,只要向右走就给奖励!! 
	因为我们知道 位置T在最右边,直观知道一直往右走就是了,
	但这样就很难分辨是Q-learning算法的可靠性,
	意义上不是寻找最佳路径,而是推荐不断往右跑	
	这样就没有学习的意义了
推荐合理的给奖励方法之一是,距离近了一点就给奖励!!
推荐最合理的给给奖励方法之二:
	统计每一回合的移动数, 当前回合比前一回合移动步数少的时候,执行以下操作,
	地图上有6个位置, 对应Q表有6行
	统计每个位置当前回合,执行最多次数的操作,被选为 加奖励操作
	然后,下一回合,
	每个位置,当前操作是否加奖励,根据上回合的决定!!
////////////////////////////////////////
基于 Q-learning , sarsa , sarsa_lamda 的例子 后面再详细分析













20170425
deep q-learning network (DQN)
对比 q-learning ,DQN 是通过 深度网络,代替 q 表 的 Q-learning 强化学习方法 
初步了解:
	比如我们当前地点 s 通过网络, 我们得到 s 对应的各个行为 的 权值队列
	例如
			left     right        up      down
	s:3 ->NN -> 0.000000  0.368750  0.000000  0.004320
	这里的学习跟 q-learning 的学习 有点不一样:
	首先重申, q-learning 的学习原理:
	假如 模型充分学习, 游戏角色可以 通过 一条连贯的路径到达终点
		每个踩点之间有 基本的联系,
		表现为 s点最大权值行为,与下一个s_点的最大权值行为的权值差比较小
		即 Q(s_,max_a_)-Q(s,max_a) 比较小,
		即是说在 s很有机会采取max_a行为到达s_点后,
		在s_点又很有机会采取max_a_行为
		行为之间就像有连续性.
	我们知道, q-learning 的学习 是 即时更新 q 表 某个权值的
		Q(s,a) = Q(s,a) + Alpha * [ r + gama * Q(s_,max_a_) - Q(s,a) ]
	但 DQN 不是,他分开两个网络, A网络是 计算出 s->QA(s,a)
		B网络是 计算出 s_->QB(s_,max_a_)
		典型,每一回合开始前,AB网络相同
		1,行走多步,得到多个 s_->QB(s_,max_a_) s->QA(s_,max_a)
		2,然后随机抽取其中 batch 步,组成一包数据
		  统筹计算 loss = r + gama * QB(s_,max_a_) - QA(s,a) 
		  此刻, AB网络还是一样!!
		3,然后通过 minimize loss 更新 了A网络
		先不覆盖,这时 A网络与B网络已经不一样
		仍然重复 一次或多次 1,2,3,步,继续更新一次或多次 A网络后
		最后把A网络拷贝覆盖B网络!!
	这种做法,相当于这样更新 Q-learning 的q表!
		有两张 q 表, A表用来计算权值QA(s,a), B表用来计算权值QB(s_,max_a_)
		先执行多步,记录每一步的 QB(s_,max_a_) 和 QA(s,a) 记为 y?, z? ?表示步数记号
		但每一步并不即时更新 A表,
		即每一步不执行 QA(s,a) = QA(s,a) + Alpha * [ r + gama * QB(s_,max_a_) - QA(s,a) ]
		等到结束或者 走了n步后,
		才整体更新 A表,
		即执行 QA(s,a) = QA(s,a) + Alpha * [ r + gama * y1 - z1 ]
		       QA(s,a) = QA(s,a) + Alpha * [ r + gama * y2 - z2 ]
			....
		       QA(s,a) = QA(s,a) + Alpha * [ r + gama * yn - zn ]
		更新了A表, B 表仍然不变!! 
		A表依然用来计算算QA(s,a), B表依然用来计算QB(s_,max_a_)
		重复上述更新 A 表的 组合操作, 多次更新 A 表
		然后把 A 表 拷贝更新覆盖 B表
		这一轮的学习就算结束.
		下一轮的学习重复上述过程
初步了解 DOUBLE DQN	
	与 DQN 的区别是 r + gama * QB(s_,max_a_) 这一步,
	DQM是通过 B网络得到 QB(s_,max_a_)的
	DOUBLE DQN 是先 通过  当前 A 网络 得到 s_ 输出的 a_ 的权值
		其中 权值最大的 行为记为  angmax_a  
	然后, 替换成 r + gama * QB(s_,angmax_a)
初步了解 Prioritized Experience Replay DQN (优化记忆DQN)
	与 DQN 的区别是 "随机抽取其中 batch 步,组成一包数据" 这一步,
	PERDQN 并bu会随机抽选 而是有有选择针对的 的抽选
	一般  r + gama * QB(s_,max_a_) - QA(s,a) 我们称为 TD_error
	TD_error 越大, 表明这一步 误差 比较大,被认为应该是重点学习的对象
	所以选出多个数据组成 batch 数据包,会选上这一步!! 
初步了解 Dueling DQN (优化记忆DQN)
	这个比较不好理解,虽然内容简单
	与 DQN 的区别: 
	DQN是: s -> NN -> 每个行为的权值

                       /当前 s 得到的的一个状态值 V \
	DDQN是: s -> NN                             各个行为的权值
                       \  每个行为的相关数值 A     /
	为什么这么拆分还没有明白



详细分析 梯度下降的的应用和意义
本来梯度的概念用于函数 是描述函数的变化程度
	例如: y = x^2 + 5 =f(x) 那么: f(x) 的某x值的梯度为 ∂y/∂x
	所谓梯度下降就是 寻找函数的最凹值的位置 这里可直观看到  ∂y/∂x 等于0时对应的 x 使得 y 有最小值
	就是我们的寻找的最凹值
	虽然,我们直观地找到了最凹值,但是,由于计算机的限制,并不能这样直观工作
	所以我们的通过以下逼近的方式找到 x 
	x <- x + Δx = x + r * ∂y/∂x
	在计算机上 ∂y/∂x 表示为:
		∂y/∂x = ( f(x+Δ) - f(x) ) / Δ
	Δ是一个无穷小量,但计算机无法得到无穷小值,所以是个超级小值
	r是步长幅度,表示逐步逼近的步长大小
	通过不断加一个小步长Δx的方式找到合适的 x 逼近到最凹值的方法,也是面对复杂问题的最常规方法
	这种常规的方法就是我们所说的梯度下降法
接下来,我们用一个 符合2次曲线散点图的拟合例子来 讨论 梯度下降的实际应用
	有一堆样本,例如有 200 个 [x,y] 点
	我们不知道他合乎什么样的曲线规则,所以想法去拟合,
	如果我们要求高度拟合,即过拟合,即每个点都几乎在同一根曲线上,就需要匹配高次函数模型
		例如: A*x^6 + B*x^5 + C*x^4 + D*x^3 + E*x^2 + F*x + G = y = f(x)
	如果我们要求适当拟合, 匹配3次或者2次函数模型即可
		例如: D*x^3 + E*x^2 + F*x + G = y = f(x)
	这里我们匹配一个 3次函数模型.
	如果一个样本很好符合曲线,表示样本与曲线的误差error比较小,
		为了方便计算,我们要求误差大于0,
		即: error(x,y) = (f(x)-y)^2.
	如果一条曲线很好地符合所有样本,那么就是 200 个样本 每一个样本的误差加起来最小
		即: ∑ (error(x,y)) 最小
		也可以这样表示  ( ∑ (error(x,y)) ) / 200 最小
		我们记为 loss = ( ∑ (error(x,y)) ) / 200
	最小化loss,想相当于loss函数里,200个样本参数值作为已知常数,
		求未知数 D,E,F,G,的合理值,使得 loss最小
		即: loss = J(D,E,F,G).
	那么就是对 D,E,F,G 求偏导,并逐步逼近,即执行梯度下降
		D <- D + ΔD = D + r * ∂j/∂D,    ∂j/∂D = ( j(D+Δ) - j(D) ) / Δ
		E <- E + ΔE = E + r * ∂j/∂E,    ∂j/∂E = ( j(E+Δ) - j(E) ) / Δ
		F <- F + ΔF = F + r * ∂j/∂F,    ∂j/∂F = ( j(F+Δ) - j(F) ) / Δ
		G <- G + ΔG = G + r * ∂j/∂G,    ∂j/∂G = ( j(G+Δ) - j(G) ) / Δ
	重复多次,200个样品并当作常数代入loss执行梯度下降工作
	直到loss收敛到最小
	一般loss最小时,∂j/∂D,∂j/∂E,∂j/∂F,∂j/∂G 都是几乎接近0
		但是 ∂j/∂D,∂j/∂E,∂j/∂F,∂j/∂G 都几乎接近0的点可能有很多个
		所以我们得到的loss最小值可能是局部最小值不是全局的!!
		即有可能有很多凹点,得到的最凹点不一定是最好的最凹点
		一般来说,局部最凹点已经可以认为是最符合的结果了,
		如果结果不理想,要得到更好的或全局最凹点,就要做更多深入的工作!!
	至此得到的 D,E,F,G 组成的 D*x^3 + E*x^2 + F*x + G = y 就是我们拟合的最合理曲线
现在,假如我们有 10000000个样本, 把这么多个样本放入loss函数并不合理!!
	这时,我们可以随机选200个样品代入loss作为常数,对变量 D,E,F,G执行一次梯度下降
	重复多次,每次选200个样品并当作常数代入loss执行梯度下降工作
	这同样可以 逐步逼近 loss 的 局部最凹点,得到我们想要的结果
	这里 我们有一个 batch 的思维.
	代入loss的样本越少,拟合的曲线越有局部行为的匹配性, 
	但是每次都代入不同样品,局部匹配性就会打破,慢慢向全局匹配靠拢
	就是所,每次梯度下降操作仅针对当前填入的样品
	每一次都添加不同样品 每次梯度下降的方向都有不同程度的跑偏
	也就是说:
	每次都代入10000000个样本参数作为常数的loss执行梯度下降工作, 最后loss 得到的局部最后凹值
	和
	每次都代入随机200个样本为一batch的参数作为常数的loss执行梯度下降工作, 最后loss 收敛到的最小值 
	几乎是一样的
	这就是我们为什么 在庞大的样品库,可以 随机抽取多个组成一batch样品数据代入loss 执行梯度下降
最后就是我关于loss函数 思维的改变:
	以前未好好分析 梯度下降时,学习tensorflow 
	很多例子喂数据都是 一batch一batch的喂,而当时我却不解,
	我觉得应该是一个个样平地喂
	所以当后面看看到 loss 都是 loss = (∑ (error())) / batch,
		我就错认为等同于 loss = error( ∑(每个样品)/batch )
	这种错误的认识在于先  ∑(每个样品)/batch ,再 error() 
		会忽略了很多样品细节,破坏样品信息的完整性
		然后得到的 error 是比较有缺陷的,
		最后甚至不能让loss收敛
	事实上,每次只喂一个样平,理论上同样可行,上述虽然相当于每次只喂一个样品
		但他喂的是平均样品,会导致收敛速度几何级变慢
		同样地,即使每次喂的一个样品不是平均样品.也不建议,因为运算量大,收敛速度慢!!
	为什么喂batch数据 (∑ (error()))后会 / batch?
		只不过是让最后的loss值看起来不要太大.方便处理结果
		可以不需要的!!
		特别注意很多AI算法原理图都没有 / batch 这一步
机器学习的梯度收敛行为
	已知 x,y 符合一个模型A,而 x经过现有模型得到 y_ 与 y 有误差 loss
	将误差降到最低时,现有模型就几乎等于模型A了
	求loss 对现有模型参数 Θ 梯度,执行梯度下降
	取得最最优参数 Θ 使 loss 收敛到最小时 现有模型就几乎等于模型A
强化学习的梯度收敛行为
	以policy gradient 为例子
	机器最后学习到一种行为时,其行为策略会使得当前一套行为流程再整个episode取得 最大的价值 J(Θ)
	Θ是策略的参数
	自然这种情况下 J(Θ) 对 Θ 的梯度也是相当接近 0 的 !!
	因为如果梯度特别大,那证明同样的行为, J(Θ) 可以变得更大,
	Θ还有有变化的空间,那么表示策略还有改动的可能.即 行为的学习还有极大的空间











20180430
policy Gradients 与 Q-learning大家族不一样的另一个 RL家族
Policy Gradient 本质是以时间点为单元 当前状态 s 输入 策略网络,得到 的动作输出!!
初步了解
这里 s -> NN -> a
策略 πθ(s,a) 这里是 s通过网络选出的a,与实际选择的a的 交叉熵
注意,我们的目标要认清 policy gradient 的算法原理, 认清例子具体每一步对应原理的那个部分
目前观察看,例子好像缺了好多应有的部分
参考
https://blog.csdn.net/amds123/article/details/70242042

突然间忘记了梯度训练的相关概念:
理解 马尔可夫决策链（Markov Decision Process）
https://blog.csdn.net/bravacristina/article/details/78540779
http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching_files/MDP.pdf
RL系统课程
http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching_files/pg.pdf
http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html

https://blog.csdn.net/amds123/article/details/70242042
Gt = R.(t+1) + γ*R.(t+2) +...+ γ^(T−1)*R.(T)	//这是啥?? 这个是马尔可夫决策过程MDP的内容


https://www.cnblogs.com/mo-wang/p/4910855.html
另一家族无模型预测（Monte-Carlo Learning与Temporal-Difference Learning）


MDP 动态规划法(dynamic programming methods)
MDP 策略迭代方法是 每完成 一个 episode 才更新所有 的V(s) Q()等
    值迭代应该是 没走一步就更新一次当前V(s),并不等待完成了一个episode (后来认识到这个观点不属于值迭代)
    值迭代也有可能是完成 一个 episode 才更新所有 的V(s) 当更新当前的V(s) 是 跟上一个 episode的 V(S_)有关 
策略迭代:
从上面我们可以看到，策略迭代算法包含了一个策略估计的过程，而策略估计则需要扫描(sweep)所有的状态若干次，其中巨大的计算量直接影响了策略迭代算法的效率。我们必须要获得精确的Vπ值吗？事实上不必，有几种方法可以在保证算法收敛的情况下，缩短策略估计的过程。
这时由于 一个episode 到达 同一个 s 多次, 每一次都是一个新V(S),直到 完成这次迭代完成 V(S)才确定下来,所以过程中需要用到的V(S_)也在动态变化着
所以需要 (sweep)所有的状态若干次
值迭代:
它的每次迭代只扫描(sweep)了每个状态一次。
假如是 完成 一个 episode 才更新所有 的V(s) 因为 更新当前的V(s) 是 跟上一个 episode的 V(S_)有关 
所以再 更新所有 的V(s) 只 (sweep)一次所有的状态,就可以,因为 上一个 episode的 V(S_) 是已经确认下来的,并不会变化
动态规划的优点在于它有很好的数学上的解释，但是动态要求一个完全已知的环境模型，
这在现实中是很难做到的。另外，当状态数量较大的时候，动态规划法的效率也将是一个问题。
所以动态规划不需要实际上每个episode 都去执行实际的一连串行动 a,即不需要episode中执行过动作, 
每个episode 只有直接的迭代操作 ,即episode次数就是迭代次数, 还有用 k 表示 episode次数
只要知道所有的状态项位置,还有相关奖励机制,就开始迭代求出最优事件流!!
根本不需要 有agent 去执行一连串的实际操作 a 





MC  后面发现是我们 Qlearning 和sarar 的基础
注意 假如在一个不变的策略π 下,经过无限次episode 会收敛到这个π 下对应的行为
特别注意到,每个episode 的 每个 s 的 G 值都被记录!!都参与到 每次 得 V(s) 的计算中
如果策略π 也跟均实时的Q 在不断变化,经过无限次episode 就可能会收敛到这个系统的最好策略π,和最好的行为????
注意 http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching_files/MC-TD.pdf
     http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching_files/control.pdf
     ppt里的
	S1,A1,R2,...,Sk∼π
	{S1,A1,R2,...,ST}∼π
	都表示是 一个 episode 经过k(或者T)个timestep后 结束所记录下的参数
	例如 S1 表示 timestep 1 所处于的 状态s
	     A1 表示 timestep 1 所处于的 状态s下 采取的行动 a
	     R2 表示 timestep 1 所处于的 状态s下 采取了行动 a 后,将到达timestep 2 所处的状态s 而得到的奖励
	     Sk, ST 表示 timestep k (或者T) 所处于的 状态s 到达这个s时,episode已经结束,
	     所以不会有 Ak AT R(k+1) R(T+1)这些参数
注意 http://www.cnblogs.com/jinxulin/p/3560737.html里的
	的Vπ(s)≈ (2 + 1 – 5 + 4)/4 = 0.5 的 R1(s),R2(s),R3(s),,,
	对应着 http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching_files/MC-TD.pdf
     	       http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching_files/control.pdf
               ppt里的 的 Gt
	但是表达意义不一样
	这里 R2(s) 表示 第某个episode里,在 timestep t 到达状态s时 得到的 Rt+1+γRt+2+...+γT−1RT 
		而这次到达的 S 在所有episode 和所有timestep 统计中,是第2次到达
	而Gt仅仅就是在一个epispde中,在 timestep t 到达状态s时 得到的 Rt+1+γRt+2+...+γT−1RT 
	Rt+1+γRt+2+...+γT−1RT 表示 把 这个 episode 的 timestep t 后 所以 奖励R? 按加权平均得到的 价值,
		就是 现在所处 s 在这个 episode 的 timestep t 得到的应有价值
	特别注意 Gt 用于累计统计莫个特定 s 下的 V(s)
	所以 每个 Gt 都只为自己对应的那 个 s 服务, 不管是在哪个episode哪个timestep 到达这个 s
	只会被 这个 s 计算 V(s) 所用, 不会被其他 s 共用!!
	∑ (π Q(s,a)) = Vπ(s)??
	


policy gradient
πΘ(s,a) 是策略, 是关于参数 Θ 的 我们要求得到最好的 策略
可以通过 组建辅助观察函数 J(Θ) = ∑d(s)∑πθ(s,a)Rs,a
			        s    a
最大化(最小化??) J(Θ) 意味着 求得最好的 πΘ(s,a)

∑πθ(s,a)Rs,a 表示在当前s 每个行为a 通过策略 πθ(s,a) 后,得到的 奖励的统计值 
a
d(s)表示整个episode 能够在当前s 行使策略行为的概率,
∑d(s)(...) 表示把整个episode 每个 s 通过策略 πθ(s,a) 得到的 奖励的统计值 
s	即整个episode 在策略 πθ(s,a) 得到的统计奖励值!!
J(Θ) 应该是越大越好??


Nash equilibria 纳什均衡








20180528
经过漫长学习分析 终于切入了 policy gradient的思维
详见 policy gradient 代码, RL 是 policy gradient 算法
CartPole 和 MoutainCar 是两个例子
算法要点
根据算法定义:
function REINFORCE
    Initialise θ arbitrarily
    for each episode { s1,a1,r2,...,sT−1,aT−1,rT }∼πθ do
        for t = 1 to T−1 do
            θ ← θ + α * ∇θ ( logπθ(st,at)vt )
        end for
    end for
    returnθ
end function
	
这个例子的 policy gradient 算法 有几个重点
1,行为策略 a = πθ(s)
  在这里 在s状态 通过策略 πθ(s) 得到动作 a, 
  相应地 πθ(s,a) 的得到是 在s状态,获得动作a 对应的策略,表现为在s状态,获得动作几率
2,奖励 vt / v(t) / Gt / G(t) / R(sa) (有好多称谓),但都是同个概念,都是马尔科夫模型的一个概念之一
  当前 s 执行了a 得到 的奖励  v(t) = r + v(t+1)
  r是直接奖励,  v(t+1)是 下个 s 执行了a 得到 的奖励
  这个奖励模型拥有前瞻性意义!!,表示了当前s 执行a后得到的奖励 拥有了多少 导致episode成功结束 贡献价值
  即隐约告诉我们 是否更靠近成功的结束
3 根据 算法定义 我们的  loss 可以设置是 j(θ) =E[ ∑ -logπθ(st,at)vt ]
					    t
  logπθ(st,at) 与 πθ(s,a) 意义一样, 即在s状态,获得动作a 对应的策略,表现为在s状态,获得动作几率
  使用 logπθ(st,at) 是为了跟好计算
  假若 完全锻炼的 策略, loss 很小, 意味这 任一 timestep (即t) 都 选择了相当正确的行为a
   1, 大多数情况, 
      比如 t = 10, 执行了 a = πθ(s) 的 a , 得到很高的 vt (更近成功终点), πθ(s,a)几乎等于1
      然后 -logπθ(st,at) ≈ 0 所以 -logπθ(s,a)vt ≈ 0,表示对 loss 几乎没有贡献
      表示这一步使用 a 无比正确的策略,不用修正策略
   2, 几乎 有 1 - e_greedy 甚至更小 概率出现的情况 
      比如 t = 78, 执行了 a = πθ(s) 以外的 a1,但得到 很小的 Vt (更远离成功终点), πθ(s,a)比如等于0.1
      然后 -logπθ(st,at) ≈ 1 所以 -logπθ(s,a)vt ≈ vt, 表示 表示对 loss 几乎没有贡献
      表示这一步使用 a1 是错误的,所以策略还是正确的,不用修正策略 πθ(s),或者修正很小内容	
   2, 极少数情况, 
      比如 t = 49, 执行了 a = πθ(s) 以外的 a1 , 也得到很高的 vt (更近成功终点), πθ(s,a)等于0.3
      然后 logπθ(st,at) ≈ -0.5,即 -logπθ(st,at) ≈ 0.5 所以 -logπθ(s,a)vt >0,(例如 = 10)
      表示 这一步虽然使用了策略外的一步,但是得到很大的vt(更近成功终点),
      表示在这个 s 执行的 a = πθ(s) 的策略值得怀疑, 应该有所改变,应该更偏向 a1
4 实际情况 我们 没有使用 logπθ(s,a) 这个模块, 而是使用 all_act 和 self.tf_acts 的交叉熵来代替
  logπθ(s,a) 与 交叉熵 虽然算法不一样,但意义是一样的:
      当执行的行为是远离策略时, 值比较大, 
      当执行的行为等于策略导出的,值=0
  之所以这么替代,是因为 不知当如何构造 logπθ(s,a),相反,建立交叉熵比较简单
  all_act 是每个时刻t,策略导出 a = πθ(s) 组成的 数组
  self.tf_acts 是每个时刻t,真正执行的 a 组成的 数组
  计算 每个 t 得到 交叉熵,就等于比对那刻 执行的行为 是否等于 策略导出的,
  是则等于0,非则是一个比较大的值
  所以可完全取代了 logπθ(s,a)
5 然后这里学习 的策略参数θ 对应的 其实就是 策略网络里的 fc1 fc2 里所有的 W b
  而 这里的 self.tf_vt 是 每一刻t 执行的实际的 a 得到 奖励, 
  而不是 策略算出的行为a 对应的奖励
模型学习原理:(大多数梯度下降训练模型的原理,大抵是这种思维)
	完成了一个 episode A 后 我们得到了所有这个 episode 操作过的 a 和对应的奖励 vt
	代入 loss 得到loss数值 AA 
	把 这个 episode A 操作过的 a 和对应的奖励 vt 看成常数, 这时loss是一个 关于 w , b 的函数
	通过对 w,b 求梯度,我们更新了 w,b 
	更新的 w,b 只是让 当前 episode A 操作过的 a 和对应的奖励 vt 
	代入 loss后获得 更低的loss数值  AAA
	下一个 episode B操作过的 a 和对应的奖励 vt ,代入上次更新了w,b的loss  
	得到的 loss数值 BB, BB 不一定比上个 AAA 小. BB 和 AAA 没有什么表面上的直接联系
	根据 episode B 梯度执行第二次更新loss的 w,b 也是只让episode B 再第二次更新loss 得到更低的 BBB
	同样 下个 episode C 代入第二次更新后的loss 得到的 CC 与 BBB没有直接 关系
	CC 有可能 比 BBB 大 
	如此类推
	但是经过足够多的 episode 如此代入迭代后,比如经过100000次后,
	然后更后面的 episode 代入 loss 都会得到一个 比较相似,和 很低的 loss值
	这时我们就认为这个模型已经训练好.
	这是为什么呢? 收敛解释:
		因为假如每个episode 都是执行同样的行为结束,那么最终policy会收敛到这套动作上来,
		表现为 每次策略推导的行为就是实际执行的行为.所以每刻 t 的行为的交叉熵都为 0
		这种情况根本就可以无视奖励.因为不管奖励多少, loss最终都为 0
		但是我们设置有随机 情况 出现 策略推导的行为不是是实际执行的行为 的情况
		这时一些步得到的奖励也跟以前一直得到的不一样.
		如果 交叉熵和奖励都不同时为0,
		然后loss就不为0了, loss对w,b做梯度下降操作的意义在于:
		模型总是认为当前episode的所有实际行为,在以后的episode都会一直这么做.
		这也就是 loss 对 w,b做梯度下降时, 把所有的 s,a,vt 当作常数的行为
		做梯度下降就是 要把 这个实际行为变成是 策略可以推导出的行为 
		因为这样 所有行为的交叉熵会 变回0,loss会收敛会回到 0
		所以实际上 因为 实际行动常常与策略推导的不一样,所以策略一直在变
		直到一直到达一个死胡同, 虽然也执行了与策略推导不一样的行为,但是这时的奖励为0,
		所以 loss 还是 0, 所以loss并没有做对 w,b 的梯度下降操作,所以策略还是没有变
		这样子, 就相当于 这个模型的每次 episode 的整套动作大抵链被控制在一套方案里
		因为我们很小的随机机会作出与策略不一样的实际动作,但是这些意外的动作没有奖励
		所以那次 episode 下来 loss没变 ,策略也就没变
	但是,实际上loss 永远不为0, 交叉熵也永远不会出现0,而会接近0
		真实的行为是,最后收敛后的状态是
		在 s 选了非策略的行为a后得到很大的交叉熵, 得到一个很小的 vt  
		但是 给loss只带来了一点改变		
		loss 对 w,b 的梯度下降,把策略改偏了一点
		后面一轮 episode
		在 s 选的都是 策略导出行为为实际行为, 得到几乎接近0的交叉熵,得到比较大vt
		但是 给loss带来了 比之前非策略实际行为 还要大一点改变
		loss 对 w,b 的梯度下降,把策略偏回去了
		后面几轮 episode
		在 s 选的都是 策略导出行为为实际行为,
		因为我们只给了小几率执行非策略行为为实际行为,
		所以这时在 s 一般执行的都是策略导出行为
		这样对 loss就没有任何贡献了 ,loss也不会对 w,b做什么改变
		意味着策略并不会有什么改变
		这要反反复复,策略也没有什么实质大变化,这样模型实际上就是收敛了!
		这个模型的每次 episode 的整套动作大抵链被控制在一套方案里
		这套方案或许有几个分支可以明显分辨到的分支路线
		但其实他们都符合同一个策略,因为我们给出小几率执行非策略行为实际行为所导致
		一般分支点开始的地方都是 策略能导出多个行为的地方,
		策略之所以能导出多个行为,是因为这些行为得到的奖励大抵相同
		上一次的 episode 获得的 loss的细微改变,
		总会影响到当前 episode 在这个分支地方的策略导出
		影响策略究竟挑选哪个行为作为当前episode最优的分支行为
总结对比 归纳 两种 意义 完全不一样,但也是通过梯度收敛的的模型 以点线拟合行为例子
第一种, 点是按照固定规则 分布的. 点绝大多数分布都符合固定规则内
	比如说 点是按照某双曲线 随机分布的, 每次我们把若干散点点当常数代入拟合式子的loss式子
	然后loss式子对拟合参数作梯度下降操作,拟合式子会更像这个双曲线一些
	经过若干轮梯度下降操作,每次拟合式子都会更像这个双曲线一些
	很快现出了双曲线的影子
	这是我们见过的基本的 拟合散点曲线的例子
第二种, 点是按照动态规则 分布的,未到最后,还真不知道会拟合出上什么曲线出来
	比如说,最后得到的是双曲线的这个例子
	一开始,根据我们知道了一些隐晦的规则,也根据拟合式子的一些反馈,我们得到了一张随机的散点图,
	这张散点图 看着就像是拟合到一个椭圆似的.
	这张散点图分布并没有违反规则,
	因为不像第一种
	因为没有固定规则散点图分布得有双全线的样子,所以可以是其他样子
	然后散点图作常数代入拟合式子的loss式子 作梯度下降操作后,
	拟合式子会更像是 椭圆式子一些.
	然后根据继续 根据隐晦的规则,和拟合式子的反馈,我们得到了一张新的随机的散点图
	这张散点图 看着就像是一个三角形.	
	然后散点图作常数代入拟合式子的loss式子 作梯度下降操作后,
	拟合式子却改变方向,会更像 三角形式子一些
	若干次后, 后面出来的散点图 最后一般都会 像是双曲线的样子 而不是其他样子
	最后得到的 拟合式子也越来越像双曲线例子了
	那么是如何做到 散点图 从开始 椭圆形慢慢收敛到 最后的双曲线形?
	这就是神奇需要解释的地方!
		这可以理解为拟合式子和隐晦规则共同约束的结果
		比如说拟合式子此刻就是椭圆式子的样子.但是隐晦规则的存在,
		使得出现的散点图不局限维是 椭圆的样子,
		关键是,有机会是其他样子
		一旦是其他样子,拟合式子经梯度下降后就会变形
		当拟合式子变形成 双曲线式子,反而因为 隐晦规则的存在,
		使得出现的散点图仅仅局限在 双曲线.,不太可能是其他例子
		所以最终的散点图只会是双曲线的样子,而拟合式子也收敛到一个双曲线式子里	
而policy Gadient 一般就像是 第二种 收敛模型
机器学习, 有的符合第一种,有的符合第二种
强化学习,估计一般都符合第二种.


































 





??????????????????

约等于 ≈
　　1、几何符号
　　⊥   ∥   ∠   ⌒   ⊙   ≡   ≌    △
　　2、代数符号
　　∝   ∧   ∨   ～   ∫   ≠    ≤   ≥   ≈   ∞   ∶
　　3、运算符号
　　如加号（＋），减号（－），乘号（×或·），除号（÷或／），两个集合的并集（∪），交集（∩），根号（√），对数（log，lg，ln），比（：），微分（dx），积分（∫），曲线积分（∮）等。
　　4、集合符号
　　∪   ∩   ∈
　　5、特殊符号
　　∑    π（圆周率）
　　6、推理符号
　　|a|    ⊥    ∽    △    ∠    ∩    ∪    ≠    ≡    ±    ≥    ≤    ∈    ←
　　↑    →    ↓    ↖    ↗    ↘    ↙    ∥    ∧    ∨
　　&;   §
　　①   ②   ③   ④   ⑤   ⑥   ⑦   ⑧   ⑨   ⑩
　　Γ    Δ    Θ     Λ    Ξ    Ο    Π     Σ    Φ     Χ    Ψ    Ω
　　α    β    γ    δ    ε    ζ    η    θ    ι    κ    λ    μ     ν
　　ξ    ο    π    ρ    σ    τ    υ    φ    χ    ψ    ω
　　Ⅰ Ⅱ Ⅲ Ⅳ Ⅴ Ⅵ Ⅶ Ⅷ Ⅸ Ⅹ Ⅺ Ⅻ
　　ⅰ ⅱ ⅲ ⅳ ⅴ ⅵ ⅶ ⅷ ⅸ ⅹ
　　∈   ∏   ∑   ∕   √   ∝   ∞   ∟ ∠    ∣   ∥   ∧   ∨   ∩   ∪   ∫   ∮
　　∴   ∵   ∶   ∷   ∽   ≈   ≌   ≒   ≠   ≡   ≤   ≥   ≦   ≧    ≮   ≯   ⊕   ⊙    ⊥
　　⊿   ⌒     ℃
　　指数0123：o123
　　7、数量符号
　　如：i，2+i，a，x，自然对数底e，圆周率π。
　　8、关系符号
　　如“＝”是等号，“≈”是近似符号，“≠”是不等号，“＞”是大于符号，“＜”是小于符号，“≥”是大于或等于符号（也可写作“≮”），“≤”是小于或等于符号（也可写作“≯”），。“→ ”表示变量变化的趋势，“∽”是相似符号，“≌”是全等号，“∥”是平行符号，“⊥”是垂直符号，“∝”是成正比符号，（没有成反比符号，但可以用成正比符号配倒数当作成反比）“∈”是属于符号，“⊆ ⊂ ⊇ ⊃”是“包含”符号等。
　　9、结合符号
　　如小括号“（）”中括号“［］”，大括号“｛｝”横线“—”
　　10、性质符号
　　如正号“＋”，负号“－”，绝对值符号“| |”正负号“±”
　　11、省略符号
　　如三角形（△），直角三角形（Rt△），正弦（sin），余弦（cos），x的函数（f(x)），极限（lim），角（∠），
　　∵因为，（一个脚站着的，站不住）
　　∴所以，（两个脚站着的，能站住） 总和（∑），连乘（∏），从n个元素中每次取出r个元素所有不同的组合数（C(r)(n) ），幂（A，Ac，Aq，x^n）等。
　　12、排列组合符号
　　C-组合数
　　A-排列数
　　N-元素的总个数
　　R-参与选择的元素个数
　　!-阶乘 ，如5！=5×4×3×2×1=120
　　C-Combination- 组合
　　A-Arrangement-排列
　　13、离散数学符号
　　├ 断定符（公式在L中可证）
　　╞ 满足符（公式在E上有效，公式在E上可满足）
　　┐ 命题的“非”运算
　　∧ 命题的“合取”（“与”）运算
　　∨ 命题的“析取”（“或”，“可兼或”）运算
　　→ 命题的“条件”运算
　　A<=>B 命题A 与B 等价关系
　　A=>B 命题 A与 B的蕴涵关系
　　A* 公式A 的对偶公式
　　wff 合式公式
　　iff 当且仅当
　　↑ 命题的“与非” 运算（ “与非门” ）
　　↓ 命题的“或非”运算（ “或非门” ）
　　□ 模态词“必然”
　　◇ 模态词“可能”
　　φ 空集
　　∈ 属于（??不属于）
　　P（A） 集合A的幂集
　　|A| 集合A的点数
　　R^2=R○R [R^n=R^(n-1)○R] 关系R的“复合”
　　（或下面加 ≠） 真包含
　　∪ 集合的并运算
　　∩ 集合的交运算
　　- （～） 集合的差运算
　　〡 限制
　　[X](右下角R) 集合关于关系R的等价类
　　A/ R 集合A上关于R的商集
　　[a] 元素a 产生的循环群
　　I (i大写) 环，理想
　　Z/(n) 模n的同余类集合
　　r(R) 关系 R的自反闭包
　　s(R) 关系 的对称闭包
　　CP 命题演绎的定理（CP 规则）
　　EG 存在推广规则（存在量词引入规则）
　　ES 存在量词特指规则（存在量词消去规则）
　　UG 全称推广规则（全称量词引入规则）
　　US 全称特指规则（全称量词消去规则）
　　R 关系
　　r 相容关系
　　R○S 关系 与关系 的复合
　　domf 函数 的定义域（前域）
　　ranf 函数 的值域
　　f:X→Y f是X到Y的函数
　　GCD(x,y) x,y最大公约数
　　LCM(x,y) x,y最小公倍数
　　aH(Ha) H 关于a的左（右）陪集
　　Ker(f) 同态映射f的核（或称 f同态核）
　　[1，n] 1到n的整数集合
　　d(u,v) 点u与点v间的距离
　　d(v) 点v的度数
　　G=(V,E) 点集为V，边集为E的图
　　W(G) 图G的连通分支数
　　k(G) 图G的点连通度
　　△（G) 图G的最大点度
　　A(G) 图G的邻接矩阵
　　P(G) 图G的可达矩阵
　　M(G) 图G的关联矩阵
　　C 复数集
　　N 自然数集（包含0在内）
　　N* 正自然数集
　　P 素数集
　　Q 有理数集
　　R 实数集
　　Z 整数集
　　Set 集范畴
　　Top 拓扑空间范畴
　　Ab 交换群范畴
　　Grp 群范畴
　　Mon 单元半群范畴
　　Ring 有单位元的（结合）环范畴
　　Rng 环范畴
　　CRng 交换环范畴
　　R-mod 环R的左模范畴
　　mod-R 环R的右模范畴
　　Field 域范畴
　　Poset 偏序集范畴 


















北京大学 沈阳 性侵犯女学生
西安交大 导师周筠施压杨宝德至其跳楼
上海西南模范中学包庇 市三好学生李明泰猥琐女生
武汉理工大学包庇 王攀教授奴役陶崇明逼跳楼,大学威逼其胞姐道歉




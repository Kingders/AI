20180310:
更换高速的ubuntu16源:
	sudo gedit /etc/apt/sources.list
	注释其他源，添加：
		# 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释
		deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial main restricted universe multiverse
		# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial main restricted universe multiverse
		deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse
		# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse
		deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse
		# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse
		deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-security main restricted universe multiverse
		# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-security main restricted universe multiverse		
		# 预发布软件源，不建议启用
		# deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse
		# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse
	sudo apt-get update
	换源要谨慎,机子中途换源会容易与以前的软件冲突!!所以要么一开始就换源,要么不换

更换高速的pip3源：
	1、在用户目录下（/home/XXX）创建.pip文件夹，并创建pip.conf文件
	2、在pip.conf下输入：（注意：这里更换的是阿里云镜像源）
		[global]
		trusted-host = mirrors.aliyun.com
		index-url = http://mirrors.aliyun.com/pypi/simple
	3、 sudo apt-get update

python 虚拟环境：
	pip3 install virtualenv
	sudo pip3 install virtualenvwrapper	//虚拟环境管理模块
	mkdir $HOME/.local/virtualenvs	  //创建虚拟环境管理目录 (不要加sudo)
	sudo gedit ~/.bashrc 	//末尾添加:
		# by william
		# setting about virtualenvwrapper
		export VIRTUALENVWRAPPER_PYTHON=/usr/bin/python3
		export VIRTUALENV_USE_DISTRIBUTE=1        #  总是使用 pip/distribute                                        
		export WORKON_HOME=$HOME/.local/virtualenvs       # 所有虚拟环境存储的目录
		if [ -e $HOME/.local/bin/virtualenvwrapper.sh ];then
		   source $HOME/.local/bin/virtualenvwrapper.sh                                                
		else if [ -e /usr/local/bin/virtualenvwrapper.sh ];then
		         source /usr/local/bin/virtualenvwrapper.sh
		     fi
		fi
		export PIP_VIRTUALENV_BASE=$WORKON_HOME
		export PIP_RESPECT_VIRTUALENV=true
	source ~/.bashrc	//启动 virtualenvwrapper

	简单创建虚拟环境:
		virtualenv aaa  	//创建一个独立环境空间aaa,在当前文件夹建立一个aaa文件夹,
					//这种默认情况下,会把默认的解释机,和对应的默认软件库加入环境aaa
		virtualenv --no-site-packages bbb //创建一个独立环境空间aaa,在当前文件夹建立一个aaa文件夹,
							  //这情况下,不会把默认的软件库加入环境bbb,
		virtualenv ccc --python=python2   //创建一个独立环境空间ccc,在当前文件夹建立一个ccc文件夹,
						  //这种默认情况下,会把默认的软件库,和默认的解释机加入环境ccc
		启用虚拟环境
		cd ccc	//进入环境文件夹
		source ./bin/activate
		cd ~ //进入要执行的项目的文件夹,例如~
		查看当前状态
		(ccc) kingders@kingders-ThinkPad-T420:~$ 	//先可以直观看到(ccc)前缀,就是说现在处于 ccc 的独立python 工作环境里下
		退出虚拟环境
		deactivate
	通过管理套件创建虚拟环境:
		mkvirtualenv aaa -p python3	//创建
		workon aaa		//进入
		workon			//查看
		deactivate		//退出
		




!@!



20180319
IndentationError: expected an indented block
	这个问题要注意缩进！！






20xxx
python 基础:
np.newaxis 使用: (import numpy as np)
	a=np.array([1,2,3,4,5])
	print a.shape
	print a
	输出结果
		(5,)
		[1 2 3 4 5]

	a=np.array([1,2,3,4,5])
	b=a[np.newaxis,:]
	c=a[:np.newaxis]
	print (a.shape,b.shape,c.shape)
	print (b.shape[1])
	print (a)
	print (b)
	print (c)
	输出结果:
		(5,) (1, 5) (5, 1)
		5
		[1 2 3 4 5]
		[[1 2 3 4 5]]
		[[1]
		 [2]
		 [3]
		 [4]
		 [5]]
range()
	一个特殊函数.
	range(start, stop[, step])
	    start: 计数从 start 开始。默认是从 0 开始。例如range（5）等价于range（0， 5）;
	    stop: 计数到 stop 结束，但不包括 stop。例如：range（0， 5） 是[0, 1, 2, 3, 4]没有5
	    step：步长，默认为1。例如：range（0， 5） 等价于 range(0, 5, 1)
	>>>range(10)        # 从 0 开始到 10
	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
	>>> range(1, 11)     # 从 1 开始到 11
	[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
	>>> range(0, 30, 5)  # 步长为 5
	[0, 5, 10, 15, 20, 25]
	>>> range(0, 10, 3)  # 步长为 3
	[0, 3, 6, 9]
	>>> range(0, -10, -1) # 负数
	[0, -1, -2, -3, -4, -5, -6, -7, -8, -9]
	>>> range(0)
	[]
	>>> range(1, 0)
	[]
	以下是 range 在 for 中的使用，循环出runoob 的每个字母:
	>>>x = 'runoob'
	>>> for i in range(len(x)) :
	...     print(x[i])
	... 
	r
	u
	n
	o
	o
	b
	>>>
	Python3.x 中 range() 函数返回的结果是一个整数序列的对象，而不是列表。
	>>> type(range(10))
	<class 'range'>
	当你 help(range) 时会看到：
	Return an object...
	所以，不是列表，但是可以利用 list 函数返回列表，即：
	>>> list(range(10))
	[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
	不是列表的证据
	a = range(9)
	print (a)
	输出结果
		range(0, 9)
np.random.choice
	import numpy as np
	# 参数意思分别 是从a 中以概率P，随机选择3个, p没有指定的时候相当于是一致的分布
	# a可以是一个数表示从0到a-1之间选,也可以是一个一维向量,从向量中选第几个成员
	a1 = np.random.choice(a=5, size=3, replace=False, p=None)
	print(a1)
	# 非一致的分布，会以多少的概率提出来
	a2 = np.random.choice(a=5, size=3, replace=False, p=[0.2, 0.1, 0.3, 0.4, 0.0])
	print(a2)
	# replacement 代表的意思是抽样之后还放不放回去，如果是False的话，那么出来的三个数都不一样，如果是	
	True的话， 有可能会出现重复的，因为前面的抽的放回去了。	
append()
	给数组添加一个新成员!
np.zeros_like
	W_update=np.zeros_like(W);
	构造一个矩阵W_update，其维度与矩阵W一致，并为其初始化为全0


20180401
Tensorflow 常用:
batch 机制 的功能作用的描述 !!
	整理下认识,一般我们求出实际y与训练结果Y'的误差来反向传导更新模型
	而误差,我们可以取,方差,标准差,交叉熵什么的,
	注意,这些是 一个样本下的误差,我们说的方差交叉熵什么的,是基于同一个样本得到的,
	例如,图片a实际标签向量y:[y1,y2,..y10],训练结果标签向量Y':[y'1,y'2,..y'10].
	图片a的方差,是:	{ (y1-y'1)^2 + ..+ (y10 - y'10)^2 } /10
	那么cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction),reduction_indices=[1]))如何理解,
	我们知道,在一次训练里,输入了100张图片,即100张图片通过同样的 weights,blase,得到各自的训练结果标签向量Y'
	tf.reduce_mean括号里算出的是 每张图片自己的交叉熵,
	交叉熵指,有10个特征值的标签向量,把每个特征值的混乱程度加到一块得到的一个值
	然后通过 tf.reduce_mean,把 100 个交叉熵 算出一个 平均值,
	我们最后才通过这个 交叉熵平均值 来反向传导更新一次学习模型的参数weights和biase
	即我们是通过100张图片得到100个交叉熵,然后取100个交叉熵的平均值
	并不是通过100张图片得到1个交叉熵,而且也不知道如何得到
	也不是100个交叉熵,每个交叉熵都反向传导更新模型一次,即不是在一次训练里更新100次模型!!!
	我们只是取 100个交叉熵的平均值来更新 1 次模型
	所以.模型中的 batch 设置为100,那个100,就是这种意思
参数:
tf.Variable.init(initial_value, trainable=True, collections=None, validate_shape=True, name=None)
	initial_value 	所有可以转换为Tensor的类型 	变量的初始值
	trainable 	bool 	如果为True，会把它加入到GraphKeys.TRAINABLE_VARIABLES，才能对它使用Optimizer
	collections 	list 	指定该图变量的类型、默认为[GraphKeys.GLOBAL_VARIABLES]
	validate_shape 	bool 	如果为False，则不进行类型和维度检查
	name 		string 	变量的名称，如果没有指定则系统会自动分配一个唯一的值
从正态分布中输出随机值:	
tf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None) 
	shape: 一维的张量，也是输出的张量。
	mean: 正态分布的均值。
	stddev: 正态分布的标准差。
	dtype: 输出的类型。
	seed: 一个整数，当设置之后，每次生成的随机数都一样。
	name: 操作的名字。
	例子:
	|2,6,7| = tf.random_normal([2.3])
	|9,1,4|
占位符号:
tf.placeholder(dtype, shape=None, name=None)
	此函数可以理解为形参，用于定义过程，在执行的时候再赋具体的值
	dtype：数据类型。常用的是tf.float32,tf.float64等数值类型
	shape：数据形状。默认是None，行不定，比如[2,3]表示列是3，行是2, [None, 3]表示列是3，行不定
	name：名称。
二维数组(二维矩阵)的叠加函数	
tf.reduce_sum()
	例子1:
	[2,2,2] = tf.reduce_sum(|1,1,1|, reduction_indices=[0] )
				|1,1,1|
	|3| = tf.reduce_sum(|1,1,1|, reduction_indices=[1] )
	|3|		    |1,1,1|	
	例子2:
	6 = tf.reduce_sum(|1,1,1|, reduction_indices=[0,1] )
			  |1,1,1|	
	就是先reduction_indices=[0]得到[2,2,2],再reduction_indices=[1] 得到 6 

二维数组的乘函数
tf.matmul()
	a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3]) => [[1. 2. 3.]
	                                                      [4. 5. 6.]]

	b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2]) => [[7. 8.]
	                                                         [9. 10.]
	                                                         [11. 12.]]
	c = tf.matmul(a, b) => [[58 64]
        	                [139 154]]
二维数组的 2次方
tf.square()
	|1,  4, 9| = tf.reduce_sum(|1,2,3|)
	|16,25,36|		   |4,5,6|
二维数组的 平均值:
tf.reduce_mean()
	  2.5 = tf.reduce_mean(|1,2|)
			       |3,4|
	|2,3| = tf.reduce_mean(|1,2|, 0)
			       |3,4|
	|1.5| = tf.reduce_mean(|1,2|, 1)
	|3.5|		       |3,4|

二维数组的 最大值位置:
tf.argmax(|1, 2, 3|,0)=[3,3,1]	//数组从选出 每列中最大值的位置 (从0数)
          |2, 3, 4|
          |5, 4, 3|
          |8, 7, 2|
tf.argmax(|1, 2, 3|,1)=[2, 2, 0, 0] //数组从选出 每行中最大值的位置 (从0数)
          |2, 3, 4|
          |5, 4, 3|
          |8, 7, 2|
tf.cast 类型转换 函数:
	tf.cast([2, 3, 4], tf.float32) //把一维数组的每个int值转换为float值

tf.reduce_sum
	# 'x' is [[1, 1, 1]
	#         [1, 1, 1]]
	tf.reduce_sum(x) ==> 6
	tf.reduce_sum(x, 0) ==> [2, 2, 2]
	tf.reduce_sum(x, 1) ==> [3, 3]
	tf.reduce_sum(x, 1, keep_dims=True) ==> [[3], [3]]
	tf.reduce_sum(x, [0, 1]) ==> 6

按正太分布随机生成 多维数组:
tf.truncated_normal(shape, mean, stddev)
	shape表示生成张量的维度，mean是均值，stddev是标准差。这个函数产生正太分布，均值和标准差自己设定。
	例子: 
	|1.95758033,-0.68666345,-1.83860338, 0.78213859|= tf.truncated_normal(shape=[2,4], mean=0, stddev=1) 
        |0.38035342, 0.57904619,-0.57145643,-1.22899497|
生成tensor：
	tf.zeros(shape, dtype=tf.float32, name=None)	//零矩阵
	tf.zeros_like(tensor, dtype=None, name=None)
	tf.constant(value, dtype=None, shape=None, name='Const') //值都为value的矩阵
	tf.fill(dims, value, name=None)
	tf.ones_like(tensor, dtype=None, name=None)
	tf.ones(shape, dtype=tf.float32, name=None)
生成序列
	tf.range(start, limit, delta=1, name='range')
	tf.linspace(start, stop, num, name=None)
生成随机数
	tf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)
	tf.truncated_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)
	tf.random_uniform(shape, minval=0.0, maxval=1.0, dtype=tf.float32, seed=None, name=None)
	tf.random_shuffle(value, seed=None, name=None)
卷积操作
tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, name=None)
	第一个参数input：指需要做卷积的输入图像，它要求是一个Tensor，
		具有[batch, in_height, in_width, in_channels]这样的shape，
		具体含义是[训练时一个batch的图片数量, 图片高度, 图片宽度, 图像通道数]，
		即这个参数是一个多维数组!!
		注意这是一个4维的Tensor，要求类型为float32和float64其中之一
		然而实际操作是 [batah,in_height*in_width*in_channels]二维数组(二维tensor)
	第二个参数filter：相当于CNN中的卷积核，它要求是一个Tensor，
		具有[filter_height, filter_width, in_channels, out_channels]这样的shape，
		即这个参数是一个多维数组!!
		具体含义是[卷积核的高度，卷积核的宽度，图像通道数，输出图像通道数]，
		要求类型与参数input相同，有一个地方需要注意，第三维in_channels，就是参数input的第四维
		注意,这个参数的含义
		譬如 input是一张28*28有32个通道的图片,即有32张特征图片, 
			有一个 shape=[2,2,32,64]的filter多维向量 ,即说明
			shape=[2,2,32,64]说明 输入的这张图片是有32个通道的,即有32张特征图片的,
			shape=[2,2,32,64]也要求卷积输出后的图片只有有64个通道的,即有64张特征图片的
			shape=[2,2,32,64]也说明,filter里共有 32x64 个独立 2x2 的卷积核
			所以按道理卷积出来的 通道应该有 32x64 个,而不是64 个
			所以这里卷积的过程与我们理论学习的过程有些详细的区别!!
			理论上,输出64通道的话,32个输入通道,每个分配两个卷积核就可以了!
			而这里是每个分配 64 个卷积核,一个通道就能卷积出64这个特征图片了,
			但是接着把每个通道卷积出的64个特征图片,求平均得出2个平均特征图片
			每个通道得出2个平均特征图片,32个就得出64个,
			这64个平均特征图片就凑成最后要输出的64个通道
	第三个参数strides：卷积时在图像每一维的步长，这是一个一维的向量，长度4
		由于对于图片,只有长宽,所以,只用到中间两个值表示步数,其他为1,
		如 strides=[1, 4, 4, 1],表示长宽步长都为4,
		即不在batch和channels上做卷积
	第四个参数padding：string类型的量，只能是"SAME","VALID"其中之一
		padding可'SAME' 可`VALID, SAME表示卷积所有像素,VALID则不是
		VALID:
		 1 2 3 4 5 6 7 8 9 10 11 12 13	多出的12,13不卷积
		|___________|
			  |_____________|
		SAME:
		0 |1 2 3 4 5 6 7 8 9 10 11 12 13| 0 0	卷积所有像素,不够的补0
		|___________|                   |
			  |____________|        |
	                            |________________|	
	第五个参数：use_cudnn_on_gpu:bool类型，是否使用cudnn加速，默认为true
	做后一个是当前卷积操作的名字
		结果返回一个Tensor，这个输出，
		就是我们常说的feature map，shape仍然是[batch, height, width, channels]这种形式。
		即是下一层的input,下一层的卷积的输入图像
池化操作:
tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')	
	建立一个池化操作
	池化输出的是邻近区域的概括统计量，一般是矩形区域。池化有最大池化、平均池化、滑动平均池化、L2范数池化等
	池化与卷积意义上不一样,虽然操作上有很多地方相同
	第一个参数value：需要池化的输入，一般池化层接在卷积层后面，
		所以输入通常是feature map，依然是[batch, height, width, channels]这样的shape
	第二个参数ksize：池化窗口的大小，取一个四维向量，一般是[1, height, width, 1]，
		因为我们不想在batch和channels上做池化，所以这两个维度设为了1
	第三个参数strides：和卷积类似，窗口在每一个维度上滑动的步长，一般也是[1, stride,stride, 1]
	第四个参数padding：和卷积类似，可以取'VALID' 或者'SAME'	

tf.transpose(outputs, [1,0,2]:表示把 batch steps 两维转换
	即outputs 变成了 [steps,batch,输出向量维数]
	图例直观解析
		假如 outputs本来是这样的 shape:[3,2,5]:
		[

			[
				[1,1,1,1,1,1]
				[2,2,2,2,2,2]		
			]
			,

			[
				[3,3,3,3,3,3]
				[4,4,4,4,4,4]		
			]
			,

			[
				[5,5,5,5,5,5]
				[6,6,6,6,6,6]		
			]
			,
		]
		transpose(outputs, [1,0,2] -> outputs shape:[2,3,5]:
		只换第 batch steps 维, 
		输出向量维数 这维内容不变, 这维可以简单标记:
			A = [1,1,1,1,1,1]
			B = [3,3,3,3,3,3]
			C = [5,5,5,5,5,5]
			D = [2,2,2,2,2,2]		
			E = [4,4,4,4,4,4]		
			F = [6,6,6,6,6,6]
    		outputs 简单记为:
		[[A,B]
		 [C,D]
		 [E,F]]
		换第 batch steps 后:
		[[A,C,E]
		 [B,D,F]]
		把标记 ABCDEF换换回去就得:
		[

			[
				[1,1,1,1,1,1]
				[3,3,3,3,3,3]
				[5,5,5,5,5,5]

			]
			,

			[
				[2,2,2,2,2,2]		
				[4,4,4,4,4,4]		
				[6,6,6,6,6,6]		
			]
			,
		]		

解构数组:tf.unstack()
	没有其他参数,默认解构最前的一维,即 steps维,即第0维
	解构图示:
		假如 outputs是这样子的:
		outputs = [
	
				[
					[1,1,1,1,1,1]
					[2,2,2,2,2,2]		
				]
				,

					[
					[3,3,3,3,3,3]
					[4,4,4,4,4,4]		
				]
				,

				[
					[5,5,5,5,5,5]
					[6,6,6,6,6,6]		
				]

			]
		tf.unstack(outputs) 后,得到
		outputs[0] = [ [1,1,1,1,1,1]
			       [2,2,2,2,2,2] ]
		outputs[1] = [ [3,3,3,3,3,3]
			       [4,4,4,4,4,4] ]
		outputs[2] = [ [5,5,5,5,5,5]
			       [6,6,6,6,6,6] ]
		output 从 变量 变成了 数组变量 !!


更多 tensorflow 常用简介 : https://blog.csdn.net/lenbow/article/details/52218551









>>>>>>>>>>>>>>>>

20180401
回归:
/home/william/AI/machine learning/回归/code.py
通过散点数据训练一个模型,找到散点的落入规律,
这里找到的规律是,散点落入一个二次函数范畴的规律,也就说通过模型得到的散点建立的曲线越来越像二次曲线 
先人为制作一个 二次函数曲线 的散点图,作为样本参数 这里是建立 300 个散点
	x_data = np.linspace(-1, 1, 300, dtype=np.float32)[:, np.newaxis]
	noise = np.random.normal(0, 0.05, x_data.shape).astype(np.float32)
	y_data = np.square(x_data) - 0.5 + noise  
再通过 matplotlib.pyplot 显示散点图!! 
	plt.scatter(x_data, y_data)	
	plt.show()	//这里训练和显示散点图是冲突的,要训练,就要屏蔽显示散点图
训练模型建立:
//定义如何建立层
def add_layer(inputs, in_size, out_size, activation_function=None): //定义如何建立层
	重点如何地定义变量定义:例如:
	Weights = tf.Variable(tf.random_normal([in_size, out_size]))
	tf.random_normal 得随机变量值,这里出来是随机二维数组 in_size*out_size,其他参数默认,
	tf.Variable 把这个随机二位数组值变成 tensorflow变量
//定义训练模型的输入输出变量占位符
	xs = tf.placeholder(tf.float32, [None, 1])
	ys = tf.placeholder(tf.float32, [None, 1])
	//输入的xs.输出的ys是一维数组,而[None, 1]表示列是1，行不定的一维维数组,
	//注意是一维数组,不是一维向量,是有多个一维向量组成的一维数组
	//之所一维数组,是因为在这个例子里,样本是一个个的点坐标,而每一次训练是一次性输入XX个样本,统一计算
	//这堆样本的x分量会放入xs里,变成一个有XX个一维向量的一维数组
	//这堆样本的y分量会放入ys里,变成一个有XX个一维向量的一维数组	
//构建多层模型
	这里只有两层: l1隐藏层,和 prediction预测输出层
	l1 = add_layer(xs, 1, 10, activation_function=tf.nn.relu)
	prediction = add_layer(l1, 10, 1, activation_function=None)
	//li层,会对结果执行relu激活算法,使第一层的输出有10个变量的一维数组的变量值在0-1附近
	//使用激活函数,可以优化避免梯度消失和梯度爆炸的情况发生
//设置训练方式
	loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys-prediction), reduction_indices=[1]))
	//例子说明:例如一次训练直接输入5个样本点 [a,A][b,B][c,C][d,D][e,E]
	//那么 xs=|a| ,  ys=|A|  通过 xs 得到的 prediction=|Y|
	//	  |b|       |B|				  |H|
	//	  |c|       |C|				  |Z|
	//	  |d|       |D|				  |T|
	//	  |e|       |E|     			  |V|
	//那么 tf.square就得到 |(A-Y)^2|
	//		      |(B-H)^2|
	//		      |(C-Z)^2|
	//		      |(D-T)^2|
	//		      |(E-V)^2|
	//然后 tf.reduce_sum(..,reduction_indices=[1]) 得到:
	//	|(A-Y)^2|
	//	|(B-H)^2|
	//	|(C-Z)^2|
	//	|(D-T)^2|
	//	|(E-V)^2|
	//	因为是每行只有一个量,所 tf.reduce_sum 后并没有变化
	//然后 tf.reduce_mean()得到平均值:
	//	( |(A-Y)^2| + |(B-H)^2| + |(C-Z)^2| + |(D-T)^2| + |(E-V)^2| ) / 5 	
	train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)
	//使用 普通的梯度下降的优化方法(GradientDescent),来训练优化 loss, 学习率是 0.1
	//最终这次训练会更新所有的 weights 和 biases
//初始化tf训练环境
	sess = tf.Session(),	
//初始化tensorflow的所有变量
	init = tf.global_variables_initializer()
	sess.run(init)
//开始训练
	for i in range(1000):
    		sess.run(train_step, feed_dict={xs: x_data, ys: y_data})
	//执行1000次训练,每次训练都 读入 300 个散点,即 xs数组有300行, ys数组有300行
	//而这里我们就只有300个样本,所以每次训练都读入同一组数据
    		if i % 50 == 0:
        	print(sess.run(loss, feed_dict={xs: x_data, ys: y_data}))
	//每50次训练后,打印一次 loss

/home/william/AI/machine learning/回归/code.py
这个例子补充主要是图像监测部分!!
使用:import matplotlib.pyplot as plt
	fig = plt.figure()		//创建一个独立的视图窗口
	ax = fig.add_subplot(1,1,1)	//在窗口添加一个子视图ax
	ax.scatter(x_data, y_data)	//子视图的 x,y 轴对应 x_data, y_data
	plt.ion()			//使用交互形式,
	plt.show()			//一直显示图,(如果不开启交互模式,默认是阻塞模式,)
					//交互模式下一直显示图,图会一直显示,而程序也会继续plt.show()后的内容
					//阻塞模式下一直显示图,会一直卡在plt.show(),
	lines = ax.plot(x_data, prediction_value, 'r-', lw=5)
					//据 x_data, prediction_value 的一堆散点画出一条线
					//x值,y值,红色,宽度5
	plt.pause(1)		//暂停一秒
	ax.lines.remove(lines[0])//把刚刚画的线去掉,(这样就画下一条线,就不会挡住什么的)







20180401
学习使用tensorboard监视模型:/home/william/AI/machine learning/tensorboard
code3.py
重点是,在使用 tf.xxxx之前,先添加 with tf.name_scope('XXXX'):
例如:
with tf.name_scope('layer'):
    with tf.name_scope('weights'):
        Weights = tf.Variable(tf.random_normal([in_size, out_size]), name='W')
with tf.name_scope('inputs'):
    xs = tf.placeholder(tf.float32, [None, 1], name='x_input')
with tf.name_scope('loss'):
    loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction), reduction_indices=[1]))
with tf.name_scope('train'):
    train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)
然后,sess = tf.Session()后,init = tf.global_variables_initializer()前
    writer = tf.summary.FileWriter("logs/", sess.graph)
这样执行代码时,会把模型图加载到logs/里,
终端运行 tensorboard --logdir=logs后,就可以在浏览器http://127.0.0.1:6006/查看到模型内容

code4.py
重点添加训练参数的跟踪记录表
例如:
    Weights = tf.Variable(tf.random_normal([in_size, out_size]), name='W')
    tf.summary.histogram(layer_name + '/weights', Weights) 
    //在tensorboard的histogram和distribution栏添加Weights 的训练跟踪记录表
    loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction),reduction_indices=[1]))
    tf.summary.scalar('loss', loss)
    //在tensorboard的scalar栏添加loss 的训练跟踪记录表
然后,sess = tf.Session()后,init = tf.global_variables_initializer()前
    merged = tf.summary.merge_all()
    writer = tf.summary.FileWriter("logs2/", sess.graph)
这样执行代码时,会把模型图,还有训练跟踪表设置加载到logs2/里,
然后每隔n次训练后,给所有训练跟踪表添加新数据
    result = sess.run(merged, feed_dict={xs: x_data, ys: y_data})
    writer.add_summary(result, i)
最后终端运行 tensorboard --logdir=logs后,就可以在浏览器http://127.0.0.1:6006/查看到模型内容









20180401
分类:/home/william/AI/machine learning/分类
code5.py
注意,这里使用的交叉熵跟我之前分析的交叉熵有所区别
我之前分析的是 基于一个图像样品,得到的交叉熵再反向传导学习,
而这里却是 100个 样本的交叉熵,这里的交叉熵有点像平均值的意思,然后再反向传导学习
http://www.360doc.com/content/17/0118/20/10408243_623338635.shtml
经过慎重分析发现:
	注意不是 100个 样本的交叉熵,
	而是每个样本一个交叉熵,共100个,然后把他们都加起来除以100
	得到一个交叉熵的平均值,使用这个平均值反向传导训练模型
模型训练的思路分析:
每张图片有728个像素点: [x(1)1,x(1)2,x(1)3,...x(1)728],
	xs符合[None, 784],none=100行,即xs包含100张图片,即:
	xs = [x(1)1,x(1)2,x(1)3,x(1)4,...x(1)728],
	     [x(2)1,x(2)2,x(2)3,x(2)4,...x(2)728]
	     .......
	     [x(100)1,x(100)2,x(100)3,x(728)4,...x(100)728]
Weights符合[in_size, out_size],in_size=728,out_size=10,即
	Weights = [w(1)1,w(2)1,..w(10)1],
	          [w(1)2,w(2)2,..w(10)2]
		  [w(1)3,w(2)3,..w(10)3]
		  [w(1)4,w(2)4,..w(10)4]
		  .......
		  [w(1)728,w(2)728,..w(10)728]
bias符合[1,out_size],out_size=10,即
	bias = [b1,b2,b3,b4,..b10]
图片对应的标签向量: [y(1)1,y(1)2,..y(1)10]
	ys符合[None, 10],none=100行,即 ys 对应100张图片的 100 个标签向量:
	ys = [y(1)1,y(1)2,..y(1)10]
	     [y(2)1,y(2)2,..y(2)10]
	     [y(3)1,y(3)2,..y(3)10]
	     [y(4)1,y(4)2,..y(4)10]
	     ........
	     [y(100)1,y(100)2,..y(100)10]
那么,Wx_plus_b=y:
[x(1)1,x(1)2,x(1)3,x(1)4,...x(1)728]          * [w(1)1,w(2)1,..w(10)1]      + [b1,b2,b3,b4,..b10] = [y(1)1,y(1)2,..y(1)10]
[x(2)1,x(2)2,x(2)3,x(2)4,...x(2)728]            [w(1)2,w(2)2,..w(10)2]                              [y(2)1,y(2)2,..y(2)10]
 .......                                        [w(1)3,w(2)3,..w(10)3]                              [y(3)1,y(3)2,..y(3)10]
[x(100)1,x(100)2,x(100)3,x(728)4,...x(100)728]  [w(1)4,w(2)4,..w(10)4]                              [y(4)1,y(4)2,..y(4)10]
                                                .......						    ........
	                                        [w(1)728,w(2)728,..w(10)728]                        [y(100)1,y(100)2,..y(100)10]

batch 机制的功能作用的描述 !!
	整理下认识,一般我们求出实际y与训练结果Y'的误差来反向传导更新模型
	而误差,我们可以取,方差,标准差,交叉熵什么的,
	注意,这些是 一个样本下的误差,我们说的方差交叉熵什么的,是基于同一个样本得到的,
	例如,图片a实际标签向量y:[y1,y2,..y10],训练结果标签向量Y':[y'1,y'2,..y'10].
	图片a的方差,是:	{ (y1-y'1)^2 + ..+ (y10 - y'10)^2 } /10
	那么cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction),reduction_indices=[1]))如何理解,
	我们知道,在一次训练里,输入了100张图片,即100张图片通过同样的 weights,blase,得到各自的训练结果标签向量Y'
	tf.reduce_mean括号里算出的是 每张图片自己的交叉熵,
	交叉熵指,有10个特征值的标签向量,把每个特征值的混乱程度加到一块得到的一个值
	然后通过 tf.reduce_mean,把 100 个交叉熵 算出一个 平均值,
	我们最后才通过这个 交叉熵平均值 来反向传导更新一次学习模型的参数weights和biase
	即我们是通过100张图片得到100个交叉熵,然后取100个交叉熵的平均值
	并不是通过100张图片得到1个交叉熵,而且也不知道如何得到
	也不是100个交叉熵,每个交叉熵都反向传导更新模型一次,即不是在一次训练里更新100次模型!!!
	我们只是取 100个交叉熵的平均值来更新 1 次模型
	所以.模型中的 batch 设置为100,那个100,就是这种意思
其他重点内容:
	mnist = input_data.read_data_sets('MNIST_data', one_hot=True) //这里是导入官方训练样品库的方法
	batch_xs, batch_ys = mnist.train.next_batch(100)//从训练集,取出100个28*28图片样本和对应标签向量
	mnist.test.images, mnist.test.labels//检验集的图片样品,和对应标签向量
	
code6.py
重点是使用了sklearn 生成的样本,可以辅助我们学习使用tensorflow做很多模拟事情
	from sklearn.datasets import load_digits
	from sklearn.model_selection import train_test_split
	from sklearn.preprocessing import LabelBinarizer
	# load data
	digits = load_digits()
	X = digits.data
	y = digits.target
	y = LabelBinarizer().fit_transform(y)
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3)
还有的是,清晰告诉我们如何使用 dropout
	def add_layer(inputs, in_size, out_size, layer_name, activation_function=None, ):
	    # add one more layer and return the output of this layer
	    Weights = tf.Variable(tf.random_normal([in_size, out_size]))
	    biases = tf.Variable(tf.zeros([1, out_size]) + 0.1, )
	    Wx_plus_b = tf.matmul(inputs, Weights) + biases
	    # here to dropout
	    Wx_plus_b = tf.nn.dropout(Wx_plus_b, keep_prob)
	    if activation_function is None:
	        outputs = Wx_plus_b
	    else:
	        outputs = activation_function(Wx_plus_b, )
	    tf.summary.histogram(layer_name + '/outputs', outputs)
	    return outputs
	之前学习到:dropout不算是一个正规正矩的优化器，他的工作是，每次网络工作时，
	都随机抛弃一部分的神经元的作用，从而避免过度拟合
	从这个层建设定义中dropout的位置可看出,dropout不属于激活函数,
	同时也不能算作是一个优化器











20180403
CNN: code7.py
准确率计算:
def compute_accuracy(v_xs, v_ys):
    global prediction
    y_pre = sess.run(prediction, feed_dict={xs: v_xs, keep_prob: 1})
    correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(v_ys,1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
    result = sess.run(accuracy, feed_dict={xs: v_xs, ys: v_ys, keep_prob: 1})
    return result
	我们知道 图片对应的标签向量我们成为 one-hot 向量,即这样表示:
		数字图片0:[1,0,0,0,0,0,0,0,0,0]
		数字图片1:[0,1,0,0,0,0,0,0,0,0]
		...
		数字图片9:[0,0,0,0,0,0,0,0,0,1]
	而通过学习模型学习到的标签向量,往往不是整数的,例如:
		数字图片1:[0.01, 0.98, 0, 0.001, 0.1, 0.1, 0, 0, 0.02, 0.4]
		而标签向量 与 学习得到的标签向量, 位置1(从0数)的值都是最大的.
		这样就认为 模型准确学习识别出数字1的图片
	重点看: correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(v_ys,1))
		先tf.argmax算出 标签向量 与 学习得到的标签向量,的最大值位置
		比较这两个位置是否一样.
		注意 y_pre, v_ys 在这里是二维数值,即包含多个标签向量,
		tf.argmax后是一个表示位置意义的一维数组
		tf.equal后是一个表示正确与否意义的一维数组
	        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
		先把 correct_prediction 向量的每个值都转为 float类型,然后把加起来求平均
		这里,如果模型相当好,correct_prediction 向量的每个值基本为1,
		最后的平均值就接近1了
模型参数设置!!
权重weight
def weight_variable(shape):
    initial = tf.truncated_normal(shape, stddev=0.1)
    return tf.Variable(initial)
	按正太分布随机生成 张量的维度为shanpe 的权重数组 initial,
	然后计入 tf.Variable()
def bias_variable(shape):
    initial = tf.constant(0.1, shape=shape)
    return tf.Variable(initial)
	生成值为 0.1 ,张量的维度为shanpe 的 常量数组 inital
	然后计入 tf.Variable()
def conv2d(x, W):
    # stride [1, x_movement, y_movement, 1]
    # Must have strides[0] = strides[3] = 1
    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')
	建立一个卷积操作,注意strides=[1, 1, 1, 1]描述的是卷积核移动步长
	由于对于图片,只有长宽,所以,只用到中间两个值表示步数,其他为1,
	如 strides=[1, 4, 4, 1],表示长宽步长都为4
	padding可'SAME' 可`VALID, SAME表示卷积所有像素,VALID则不是
	VALID:
	 1 2 3 4 5 6 7 8 9 10 11 12 13	多出的12,13不卷积
	|___________|
		  |_____________|
	SAME:
	0 |1 2 3 4 5 6 7 8 9 10 11 12 13| 0 0	卷积所有像素,不够的补0
	|___________|                   |
		  |____________|        |
                            |________________|		

def max_pool_2x2(x):
    # stride [1, x_movement, y_movement, 1]
    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')
	建立一个池化操作
	池化输出的是邻近区域的概括统计量，一般是矩形区域。池化有最大池化、平均池化、滑动平均池化、L2范数池化等
	池化与卷积意义上不一样,虽然操作上有很多地方相同
	max_pool 是指最大池化的意思
最后补充内容:
	batch_xs, batch_ys = mnist.train.next_batch(100)
	sess.run(train_step, feed_dict={xs: batch_xs, ys: batch_ys, keep_prob: 0.5})
	先是,获取100张图片 的数据,
	然后加载到 占位符空间里,成为真正的参数
	compute_accuracy(mnist.test.images[:1000], mnist.test.labels[:1000]))
	输入1000用于mnist测试图片,即 batch=1000,使用定义的compute_accuracy函数检测准确率
code8.py是code7的后续
主要分析如何建设层模型!!
	# define placeholder for inputs to network
	xs = tf.placeholder(tf.float32, [None, 784])/255.   # 28x28	
	ys = tf.placeholder(tf.float32, [None, 10])
	keep_prob = tf.placeholder(tf.float32)
	x_image = tf.reshape(xs, [-1, 28, 28, 1])
	# print(x_image.shape)  # [n_samples, 28,28,1]
		占位符 xs 表示输入的每张图片是28X18=784个像素点,未知有多少图片输入所以none
		由于图片像素点值都是从值 0-255 来记录颜色的!!,增大后续计算量级,所以 除以255,
		把值域从 0-255 压缩到 0-1,只是值的比例缩小了,没有改变值记录的图像信息
		xs 是一个二维数组,一维表示图片,一维表示图片数量(即batch大小)
		所以需要转换传换成 4维数组, [batch,高,寬,深度(通道数)]
	## conv1 layer ##
	W_conv1 = weight_variable([5,5, 1,32]) # patch 5x5, in size 1, out size 32
	b_conv1 = bias_variable([32])
	h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1) # output size 28x28x32
	h_pool1 = max_pool_2x2(h_conv1)                                         # output size 14x14x32
		第一层CNN
		1*32 个 5x5 卷积核 卷积batch张,28x28的图片,最后得batch张 含32个通道的卷积后图片
		然后,卷积后图片 都经过池化得到  batch张 含32个通道的池化后图片	
	## conv2 layer ##
	W_conv2 = weight_variable([5,5, 32, 64]) # patch 5x5, in size 32, out size 64
	b_conv2 = bias_variable([64])
	h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2) # output size 14x14x64
	h_pool2 = max_pool_2x2(h_conv2)                                         # output size 7x7x64
		第二层CNN
		32*64 个 5x5 卷积核 卷积batch张,含32个通道的上一层池化后图片,
		先得到batch张,含 32*64 个通道的卷积后图片
		然后,平均压简成 batch张,含 64 个通道的卷积后图片
		然后,卷积后图片 都经过池化得到  batch张 含64个通道的池化后图片				
	## fc1 layer ##
	W_fc1 = weight_variable([7*7*64, 1024])
	b_fc1 = bias_variable([1024])
	# [n_samples, 7, 7, 64] ->> [n_samples, 7*7*64]
	h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])
	h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)
	h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)
		全连接层1
		第二层CNN 得到的  batch张 含64个通道的池化后图片 是一个[batch,高,寬,深度(通道数)]4维数组
		转换成 二维数组[batch,第二层池化后图片],
		然后换 Wx_plus_b 的层模型,使用relu激励函数 继续构建
		这里添加了 dropout 处理,是为了避免过拟合问题	
	## fc2 layer ##
	W_fc2 = weight_variable([1024, 10])
	b_fc2 = bias_variable([10])
	prediction = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)
		全连接层1
		同样使用 Wx_plus_b 的层模型 使用softmax激励函数 最后得到 学习的 one-hot 标签向量!!
	# the error between prediction and real data
	cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction),
                                              reduction_indices=[1]))       # loss
	train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)
		同样通过 -tf.reduce_sum(ys * tf.log(prediction),reduction_indices=[1])
		算出每张图片的 交叉熵,然后 tf.reduce_mean 求平均得到一个平均交叉熵
		通过 优化器 AdamOptimizer 优化器处理平均交叉熵 来执行反向传导,
		更新一次所有的学习参数(包括所有的卷积核,所有的weight和biase)










20180404
RNN 
code9.py 根据sin 画出cos
首先看 执行训练的部分:
	_, cost, state, pred = sess.run(
            [model.train_op, model.cost, model.cell_final_state, model.pred],
            feed_dict=feed_dict)
	sess.run先后执行了4个函数模块 model.train_op, model.cost, model.cell_final_state, model.pred
	sess.run执行 train_op时,会回溯执行嵌套操作
		当前的 cost,cell_final_state, pred,最后更新了一次学习模型
	执行完train_op后 ,sess.run继续执行 cost
		这时执行的 cost,得到的是 更新学习模型后的状态下得到的 cost
	sess.run继续执行 cell_final_state,得到的是 更新学习模型后的状态下得到的 cell_final_state
	sess.run继续执行 pred,得到的是 更新学习模型后的状态下得到的 pred
初步了解 (只考虑batch为1,即喂一堆段数据的情况)
	这里每次input是一段数据有 20 个数据单元(20个x数据得到的sin(x) ) ,
	然后每个数据单元通过 同一输入转化矩阵 wx_plus_b 扩成 10维输入向量:x1,x2...x10
	就是说输入转化矩阵有 10 个 W 和 B 需要学习
	然后 lstm 的cell 和hidden_unit 状态向量 都是10维向量 , 
	每次 给 lstm 喂 一个 10维输入向量 得到 一个 cell 和 hidden_unit 状态向量
	这个 cell 和 hidden_unit 状态向量,又作为下一次 喂 输入向量需要 的 cell 状态向量 和hidden_unit 状态向量
	有20个数据单元,所以一共喂 20次 10维输入向量
	收集 每次得到lstm 输出的 hidden_unit 状态向量,作为输出向量 即收集到 20个
	每个输出向量又通过 另一个 相同的 矩阵乘法 wx_plus_b 缩成一个输出数据
	输出转化矩阵有 10 个 W 和 B 需要学习
	得到20个输出数据(20个),
	再与20个真正的输出数据(20个x数据对应cos(x) 比较 得到一个 误差值(不是误差向量) 
	误差越小, 表示 从sin 推导出的 cos 越准确 !! 
	 
code10.py 同样先初步了解
这里是,一串一串地输入图片像数流数据,最后学习分辨出是什么图片!!!
初步了解
	这里每次input是一段数据有 28 条像素数据,
	每条像素数据 含28个像素点信息.
	每条像素数据 通过 同一个输入转化矩阵 wx_plus_b  扩成 128维向量x1,x2...x128
	输入转化矩阵有 128*28 个 W 和 128 个 B 需要学习
	然后 lstm 的 cell 和hidden_unit 状态向量 都是128维向量 , 
	每次 给 lstm 喂 一个 128维输入向量 得到 一个 cell 状态向量 和 hidden_unit 状态向量
	这个 cell 和 hidden_unit 状态向量,又作为下一次 喂 输入向量需要 的 cell 和hidden_unit 状态向量
	有 28 条像素数据,所以一共喂 28次 128维输入向量
	收集 每次得到lstm 输出的 hidden_unit 状态向量,作为输出向量 即收集到 20个
	把这 28 个 输出向量组成 一个 28维数组,即 28*128 矩阵,
	然而其实,我们只需要最后一个输出向量,(第28个),其他的丢弃
	后一个输出向量 乘以一个 转化矩阵 得到  一个 10维变量
	这个转化矩阵的 W 和 B 也需要学习
	这个10维变量 与 实际图片对应的 one-hot 变量比较 得到一个 误差值(不是误差向量)
	误差越小, 识别图片的准确率越高. 
参考:	
	http://dy.163.com/v2/article/detail/CTIPFRJF0511K58A.html
	https://www.zhihu.com/question/40819582
	https://blog.csdn.net/u014595019/article/details/52605693
	https://blog.csdn.net/u014595019/article/details/52759104
	https://www.jianshu.com/p/4e285112b988
lstm单元:
	t表示当前时间

                                     ht
		  ___________________|____
	C(t-1)----|                      |-----Ct
	          |                      |
		  |                      |
	          |                      |
	h(t-1)----|______________________|-----ht
	            |
		   xt

	xt     当前输入值,是一个向量!!
	h(t-1) 前一刻的 h 状态值,是一个向量!!  h 俗称 hidden_unit
	C(t-1) 前一刻的 C 状态值,也是一个向量!! C 俗称 Cell	
	ht     当前的 h 状态值,是一个向量!!  
	Ct     当前的 C 状态值,也是一个向量!! 
	注意: xt, h(t-1), ht, Ct, C(t-1) 向量维数相同,(一维数组 也称为向量)
	还有,xt是处理过的输入,比如,这代表一个句子中的一个词语,xt不是词语本身,而是对应处理过得到的向量
 	同样,yt是未处理的输入,yt这个向量需要做处理后才得到我们直接观察的结果
	三个门向量:
		输入门: it = sigmod{ (Wxi)(xt) + (Whi)(h(t-1)) }
		忘记门: ft = sigmod{ (Wxf)(xt) + (Whf)(h(t-1)) }
		输出门: ot = sigmod{ (Wxo)(xt) + (Who)(h(t-1)) }
	候选值向量:
		       ~
	               Ct  =  tanh{ (Wxc)(xt) + (Whc)(h(t-1)) }
 	当前的 C 状态值:
		                                     ~
	               Ct  =  ft ⊙  C(t-1)  +  it ⊙  Ct
	当前的 h 状态值:
		       ht  =  ot ⊙  Ct
	注意:⊙ 是 自定义乘法, 这里是按元素乘法 ,例如:门[0,1,1,0,0,1] X 向量[2,3,4,5,6,7] = [0,3,4,0,0,7]
	向量的按元素乘法也叫:Hadamard product (also known as the Schur product
	这里由于门向量的成员基本上不是0,就是1,所以就有所谓开关的意义,
	所以就可以让一部分内容向后传输,阻隔一部分输出,达到长短记忆的作用
	而且也说明为啥 xt, h(t-1), ht, Ct, C(t-1) 向量维数相同
	注意 sigmod 得到的值不是无限接近0就是无限接近1
	注意 tanh   得到的值是 -1到1 之间
	后向传播:(BPTT算法) 参考
		   :https://blog.csdn.net/dark_scope/article/details/47056361
		   :https://blog.csdn.net/u012319493/article/details/52802302
rnn-lstm输入例子图:	
	          ht        h(t+1)        h(t+2)        h(t+3)
	       ___|__    ___|__        ___|__        ___|__
	C(t-1)-|    |-Ct-|    |-C(t+1)-|    |-C(t+2)-|    |-C(t+3)-....
	       |    |    |    |        |    |        |    |
	       |  A |    |  A |        |  A |        |  A |
	       |    |    |    |        |    |        |    |
	h(t-1)-|____|-ht-|____|-h(t+1)-|____|-h(t+2)-|____|-h(t+3)-....
	        |         |             |             |
	        xt       x(t+1)        x(t+2)        x(t+3)
rnn-lstm多测层网络例子图:	
		           .         .              .              .
		           .         .              .              .
		           .         .              .              .
		          h3t      h3(t+1)        h3(t+2)        h3(t+3)
		        __|___     __|___         __|___         __|___
		C3(t-1)-|    |-C3t-|    |-C3(t+1)-|    |-C3(t+2)-|    |-C3(t+3)-....
		        |    |     |    |         |    |         |    |
	第三层	        |  C |     |  C |         |  C |         |  C |
		        |    |     |    |         |    |         |    |
		h3(t-1)-|____|-h3t-|____|-h3(t+1)-|____|-h3(t+2)-|____|-h3(t+3)-....
		          |          |              |              |
		          h2t      h2(t+1)        h2(t+2)        h2(t+3)
		        __|___     __|___         __|___         __|___
		C2(t-1)-|    |-C2t-|    |-C2(t+1)-|    |-C2(t+2)-|    |-C2(t+3)-....
		        |    |     |    |         |    |         |    |
	第二层	        |  B |     |  B |         |  B |         |  B |
		        |    |     |    |         |    |         |    |
		h2(t-1)-|____|-h2t-|____|-h2(t+1)-|____|-h2(t+2)-|____|-h2(t+3)-....
		          |          |              |              |
		          h1t      h1(t+1)        h1(t+2)        h1(t+3)
		        __|___     __|___         __|___         __|___
		C1(t-1)-|    |-C1t-|    |-C1(t+1)-|    |-C1(t+2)-|    |-C1(t+3)-....
		        |    |     |    |         |    |         |    |
	第一层	        |  A |     |  A |         |  A |         |  A |
		        |    |     |    |         |    |         |    |
		h1(t-1)-|____|-h1t-|____|-h1(t+1)-|____|-h1(t+2)-|____|-h1(t+3)-....
		          |          |              |              |
		         xt        x(t+1)         x(t+2)         x(t+3)
一次典型的训练过程:
	例如,每次给 rnn 喂一句话,然后反向传导训练一次网络.的过程
	首先,每句话后假设有 30 个单词,少于30个词语的,也假作有30个单词,剩余用"空白"代替单词位置
	于是可以把句子 分成 30 个steps, 又把每个单词通过 Wx_plus_b 转成 输入向量.
	向量成员数 与 rnn 的 cell/hidden_unit的成员数一样
	假如我们 使用的 是一个 3层lstm 网络,如上图, 那么只有三个lstm单元 A B C
	第1刻  输入第一个单词 向量 xt 到 A单元, 
	       A单元 输出的  h1t     输入到 B单元,
	       B单元 输出的  h2t     输入到 C单元,
	       最后  C单元输出  h3t
	       每个 lstm 单元都需要的 h?(t-1),C?(t-1)可能是最新一次训练的到的值,也可能是第一次训练,所以随机值
	第2刻  输入单词向量  x(t+1)  到A单元, 
	       A单元 输出的  h1(t+1) 输入到 B单元,
	       B单元 输出的  h2(t+1) 输入到 C单元,
	       最后  C单元输出  h3(t+1)
	....
	第30刻 输入单词向量  x(t+29) 到A单元,
	       A单元 输出的 h1(t+29) 输入到B单元,
	       B单元 输出的 h2(t+29) 输入到C单元,
	       最后  C单元输出  h3(t+29)
	根据目的选取结果: h3t,h3(t+1),..h3(t+29) 组成的就是一个回答向量组
		然后再对每个向量执行 另一套 Wx_plus_b 转成 单词	最后组成答句.
		对比本来的答句 得到 误差 然后就可以执行反向传导 
		更新 3个lstm A,B,C,单元的参数,还有输入输出转化矩阵的参数
		特别注意, 还会更新 输入单词 转 输入向量的 Wx_plus_b
			  还会更新 输出h3(t+?) 转 输出单词的 Wx_plus_b
	也就是对于 lstm 3个单元 A,B,C 会连续 喂 30 遍数据,算出最后得到的结果,
	才会执行一次反向传导更新 A,B,C 单元里的内容 !!	
	还有一种情况,就是 只选取 最后一个 h3(t+29) 作为输出
		其他时间点的 h3(t+?) 直接丢弃
		根据 h3(t+29) 经输出转化矩阵得到的结果 与真实结果比较 得到误差
		最后,反向传导,更新 更新 3个lstm A,B,C,单元的参数,还有输入输出转化矩阵的参数

深入分析code9.py:
首先获取 batch 段数据, 每段数据由 TIME_STEPS 个数据单元组成,一次训练 喂 batch段数据
	所以每次输入数据的 self.xs 的 shape 是[(batch, n_steps, 输入数据单元)]
	把每个数据单元 转成 cell_size 维数据单元向量. 
	那么得到的转换后 的 输入数据 self.l_in_y 的 shape 是 [(batch, n_steps, cell_size)]
这里的 RNN 只用一层 lstm, 就是说只有一个 lstm单元
	初始化 lstm 单元:
		lstm_cell = tf.contrib.rnn.BasicLSTMCell(self.cell_size, forget_bias=1.0, state_is_tuple=True)
		BasicLSTMCell参数:
		self.cell_size 就是 C状态值维数,h状态值维数,输入数据单元向量维数,都是同一个数量
		forget_bias=1.0 如果忘记门是有偏移的 ft = sigmod{ (Wxf)(xt) + (Whf)(h(t-1)) + bias }
		这个就是那个偏移值参数
		state_is_tuple=True 意味着 每次 lstm单元 输出的状态值 是 [C状态值,h状态值] 的组合数组  
	初始化 lstm 单元的C状态值 和 h状态值 这里初始化都为零:           
		self.cell_init_state = lstm_cell.zero_state(self.batch_size, dtype=tf.float32)
		self.cell_init_state 模型状态值初始值,因为 state_is_tuple=True 的原因
			shape是 [batch_size,[C状态值维数+h状态值维数=]]
	设置 rnn 的训练过程:
		self.cell_outputs, self.cell_final_state = tf.nn.dynamic_rnn(
            		lstm_cell, self.l_in_y, initial_state=self.cell_init_state, time_major=False)
		这里 rnn 网络 是 单层的 lstm,
		rnn输入数据是 self.l_in_y
		rnn 状态的初始值,这里指的是lstm的状态初始值 self.cell_init_state
		time_major 跟 input数据有关系,其实是跟训练方式有关系:
			当 self.l_in_y的shape是 [(batch, n_steps, cell_size)],time_major=False
			当 self.l_in_y的shape是 [(n_steps, batch, cell_size)],time_major=True
		self.cell_outputs 是训练后得到的输出, shape是 (batch * n_steps, cell_size)
			也就说,每刻(step)喂一数据单元数组到rnn网络, 就有 cell_size维输出向量
			喂完一段数据,就是前后喂完 n_steps , 得到 n_steps个cell_size维输出向量
			喂了 batch 段数据, 就有 batch 个 (n_steps个cell_size维输出向量)
		self.cell_final_state 是得到最后模型状态值, shape与self.cell_init_state的一样
rnn网络输出数据处理:
	self.cell_outputs 的 shape是 (batch * n_steps, cell_size)
	我们要对 cell_size维输出向量 转成 我们处理的输出数据单元
	最后得到输出数据 self.pred , shape为: (batch * steps, 输出数据单元)
误差处理:
	经过训练得到的输出数据 和 真实输出数据 的误差
	    def compute_cost(self):
	        losses = tf.contrib.legacy_seq2seq.sequence_loss_by_example(
	            [tf.reshape(self.pred, [-1], name='reshape_pred')],
	            [tf.reshape(self.ys, [-1], name='reshape_target')],
	            [tf.ones([self.batch_size * self.n_steps], dtype=tf.float32)],
	            average_across_timesteps=True,
	            softmax_loss_function=self.ms_error,
	            name='losses'
	        )
	        with tf.name_scope('average_cost'):
	            self.cost = tf.div(
	                tf.reduce_sum(losses, name='losses_sum'),
	                self.batch_size,
	                name='average_cost')
	            tf.summary.scalar('cost', self.cost)
	    @staticmethod
	    def ms_error(labels, logits):
	        return tf.square(tf.subtract(labels, logits))
	tf.contrib.legacy_seq2seq.sequence_loss_by_example 是计算误差的一种方法,
	现在仅仅就具体例子分析,未打算系统说明这个函数的内容:
		参考:https://tensorflow.google.cn/api_docs/python/tf/contrib/seq2seq/sequence_loss
		    :https://blog.csdn.net/liuchonge/article/details/71424432
		第一个参数 logits, 一般是shape为 [batch,nsteps]
			特别要讨论下这里的shape:
			譬如,输入本来的shape 是 [batch, steps, 数据单元向量]:[50, 30, 20],
			必须reshape成二维数组: [batch, steps*数据单元向量]:[50, 600],才能作为logits输入,
			这时的 nsteps 就是 600 了!!
			现在,我们输入的 self.pred 的shape 是 (batch * steps, 输出数据单元):[50*20,1]
			对应着 logits 的 shape [batch,nsteps]: [50*20,1]
			而我们先把 self.pred reshape 成 一维数组 [batch]:[50*20*1] 再输入到 logits
			显然 logits 把输入的 self.pred:[batch]:[50*20*1]看成self.pred:[batch,nsteps]:[50*20,1]
			也就说 一维数组 [batch], 和二维数组 [batch,1] 并没有区别
		第一个参数 targets, 一般是shape为 [batch,nsteps]
			同样我们先把 self.ys reshape 成 一维数组 [batch]:[50*20*1] 再输入到 targets
			即,显然 targets 把输入的 self.ys:[batch]:[50*20*1]看成self.ys:[batch,nsteps]:[50*20,1]
		第三参数 权重 Weight 表示要对不同loss,的重视程度: 一般是shape为 [batch,nsteps]
			这里要求每个loss重视程度都一样,所以 weights 都为1
		第四参数:average_across_timesteps=True,表示求 timesteps 平均,后续解释
		第五参数:average_across_batch=True,表示求 batch 平均,注意:这里没有填入,没有使用,所以默认False后续解释
		第六参数:softmax_loss_function=None,如果使用默认loss单元函数,填None
		第七参数:name=None给这个 定义的计算误差的方法 命名,也可以不命名,为none
			图例解释: logits,target,weights 的shape是一样的!!!
			假如
			self.pred:[batch,n_step,数据单元向量]:[2,3,4]
				|[p111,p112,p113,p114]| ,|[p211,p212,p213,p214]|
				|[p121,p122,p123,p124]|  |[p221,p222,p223,p224]|			
				|[p131,p132,p133,p134]|  |[p231,p232,p233,p234]|
			self.ys:[batch,n_step,数据单元向量] 也一定是 [2,3,4]
				|[y111,y112,y113,y114]| ,|[y211,y212,y213,y214]|
				|[y121,y122,y123,y124]|  |[y221,y222,y223,y224]|			
				|[y131,y132,y133,y134]|  |[y231,y232,y233,y234]|
			假如被reshape成以下样子:
			self.pred 成 logits:[batch,nsteps]:[2,3*4]
				|p111,p112,p113,p114,p121,p122,p123,p124,p131,p132,p133,p134|
				,
 				|p211,p212,p213,p214,p221,p222,p223,p224,p231,p232,p233,p234|
			self.ys 成 targets:[batch,nsteps]:[2,3*4]
				|y111,y112,y113,y114,y121,y122,y123,y124,y131,y132,y133,y134| 
				,
				|y211,y212,y213,y214,y221,y222,y223,y224,y231,y232,y233,y234|
			那么我们要求 weight 也是这样子:
				|w11,w12,w13,w14,w15,w16,w17,w18,w19,w110,w111,w112|
				,	
				|w21,w22,w23,w24,w25,w26,w27,w28,w29,w210,w211,w212|
			如果 softmax_loss_function=None ,就使用default-loss单元函数,
				如果不想使用 default-loss单元函数,就得设置 softmax_loss_function
				假如我们使用的误差是 均方差:mean squared error(MSE), 
				有必要说明 均方误差 MSE = E( (y-y')^2 ) =  ( (y1-y'1)^2 + .. + (yn-y'n)^2 ) /n
				我们设置的 softmax_loss_function = ms_error(labels, logits) 
				而函数 ms_error(labels, logits) 只实现了 差的平方 (y-y')^2,
				即只有 tf.square(tf.subtract(labels, logits))
				而E()部分,"即(..+..+..+..)/n)" 并不在 ms_error 里实现.
				而是 tf.contrib.legacy_seq2seq.sequence_loss_by_example 透过
				average_across_batch 或者 average_across_timesteps 条件实现
				一般 average_across_timesteps,  average_across_batch 只能选其中一个为True
			假如 average_across_timesteps=True 表示平均 nsteps 这维
			那么:
			这个定义了的误差方法得到的 结果
			A =  ( w11*(p111-y111)^2 + w12*(p112-y112)^2 + ... + w112*(p134-y134)^2 ) / (3*4)
			B =  ( w21*(p211-y211)^2 + w22*(p212-y212)^2 + ... + w212*(p234,y234)^2 ) / (3*4)
			当weights数组的值 w?? 都为 1 时 , 表示所有 差的平方 都重视
			这样就实现了在 nsteps维上 的 均方差 MSE = E( (y-y')^2 )
			当weights数组的值 w?? 不一样时 , 表示部分 差的平方 得到重视 
			这样就实现了在 nsteps维上 的 配置了权重的均方差 MSE = E( (y-y')^2 )
			最后得到 一个 二维向量(一维数组) [A,B]
			假如 average_across_batch=True 表示平均 batch 这维
			那么: 
			这个定义了的误差方法得到的 结果
			a =  ( alo(p111,y111)*w11 + alo(p211,y211)*w21 ) / 2
			b =  ( alo(p112,y112)*w12 + alo(p212,y212)*w22 ) / 2
			c =  ( alo(p113,y113)*w13 + alo(p213,y213)*w23 ) / 2
			b =  ( alo(p114,y114)*w14 + alo(p214,y214)*w24 ) / 2
			....
			l =  ( alo(p134,y134)*w112 + alo(p234,y234)*w212 ) / 2
			当weights数组的值 w?? 都为 1 时 , 表示所有 差的平方 都重视
			这样就实现了在 batch维上 的 均方差 MSE = E( (y-y')^2 )
			当weights数组的值 w?? 不一样时 , 表示部分 差的平方 得到重视 
			这样就实现了在 batch维上 的 配置了权重的均方差 MSE = E( w(y-y')^2 )
			最后得到 一个 十二维向量(一维数组) [a,b,c,d,e,f,g,h,i,j,k,l]
	回到 losses = tf.contrib.legacy_seq2seq.sequence_loss_by_example()
		self.pred:[batch]:[50*20*1] -> logits :[batch,nsteps]:[50*20*1,1]
		self.ys  :[batch]:[50*20*1] -> targets:[batch,nsteps]:[50*20*1,1]
		weights  :[batch]:[50*20*1] -> weights:[batch,nsteps]:[50*20*1,1]
		即 |p1,p2,p3,,,,p100|
		   |y1,y2,y3,,,,y100|
		   |w1,w2,w3,,,,w100|
		而且 w?? 的值都为 1
		我们斌不打算使用 tf.contrib.legacy_seq2seq.sequence_loss_by_example 默认方式求误差,
		同时 我们通过 均方差的方式 求出,误差, 并不是 交叉熵的方式,
		所以我们设置 softmax_loss_function=self.ms_error,只实现了 差的平方 (y-y')^2,
		我们设置了 average_across_timesteps=True,平均 nsteps 这维
		因为 nsteps 为 1,即只有一个成员
		于是,这个定义了的误差方法得到的 结果
		l1 = ( (p1-y1)^2 ) / 1
		l2 = ( (p2-y2)^2 ) / 1
		l3 = ( (p3-y3)^2 ) / 1
		...
		l1000 = ( (p100-y100)^2 ) / 1
		即得到一个 100维向量 [l1,l2,l3,,,l100]
		tf.contrib.legacy_seq2seq.sequence_loss_by_example 输出的 losses:[l1..l1000],
		是针对这个函数意义上的batch:1000 ,
		实际数据只有 50个batch, 每个batch有20个step,
		我们使用这个函数就预处理为把每个step都当成batch处理
	计算cost:  self.cost = tf.div(
	                tf.reduce_sum(losses, name='losses_sum'),
	                self.batch_size,
	                name='average_cost')
	            tf.summary.scalar('cost', self.cost)
		tf.reduce_sum: 把losses的1000个成员都加起来,
		tf.div: 然后除以 self.batch_size:50,
		最后得到的就是 平均 cost ,是一个值, 
		相当于每个 batch 的 20个Time_steps的loss加起来得到一个cost
		把50个cost加起来,再平均!!!
最后分析训练过程!!
	第一次:	feed_dict = {
	                    model.xs: seq,
	                    model.ys: res,
	                    # create initial state
	            }
		_, cost, state, pred = sess.run(
        	    [model.train_op, model.cost, model.cell_final_state, model.pred],
        	    feed_dict=feed_dict)
		先执行model.train_op,
			执行 tf.train.AdamOptimizer(LR).minimize(self.cost)
			需要 self.cost,
			所以得先执行 tf.div(.. losses, ,,self.batch_size),
			需要 losses 和 self.batch_size, self.batch_size已知,losses,未知
			所以得先执行 tf.contrib.legacy_seq2seq.sequence_loss_by_example
			需要 self.pred, self.ys, self.batch_size, self.n_steps	 
			self.batch_size, self.n_steps 已知,
			self.ys 是占位符号: 传入了 model.ys: res
			要取得self.pred,所以得先执行tf.matmul,
			需要 l_out_x, Ws_out, bs_out
			Ws_out, bs_out 是 被定义为要 训练 variable
			要取得 l_out_x,即取得 self.cell_outputs,
			所以得先执行 tf.nn.dynamic_rnn
			需要 lstm_cell, self.l_in_y, self.cell_init_state,
			lstm_cell, self.cell_init_state, 已知
			要取得 self.l_in_y, 即取得 l_in_x ,即取得 self.xs
			self.xs 是占位符号: 传入了 model.xs: seq
			最后,再最后一步步往前执行,
			回到 tf.train.AdamOptimizer(LR).minimize(self.cost) 执行反向传导更新参数
			最后执行完毕,返回 数据 放入 _
		然后执行model.cost,
			得先执行 tf.div(.. losses, ,,self.batch_size),
			需要 losses 和 self.batch_size, self.batch_size已知,losses,未知
			所以得先执行 tf.contrib.legacy_seq2seq.sequence_loss_by_example
			需要 self.pred, self.ys, self.batch_size, self.n_steps	 
			self.batch_size, self.n_steps 已知,
			self.ys 是占位符号: 传入了 model.ys: res
			要取得self.pred,所以得先执行tf.matmul,
			需要 l_out_x, Ws_out, bs_out
			Ws_out, bs_out 是 被定义为要 训练 variable
			要取得 l_out_x,即取得 self.cell_outputs,
			所以得先执行 tf.nn.dynamic_rnn
			需要 lstm_cell, self.l_in_y, self.cell_init_state,
			lstm_cell, self.cell_init_state, 已知
			要取得 self.l_in_y, 即取得 l_in_x ,即取得 self.xs
			self.xs 是占位符号: 传入了 model.xs: seq
			最后,再最后一步步往前执行,直到完毕,返回 数据 放入 cost
		然后执行 model.cell_final_state,
			得先执行 tf.nn.dynamic_rnn
			需要 lstm_cell, self.l_in_y, self.cell_init_state,
			lstm_cell, self.cell_init_state, 已知
			要取得 self.l_in_y, 即取得 l_in_x ,即取得 self.xs
			self.xs 是占位符号: 传入了 model.xs: seq
			最后,再最后一步步往前执行,
			tf.nn.dynamic_rnn 会得到 self.cell_outputs, self.cell_final_state
			但是 sess.run 只想得到 self.cell_final_state
			完毕后只返回 self.cell_final_state 数据 放入 state
		最后执行 model.pred
			得先执行tf.matmul,
			需要 l_out_x, Ws_out, bs_out
			Ws_out, bs_out 是 被定义为要 训练 variable
			要取得 l_out_x,即取得 self.cell_outputs,
			所以得先执行 tf.nn.dynamic_rnn
			需要 lstm_cell, self.l_in_y, self.cell_init_state,
			lstm_cell, self.cell_init_state, 已知
			要取得 self.l_in_y, 即取得 l_in_x ,即取得 self.xs
			self.xs 是占位符号: 传入了 model.xs: seq
			最后,再最后一步步往前执行,直到完毕,返回 数据 放入 pred
	第一次以后的:feed_dict = {
	                model.xs: seq,
	                model.ys: res,
	                model.cell_init_state: state    # use last state as the initial state for this run
	            }
		_, cost, state, pred = sess.run(
        	    [model.train_op, model.cost, model.cell_final_state, model.pred],
        	    feed_dict=feed_dict)
		这时 特别注意到 model.cell_init_state: state
		因为,model.cell_init_state 不像 xs,ys 被定义为 placeholder 占位符号, 
		也不像 weight, bias,那种 被定义为 要被训练的 variable
		初始化时,是这样子的
			self.cell_init_state = lstm_cell.zero_state(self.batch_size, dtype=tf.float32)
		应该像 self.cell_size 那样的变量, model里内部初始化,内部赋值的
		但现在, model.cell_init_state: state 的操作,
		相当于, self.cell_init_state  被外部赋值了, 有占位符号那样的作用.
		也就说模型里 非 variable 和 placeholder 变量, 其实也有 placeholder的作用
		可以 ,被外部赋值,取代原有值 !!

深入分析code10.py
	和code9.py的区别是, 输入的是一张28*28图片
	一张图片 分成 28 条数据,即分28次喂入模型, 每条数据 28个像素点
	每次喂 1 条数据, 每条数据 转换成 128维向量 再喂入单层 lstm 模型
	只取 最后一次,即第28次喂入数据后 得到的 128维输出向量
	再 转化 得到 10维向量 与 图片原本指向的 10维向量做比较
cell 状态向量 和hidden_unit 状态向量 初始值问题:
	由于每张图片输入rnn 模型过程中,都与另一张输入不一样
	所以前一张的图片训练得到 的 cell 状态向量 和hidden_unit 状态向量
	不需要传导到 下一张图片的识别,所以每次训练一张图片用到的 
		 cell 状态向量 和hidden_unit 状态向量的初始值都为 0 !!
只获取第28次喂入数据后 得到的 128维输出向量 做比较的问题 
	这里的重点是 如何取第28次得到 128维输出向量:
    	outputs, final_state = tf.nn.dynamic_rnn(cell, X_in, initial_state=init_state, time_major=False)
	输出的 outputs 的shape [batch,steps,输出向量维数]:[128,28,128]
		即包含 128张图片 同时喂入模型 得到的128个结果,
		每个结果有28条 128维输出向量, 
		每条输出向量 对应 每次(step) 喂入的一个输入向量
		现在我们要 每个结果 的 最后一条 128维输出向量, 即第28次喂入输出向量 得到的 128维输出向量
		outputs = tf.unstack(tf.transpose(outputs, [1,0,2]))
		outputs 是一个 三维数组 即shape [batch,steps,输出向量维数]
		batch 这一维的 标记为0,
		steps 这一维的 标记为1,
		输出向量维数 这一维的 标记为2,
		tf.transpose(outputs, [1,0,2]:表示把 batch steps 两维转换
		即outputs 变成了 [steps,batch,输出向量维数]
		图例直观解析
			假如 outputs本来是这样的 shape:[3,2,5]:
			[
	
				[
					[1,1,1,1,1,1]
					[2,2,2,2,2,2]		
				]
				,
	
				[
					[3,3,3,3,3,3]
					[4,4,4,4,4,4]		
				]
				,
	
				[
					[5,5,5,5,5,5]
					[6,6,6,6,6,6]		
				]
				,
			]
			transpose(outputs, [1,0,2] -> outputs shape:[2,3,5]:
			只换第 batch steps 维, 
			输出向量维数 这维内容不变, 这维可以简单标记:
				A = [1,1,1,1,1,1]
				B = [3,3,3,3,3,3]
				C = [5,5,5,5,5,5]
				D = [2,2,2,2,2,2]		
				E = [4,4,4,4,4,4]		
				F = [6,6,6,6,6,6]
	    		outputs 简单记为:
			[[A,B]
			 [C,D]
			 [E,F]]
			换第 batch steps 后:
			[[A,C,E]
			 [B,D,F]]
			把标记 ABCDEF换换回去就得:
			[
	
				[
					[1,1,1,1,1,1]
					[3,3,3,3,3,3]
					[5,5,5,5,5,5]
	
				]
				,
	
				[
					[2,2,2,2,2,2]		
					[4,4,4,4,4,4]		
					[6,6,6,6,6,6]		
				]
				,
			]		
	
		解构数组:tf.unstack()
		没有其他参数,默认解构最前的一维,即 steps维,即第0维
		解构图示:
			假如 outputs是这样子的:
			outputs = [
		
					[
						[1,1,1,1,1,1]
						[2,2,2,2,2,2]		
					]
					,
	
					[
						[3,3,3,3,3,3]
						[4,4,4,4,4,4]		
					]
					,
		
					[
						[5,5,5,5,5,5]
						[6,6,6,6,6,6]		
					]
	
				]
			tf.unstack(outputs) 后,得到
			outputs[0] = [ [1,1,1,1,1,1]
				       [2,2,2,2,2,2] ]
			outputs[1] = [ [3,3,3,3,3,3]
				       [4,4,4,4,4,4] ]
			outputs[2] = [ [5,5,5,5,5,5]
				       [6,6,6,6,6,6] ]
			output 从 变量 变成了 数组变量 !!
			另外 outputs[-1] 等于 outputs[2], outputs[-1]表示数组的变量的最后一个成员!!
		因此 真正outputs tf.transpose转换后 得 shape:[steps,batch,输出向量维数]
		说明最后一steps 的 [batch,输出向量维数]:[128,128]
		放着是 128 条 第28次喂入输出向量 得到的 128维输出向量 
		我们只要最后一steps 的 [batch,输出向量维数]:[128,128] 
		所以执行了 tf.unstack 解构,
		outputs变成了 数组变量,我们只有这个数组变量最后一个,即 outputs[-1]
		results = tf.matmul(outputs[-1], weights['out']) + biases['out']    # shape = (128, 10)
		然后 每条最后的128维输出向量 经过同一个 [128,10]的转换矩阵 得到 一条 10维one-hot向量
		得到的 results 是含有 128 条 10维one-hot向量,
		即128张图片 经网络后得到 128条  one-hot向量
		最后对比 one-hot向量 与图片实际的 标记向量, 得到误差,然后反向传导更新网络参数
	由于我们只需要 最后一条 128维输出向量, 即第28次喂入输出向量 得到的 128维输出向量
	而这个内容 跟 final_state 这个数组变量的 第2个成员 final_state[1] 是一样的!
	results = tf.matmul(final_state[1], weights['out']) + biases['out']
	同样得到含有 128 条 10维one-hot向量, 的 results
final_state 这个内容 是lstm训练后得到的 c状态值 和 h状态值!!  	
	final_state[0] 是 c状态值
	final_state[1] 是 h状态值,也等于当前的输出向量	

		







20180415
讨论无监督学习问题:
并没有什么特别难的内容
图片 128维向量 得到一个 128维向量 (称为编码过程)
128维向量 128维向量 自己组成一幅图片 (称为解码码过程)
图片本身,与网络生成的图片对比,得到 误差,
通过误差,反向传导,训练更新编码网络和解码网络








20180415
batch nomalizeion (BN)
与 优化器 和 激活函数 的概念都不一样!
有效 避免 梯度消失爆炸的问题.
有效 加速迭代,减少训练次数,减少计算负担,提高效率
跟 白化 这样的预处理 差不多,但意义性质又不太一样!!
https://blog.csdn.net/whitesilence/article/details/75667002
https://blog.csdn.net/intelligence1994/article/details/53888270
https://blog.csdn.net/happynear/article/details/44238541
http://ufldl.stanford.edu/wiki/index.php/%E7%99%BD%E5%8C%96

BN跟学习过的 向量正规化,归一化 矩阵归一化什么的,没有任何数学关系,或者借鉴意义.
假想我们有一 batch 样本,他们很多时都不可避免可能会有些比较固执的地方,
比如,某段时间,样本值,不是无限接近A,就是无限接近B,
然而 我们知道真实情况 样本值是平均分布在 A-Z 之间的.
那么这样导致的结果就是,这段时间内的多次模型学习训练,梯度收敛都特别小,甚至没有收敛. 
这种情况下, 我们使用BN 就是使这些样本的近似部分弱化,差异部分放大.
这样子,学习训练时,梯度下降特别快,模型的学习效率提高!!
BN操作,不仅仅在数据开始时操作,而且也要在 模型层里加入处理
输入数据的BN处理!!
	一个batch有 m 个样本向量 x1,x2,...xm
	             m
	uB = (1/m) * ∑ xi	//求均值
		    i=1

	             m
	oB = (1/m) * ∑ (xi-uB)^2  //求均方差
		    i=1

	      	 xi-uB
	xi' = ____________	  //标准化, e是一个很小的定值, 比如0.001, 是为了防止分母为0的情况
	      √(oB^2 + e )

		
	yi = Y * xi' + B	//反标准化, Y是scalc, B是shift, Y和B是要学习的参数,
				//这里的作用是, 配置标准化的程度
	最后大得到的 y1,y2,,,ym 就是 BN 后得到的 样本,
	简记为 y = BN(X)
学习模型層的BN处理!!
	我们知道数据经过一层处理后得到的数据,作为下一层的输出例如:
		某层的输入 为 x, 经过一层神经元处理,得到输出 wx_plus_b
		而 wx_plus_b 会经过激活函数例如relu处理后得到 relu(wx_plus_b)
		relu(wx_plus_b) 就是下一层的输入!!
	而添加BN处理时,一般是这样的:
		某层的输入 为 x, 经过一层神经元处理,得到输出 wx_plus_b
		输出先做BN处理得 BN(wx_plus_b)
		然后 BN(wx_plus_b) 经过激活函数例如relu处理后得到 relu(BN(wx_plus_b))
		relu(BN(wx_plus_b)) 就是下一层的输入!!
	注意,如果未先BN处理, 最后得到的 relu 后 内容可能大部分分布在小于0的地方,而直接被等于0处理
		这样就没有更多有效信息传到下一层,而导致模型接下来的模型层没有明显梯度传导
		最终传导的学习梯度下降缓慢,
		BN处理后,最后得到的 relu 后 内容,会相对减少分布在小于0的地方,
		可以把更多有用信息传到下一层.最终传导的加速学习梯度下降,
	更直观的来说,经过 BN 处理后的内容被缩小到处于 [-1,1] 之间, 
		这样, 再经过激活函数得到的内容 大多数据 不会处于 几乎或过分饱和值域!!
		这样就有充分信息 向下一级传导!!
		有效避免梯度爆炸和消失
解析了这么多,但仍然觉得并不了解 BN 的实质!!,日后再深化吧

code12.py
	np.random.seed(1)	//用来产生相同的随机数,生成随机数操作前,都seed(同一个数),生成的随机数相同
	这个例子的样品是 2次曲线 散布点,
	学习散布点分布规律
通过 code12.py
深化认识 python 的 一些语法原理 特别是函数运算空间
	认识 python 的全局变量和 局部变量 , 局部函数引入全局变量要加 global
	每执行一次 局部函数 都是重新建立一个空间存储 当前局部环境和变量
函数执行环境分析
	train_op, cost, layers_inputs = built_net(xs, ys, norm=False)   # without BN 
	train_op_norm, cost_norm, layers_inputs_norm = built_net(xs, ys, norm=True) # with BN
	这时,其实是建立了两个分别不一样的 built_net 函数环境
A环境:
train_op, cost, layers_inputs: 
___________________________________________________
|build_net (A)                                	  |					
|                                                 |
|  train_op: 	//仅属于这里的train_op            |
|  _____________________________                  |
|  |Gradient.minimize (A)      |                  |
|  |                           |                  |
|  |___________________________|                  |
|                                                 |
|    ^                                            |
|    |                                            |
|  cost:	//仅属于这里的cost                |
|  _____________________________                  |
|  |reduce_mean (A)            |                  |
|  |                           |                  |
|  |___________________________|                  |
|                                                 |
|    ^                                            |
|    |                                            |
|  prediction:     //仅属于这里的prediction       |
|  __________________________________________     |
|  |add_layer  (A,n+1)                      |     |
|  |    这个add_layer操作 自己的的局部变量有 |     |
|  |    weights,biases                      |     |
|  |    Wx_plus_b,                          |     |
|  |    outputs                             |     |
|  |________________________________________|     |
|                                                 |
|    ^                                            |
|    |                                            |
|  layers_inputs[n]:     //仅属于这里的prediction |
|  __________________________________________     |
|  |add_layer  (A,n)                        |     |
|  |    这个add_layer操作 自己的的局部变量有 |     |
|  |    weights,biases                      |     |
|  |    Wx_plus_b,                          |     |
|  |    outputs                             |     |
|  |________________________________________|     |
|                                                 |
|    ^                                            |
|    |                                            |
|  layers_inputs[n-1]:   //仅属于这里的prediction |
|  __________________________________________     |
|  |add_layer  (A,n-1)                      |     |
|  |    这个add_layer操作 自己的的局部变量有 |     |
|  |    weights,biases                      |     |
|  |    Wx_plus_b,                          |     |
|  |    outputs                             |     |
|  |________________________________________|     |
|                                                 |
|  ..........                                     |   
|                                                 |
|  layers_inputs[0]:     //仅属于这里的prediction |
|  __________________________________________
|  |add_layer  (A,0)                        |     |
|  |    这个add_layer操作 自己的的局部变量有 |     |
|  |    weights,biases                      |     |
|  |    Wx_plus_b,                          |     |
|  |    outputs                             |     |
|  |________________________________________|     |
|                                                 |
|    ^                                            |
|    |                                            |
|    xs                                           |
|_________________________________________________|

B环境:
train_op, cost, layers_inputs: 
___________________________________________________
|build_net (A)                                	  |					
|                                                 |
|  train_op: 	//仅属于这里的train_op            |
|  _____________________________                  |
|  |Gradient.minimize (A)      |                  |
|  |                           |                  |
|  |___________________________|                  |
|                                                 |
|    ^                                            |
|    |                                            |
|  cost:	//仅属于这里的cost                |
|  _____________________________                  |
|  |reduce_mean (A)            |                  |
|  |                           |                  |
|  |___________________________|                  |
|                                                 |
|    ^                                            |
|    |                                            |
|  prediction:     //仅属于这里的prediction       |
|  __________________________________________     |
|  |add_layer  (A,n+1)                      |     |
|  |    这个add_layer操作 自己的的局部变量有 |     |
|  |    weights,biases                      |     |
|  |    Wx_plus_b,                          |     |
|  |    Wx_plus_b = f(Wx_plus_b)            |     |
|  |    outputs                             |     |
|  |________________________________________|     |
|                                                 |
|    ^                                            |
|    |                                            |
|  layers_inputs[n]:     //仅属于这里的prediction |
|  __________________________________________     |
|  |add_layer  (A,n)                        |     |
|  |    这个add_layer操作 自己的的局部变量有 |     |
|  |    weights,biases                      |     |
|  |    Wx_plus_b,                          |     |
|  |    Wx_plus_b = f(Wx_plus_b)            |     |
|  |    outputs                             |     |
|  |________________________________________|     |
|                                                 |
|    ^                                            |
|    |                                            |
|  layers_inputs[n-1]:   //仅属于这里的prediction |
|  __________________________________________     |
|  |add_layer  (A,n-1)                      |     |
|  |    这个add_layer操作 自己的的局部变量有 |     |
|  |    weights,biases                      |     |
|  |    Wx_plus_b,                          |     |
|  |    Wx_plus_b = f(Wx_plus_b)            |     |
|  |    outputs                             |     |
|  |________________________________________|     |
|                                                 |
|  ..........                                     |   
|                                                 |
|  layers_inputs[0]:     //仅属于这里的prediction |
|  __________________________________________
|  |add_layer  (A,0)                        |     |
|  |    这个add_layer操作 自己的的局部变量有 |     |
|  |    weights,biases                      |     |
|  |    Wx_plus_b,                          |     |
|  |    Wx_plus_b = f(Wx_plus_b)            |     |
|  |    outputs                             |     |
|  |________________________________________|     |
|                                                 |
|    ^                                            |
|    |                                            |
|    b(xs)  //b()是做了batch-noralization (bN)    |
|    ^      //f()是先BN,最做激活函数处理           |
|    |                                            |
|    xs                                           |
|_________________________________________________|

	然后sess.run 的使用,就是重新利用这些已存在的函数空间,
	并不是再重新建立新空间!!
	虽然for循环 调用了多个 add_layer,
	但是,每一个 add_layer 都不一样.都有自己的weights等变量
	然而比较困惑在于, 每个 不一样的 add_layer 都把结果都给同一个 output变量
	那么gradient-minimize时,就追溯到 output 时 就不知道会发生什么事情.
	还有一个是 Wx_plus_b = f(Wx_plus_b) 的问题
	同样追溯到 Wx_plus_b 时, 又不知道要如何反向传导下去了,
	因为他们都请求自己,不过是请求之前的自己
	反向传导时应该会分析到 同个Wx_plus_b, output 变量被赋值的先后次序吧.
以上是值得思考的代码细节问题!!
	但先不再深究了








20180421
开始学习 强化学习!
学习分析 Q-learning 算法!!
	以地图游戏,寻找出口例子分析:
		先建立 一张 q 表, 记录每一位置点 s 的 所有行为 a 的权值 例如:
		       left     right        up      down
		0  0.000000  0.004320  0.000000  0.004320
		1  0.000000  0.025005  0.000400  0.004320
		2  0.000030  0.111241  0.000002  0.004320
		3  0.000000  0.368750  0.000000  0.004320
		4  0.027621  0.745813  0.000099  0.004320
		5  0.000000  0.000000  0.000000  0.004320
		Q(s,a)函数是得出 s 点,a 行为的 的 权值,
		例如 Q(4,right) = 0.745813, right:0.745813比其他行为的权值高,
		所以 在 4 点 采取 right行为 是最为推荐的选择.
		每个学习回合(episode)初,先确定好 q 表, 
		如果是开始回合,就初始化q表,否则沿用上回合最终更新的q表
	每个学习回合开的每一步工作内容如下:
		在当前点 s , 通过行为选取算法选取 一个行为 a
			其中 有 e_greedy 概率通过q表权值操作来选取 行为 a, 一般选最大权值的行为
			有 1-e_greedy 概率 随机选取行为!!
		执行操作 a , 根据操作到达下个点 s_, 并根据奖励算法得到行为应有的奖励 r
		询问q表得到 下个点 s_ 的 最大权值的行为 max_a_
		取得 Q(s_,max_a_) 为我们的参考权值
		执行学习更新 当前点s,当前行为a的 权值 Q(s,a):
			Q(s,a) = Q(s,a) + Alpha * [ r + gama * Q(s_,max_a_) - Q(s,a) ]
			Alpha是学习率
			GAMMA 是 discount factor
	重复一步又一步,直到第某步,执行操作 a 到达的下一点 s_ 是目标点,或者是陷阱点
		时,才正式结束结束这一回合.准备开始下一回合.				
	这里 奖励算法和行为选取算法都是自定义的,需要根据实际情况谨慎设计
	这里有必要详细讨论学习原理:
	     1,	一般学习时
		假如当前 s:3 选出的 a:up 是 s 里最大权值行为, 
		我们也知道 a:up 后 到达的 S_:1 的最大权值行为是 max_a_:left
		如果 r + gama * Q(s_,max_a_) - Q(s,a) 很小,
		意味着 s:3 执行 a:up 后到 s_:1 然后执行 max_a_:left 是当前认为是没毛病的操作流程
		如果 r + gama * Q(s_,max_a_) - Q(s,a) 很大时,
		s:3 执行 a:up 后到 s_:1 然后执行 max_a_:left 这串操作流被认为更加值得去执行
		于是, 把 r + gama * Q(s_,max_a_) - Q(s,a) 反馈更新 s:3 中 a:up 的权值,
		使得以后有机会重新回到 s:3 时,更有机会选出行为 a:up ,

	     2,	假若是充分学习后
		注意除了到达成功终点的那一步有 r外, 其他步的 r 都为 0, 陷阱得 负r
		由于只有成功终点有奖励,邻进终点的 s 里走向终点 的 a 有极高权值
		然后收敛的能到达成功终点的一套行为 [a] 每个都是 其所在 s 里 最高的权值
		假如当前 s:3 选出的 a:up 是 s 里最大权值行为, 
		我们也知道 a:up 后 到达的 S_:1 的最大权值行为是 max_a_:left
		由于 Q(s_,max_a_) 和 Q(s,a) 都是最大权值,而且比较接近
		r + gama * Q(s_,max_a_) - Q(s,a) 便很小,
		意味着 s:3 执行 a:up 后到 s_:1 然后执行 max_a_:left 是当前认为最正确操作流程
		Q(s:3,a) 并没有做太显著的改动
		假如当前 s:3 选出的 a:down 不是是 s 里最大权值行为,是随机行为 
		我们也知道 a:up 后 到达的 S_:4 的最大权值行为是 max_a_:right
		Q(s,a) 比较小, S_:4很小到达过,所以 Q(s_,max_a_) 也不大
		 r + gama * Q(s_,max_a_) - Q(s,a) 很小,
		意味着 s:3 执行 a:up 后到 s_:1 然后执行 max_a_:left 是当前认为最正确操作流程
		Q(s:3,a:dowm) 并没有做太显著的改动,
		那么 Q(s:3,a:up) 还是最大的,下次到 s:3, 还是会选 a:up
	逆向思考学习原理:
		假如有一个充分学习后的模型.q值表是这样的:
		       left     right        up      down
		0  0.999999  0.004320  0.000000  0.004320
		1  0.000000  0.025005  0.000400  0.004320
		2  0.000030  0.999999  0.000002  0.004320
		3  0.000000  0.368750  0.000000  0.004320
		4  0.027621  0.745813  0.999999  0.004320
		5  0.000000  0.000000  0.000000  0.004320
		6  0.000000  0.999999  0.000000  0.004320
		7  0.000000  0.025005  0.000400  0.004320
		8  0.000030  0.111241  0.999999  0.004320
		9  0.000000  0.000000  0.000000  0.000000
		10 0.027621  0.745813  0.000099  0.004320
		11 0.033030  0.000000  0.564363  0.004320
		先从地点 0 开始:
			0:left -> 4:up -> 2:right -> 6:right -> 8:up -> 9终点
			以后的回会没有意外都会沿着这条路径跑
		因为 s:0:a:left -> s_4:max_a_:up 的  r + gama * Q(s_,max_a_) - Q(s,a) 很小
		Q(0,left)就几乎没变化,所以下次到达地点0时,还会很大机会选择left行为到 地点4
		同样道理, Q(4,up),Q(2,right),Q(6,right),Q(8,right) 都几乎不变化
		所以 0:left -> 4:up -> 2:right -> 6:right -> 8:up -> 9终点 
		就是一条当前最为正确 操作流
	学习原理的假设并没有问题,但其实之所以 有 r + gama * Q(s_,max_a_) - Q(s,a) 
		这种算法方式 是基于基本强化学习模型 MDP 模型的的数学方程 简化后 
		才有的这种乍一看超级迷离,并不知道如何解释的这个算式
	Q-learning 也称 off-policy, 是因为选取 s_ 参考权值行为只选取最大权值的,
		因为只去看 s_ 哪个行为权值最大 ,就选他 (max_a_) 
		意味着,下一步最有可能执行这个行为
		下一步执行这个最有可能的行为 max_a_ 后,
		并无法知道 会在下下步到达 的地方 S__ 会不会是陷阱或禁止区域什么的
		这样子,前进就比较冒进,勇敢, 被认为是缺乏策略性的学习行为
		
学习分析 Sarsa 算法!!
	同样以地图游戏,寻找出口例子分析:
		先建立 一张 q 表, 记录每一位置点 s 的 所有行为 a 的权值 例如:
		       left     right        up      down
		0  0.000000  0.004320  0.000000  0.004320
		1  0.000000  0.025005  0.000400  0.004320
		2  0.000030  0.111241  0.000002  0.004320
		3  0.000000  0.368750  0.000000  0.004320
		4  0.027621  0.745813  0.000099  0.004320
		5  0.000000  0.000000  0.000000  0.004320
		Q(s,a)函数是得出 s 点,a 行为的 的 权值,
		例如 Q(4,right) = 0.745813, right:0.745813比其他行为的权值高,
		所以 在 4 点 采取 right行为 是最为推荐的选择.
		每个学习回合(episode)初,先确定好 q 表, 
		如果是开始回合,就初始化q表,否则沿用上回合最终更新的q表
	每个学习回合开的每一步工作内容如下:
		当前点 s , 有一个确认的行为 a
			如果不是执行在第一个回合 当前点s 是上一步的 s_, 
			行为 a 是上一步 s_ 在上一步通过 行为选取算法 得到的 a_
			如果是执行在第一个回合 当前点 s , 通过行为选取算法得到行为 a 
			行为选取算法典型有以下操作:
			有 e_greedy 概率通过q表权值操作来选取 行为 a_, 一般选最大权值的行为
			也有 1-e_greedy 概率 随机选取行为 a_!!
		执行操作 a , 根据操作会到达下个点 s_, 并根据奖励算法得到行为应有的奖励 r
		Q-learning 和 sarsa 算法 区别在于 参考权值 选取方式不一样
		再一次通过 行为选取算法 给S_ 选取 一个行为 a_
		取得 Q(s_,a_) 为我们的参考权值
		还有的是,下一步的这个行为a_,在下一步 s_ 点 一定会执行的!! 
		执行学习更新 当前点s,当前行为a的 权值 Q(s,a):
			Q(s,a) = Q(s,a) + Alpha * [ r + gama * Q(s_,a_) - Q(s,a) ]
			Alpha是学习率
			gama 是 discount factor
	重复一步又一步,直到第某步,执行操作 a 到达的下一点 s_ 是目标点,或者是陷阱点
		时,才正式结束结束这一回合.准备开始下一回合.
		然而一般可以很好地避开陷阱点.				
	这里 奖励算法和行为选取算法都是自定义的,需要根据实际情况谨慎设计
	Sarsa 也称 on-policy, 因为是通过 行为选取算法 选取 s_ 的 下一步行为 a_作为参考权值行为
		下一步一定会执行这个行为
		下一步一定会执行的 a_ 是通过 行为选取算法选出的,
		必须强调的是,s与s_使用相同,行为选取算法,而这个算法的设计还是有一定讲究的
		开始好几个回合, s 选取的 a 或者是 s_ 选取的 a_ 都有可能使下一步掉入陷阱区,
		多次回合后,q表有初步的更新,然后行为选取算法选出的行为 一般都是能避开陷阱区的行为
		所以,多个回合后,后面学习 执行的行为 一般都可以很好避开陷阱,比较谨慎的前进.
		就好象会察觉到危险而不会贸然跌入陷阱那样有策略地前进.

学习分析 Sarsa(lamda) 算法!!
	同样以地图游戏,寻找出口例子分析:
		先建立 一张 q 表, 记录每一位置点 s 的 所有行为 a 的权值 例如:
		       left     right        up      down
		0  0.000000  0.004320  0.000000  0.004320
		1  0.000000  0.025005  0.000400  0.004320
		2  0.000030  0.111241  0.000002  0.004320
		3  0.000000  0.368750  0.000000  0.004320
		4  0.027621  0.745813  0.000099  0.004320
		5  0.000000  0.000000  0.000000  0.004320
		Q(s,a)函数是得出 s 点,a 行为的 的 权值,
		例如 Q(4,right) = 0.745813, right:0.745813比其他行为的权值高,
		所以 在 4 点 采取 right行为 是最为推荐的选择.
		每个学习回合(episode)初,先确定好 q 表, 
		如果是开始回合,就初始化q表,否则沿用上回合最终更新的q表
		此外,一张 e 表,
		e表结构与q表一致,但是e表的成员意义 不是 行为权值, 而是行为相关度 例如:
		       left     right        up      down
		0         5         1         0         0
  		1         0         7         1         3
		2         2         4         1         3
		3        10         0         0         1   
		4         0         0         0         0 
		5         3         6         1         9
		回合的每一步都会根据当前步更新了的 e 表, 整体更新 q 表所有行为权值		
		假设此刻为回合的最后一步,已经到达终点,
			此刻的 e 表 记录的相关度可以这么说说明一个事实:
			要到达终点,在地点0 left行为比较相关,在地点0,更应该执行 left 行为,同理
				在地点1,更应该执行 right 行为
				在地点2,更应该执行 right 行为
				在地点3,更应该执行  left 行为
				在地点5,更应该执行  down 行为
				在地点4,是终点.
			因此这一步,更新 q 表权值时,
			更新 0:left, 1:right, 2:right, 3:left, 5:down 位置的权值的幅度比较大
			这些位置的 权值有比较显著的提高.
		假设此刻为并非回合的最后一步.
			此刻的 e 表 记录的相关度可以这么说说明一个事实:
			如果要到达像这一刻所处的位置,或者状态:
				在地点0,更应该执行 left 行为
				在地点1,更应该执行 right 行为
				在地点2,更应该执行 right 行为
				在地点3,更应该执行  left 行为
				在地点5,更应该执行  down 行为
				由于还没有到达过地点4,
				所以地点4,并未采取过行为,
				所以并没有行为相关度统计
				所以并不知道地点4更应该执行啥,
				也不知道 地点4 是终点还是其他
			这一步,更新 q 表权值时,同样地,
			更新 0:left, 1:right, 2:right, 3:left, 5:down 位置的权值的幅度比较大
			这些位置的 权值有比较显著的提高.
			并不更新 地点4 的权值
		e表 相关度 的统计方式可以有很多种,上述表是每当执行一次对应行为,就在e表对应项加1
		还有其他的统计方式,比如也有像以下一样的 e表
		       left     right        up      down
		0       0.5      0.91       0.1      0.22
  		1       0.3      0,43      0.87     0.334
		2         0         0      0.13      0.34
		3       0.1     0.465     0,112       0.3  
		4     0.445         0     0.345       0.9
		5      0.13      0.56      0.22         0		
		每个回合开始前, e 表所有项 都先被赋0										 
	每个学习回合开的每一步工作内容如下:
		当前点 s , 有一个确认的行为 a
			如果不是执行在第一个回合 当前点s 是上一步的 s_, 
			行为 a 是上一步 s_ 在上一步通过 行为选取算法 得到的 a_
			如果是执行在第一个回合 当前点 s , 通过行为选取算法得到行为 a 
			行为选取算法典型有以下操作:
			有 e_greedy 概率通过q表权值操作来选取 行为 a_, 一般选最大权值的行为
			也有 1-e_greedy 概率 随机选取行为 a_!!
		执行操作 a , 根据操作会到达下个点 s_, 并根据奖励算法得到行为应有的奖励 r
		类似 sarsa 算法,再一次通过 行为选取算法 给S_ 选取 一个行为 a_
		取得 Q(s_,a_) 为我们的参考权值
		然后得到误差值 error = [ r + gama * Q(s_,a_) - Q(s,a) ]
			                gama 是 discount factor
		注意,跟sarsa差不多,下一步的这个行为a_,在下一步 s_ 点 一定会执行的!! 
		接着讨论 e 表, E(s,a) 表示 e表, 地点s,行为a 的相关值,比如当前e表为
			       left     right        up      down
			0       0.5      0.91       0.1      0.22
	  		1       0.3      0,43      0.87     0.334
			2         0         0      0.13      0.34
			3       0.1     0.165     0,112       0.3  
			4     0.445         0     0.345       0.9
			5      0.13      0.56      0.22         0
		通过相关值统计算法 更新 e表中 地点s,行为a 的相关值, 
		比如加 0.3 算法, 比如 s:3, a:right
			E(s,a) = E(s,a) + 0.3
		即 e 表变为:
		接着讨论 e 表, E(s,a) 表示 e表, 地点s,行为a 的相关值,比如当前e表为
			       left     right        up      down
			0       0.5      0.91       0.1      0.22
	  		1       0.3      0,43      0.87     0.334
			2         0         0      0.13      0.34
			3       0.1     0.465     0,112       0.3  
			4     0.445         0     0.345       0.9
			5      0.13      0.56      0.22         0
		然后 就 通过 e 表整体更新 q表: 譬如当前 e 表 q 表为:
			       left     right        up      down
			0  0.000000  0.004320  0.000000  0.004320
			1  0.000000  0.025005  0.000400  0.004320
			2  0.000030  0.111241  0.000002  0.004320
			3  0.000000  0.368750  0.000000  0.004320
			4  0.027621  0.745813  0.000099  0.004320
			5  0.000000  0.000000  0.000000  0.004320	
		更新后的 q 表: (注意是按位算法,不是矩阵算法)
	    left     right        up      down                     left  right    up  down
	|0.000000  0.004320  0.000000  0.004320|                 |  0.5   0.91   0.1  0.22|
	|0.000000  0.025005  0.000400  0.004320|                 |  0.3   0,43  0.87 0.334|
	|0.000030  0.111241  0.000002  0.004320|                 |    0      0  0.13  0.34|
	|0.000000  0.368750  0.000000  0.004320| + Alpha * error |  0.1  0.465 0,112   0.3|
	|0.027621  0.745813  0.000099  0.004320|                 |0.445      0 0.345   0.9|
	|0.000000  0.000000  0.000000  0.004320|                 | 0.13   0.56  0.22     0|
						   Alpha是学习率
		然后整体更新 e 表:
                                  left  right    up  down
                                |  0.5   0.91   0.1  0.22|
                                |  0.3   0,43  0.87 0.334|
                 gama * lamda * |    0      0  0.13  0.34|
                                |  0.1  0.465 0,112   0.3|
                                |0.445      0 0.345   0.9|
                                | 0.13   0.56  0.22     0|
		gama 还是上述的那个 gama
		lamda 就是 Sarsa(lamda) 提到的 lamda
	到这里,这一步的内容就算是完结了,下一步同样重复这样的内容!!
		直到第某步,执行操作 a 到达的下一点 s_ 是目标点,或者是陷阱点时,
		才正式结束结束这一回合.准备开始下一回合.
	这种算法如 sarsa 一般可以很好地避开陷阱点, 而且比 sarsa 的 学习速度更快.
	这种算法其实就是 sarsa 变种,下一步的这个行为a_,在下一步 s_ 点 一定会执行的!!	
	现在 讨论 lamda 的作用意义:
	如果 lamda 设定为0, 
		那么可以看到,每一步执行前, e 表所有项都是 0
		经过 E(s,a) = E(s,a) + 0.3 后,只有 地点s,行为a 那项目有相关值 E(s,a)
		然后更新 q 表的时候,其实就只是更新 地点s,行为a 的权值.
		本来其他操作都跟 sarsa 一样, 
		加上这里:每一步更新权值,只更新 地点s,行为a 的权值
		这样就跟 sarsa 一模一样了. 这样有单步更新的样子
	如果 lamda 设定为1, 	
		那么, 在这回合里, 每一步的统计行为 E(s,a) = E(s,a) + 0.3 都被记录
		如果这一步还没到达 终点,
			那么从回合开始到现在,所执行的所有行为都被记录
			其中那些多次被执行的行为, 相关值比较高!
			被认为是能到达当前地点所 相对必要执行的行为
			所以,这一步更新 q 表时,更要大幅度更新那些多次被执行的行为的权值.
		如果这一步到达 终点,
			那么从回合开始到回合结束,所执行的所有行为都被记录
			那些多次被执行的行为, 相关值比较高!
			被认为是能到达终点所 相对必要执行的行为
			所以,这一步更新 q 表时,更要大幅度更新那些多次被执行的行为的权值.
			这样就有 我们所认识 的回合更新 的样子
	如果 lamda 设定为 0~1 之间,
		那么, 在这回合里, 每一步的统计行为 E(s,a) = E(s,a) + 0.3 
		随着新一步的到来,被弱化一次.
		比如 A时刻,当前处于地点0,
			这一步执行了left,统计了一次行为 E(0,left) = E(0,left) + 0.3
		 	然后更新 q 表.
			然后更新 e 表,
			更新 e 表的时候,乘上了 lamda ,即整体弱化了 e 表 所相关度
		经过n步后,到达b时刻 ,处于 地点5,
			这段时间未回到 地点0 执行left,
			那么 e表 地点0 所有行为包括left 的相关度 就被弱化了n次.变得非常低
			那么 说明,要到达地点5, 与在地点0不管执行什么操作,并没有什么关系
		这样就有一个这样子的推论.
			回合开始,经过 m 步到达 终点,回合结束
			开始点的行为是什么,并不重要.
			但慢慢的,越靠近终点的 地点,可能两三步就到终点了,
			他们的 行为 的相关度 显得相当重要.
			就是说要到达终点,开始点的行为并不重要
			越靠近终点,的地点的行为,相关度受到重视,
			对应的 q 表权值的 更新幅度也就比较大
			就好像越靠近终点,就越能看到到终点的路一样
		这样看起来 有一种介于 单步更新 和 回合更新 之间的样子		   				
	奖励算法,行为选取算法,相关值统计算法 都是自定义的,需要根据实际情况谨慎设计
	Sarsa(lamda) 是变种 sarsa,所以也称 on-policy, 
		因为也是通过 行为选取算法 选取 s_ 的 下一步行为 a_作为参考权值行为
		下一步一定会执行这个行为
		下一步一定会执行的 a_ 是通过 行为选取算法选出的,
		再三强调,s与s_使用相同,行为选取算法,而这个算法的设计还是有一定讲究的
		开始好几个回合, s 选取的 a 或者是 s_ 选取的 a_ 都有可能使下一步掉入陷阱区,
		多次回合后,q表有初步的更新,然后行为选取算法选出的行为 一般都是能避开陷阱区的行为
		所以,多个回合后,后面学习 执行的行为 一般都可以很好避开陷阱,比较谨慎的前进.
		就好象会察觉到危险而不会贸然跌入陷阱那样有策略地前进.
		而且 e 表 和 lamda 的 补充 , 学习速度更快!!!
	关键小结:
		注意理解 lamda=0 相当于 sarsa算法 相当于但不更新
			lamda = 1 相当于 回合更新
			lamda = [0,1]之间时 相当于越靠近终点,越有把握选择正确步数	
		每一个新回合开始前,都必须 重置 相关度表E 全为0.
		每一步都 更新 q表和 E 表全部内容
		每一回合到达终点时, E表都记录了这个回合里 那些地点哪些行为执行得比较多
		表示这些 地点的行为相关度比较高,将更大幅度更新对应权值

奖励算法同样重要和讲究:
注意理解分析 到达重点才给奖励的行为, 这样靠近终点的地点的 的最大权值行为的权值比较高,
	靠近起点的最大权值行为的权值相对低些.
	即越靠经终点越有把握	
还有一个是 每一步都给同样奖励的行为,可能导致并无法学习
	每一步都给同等奖励0的行为,相当于每一步都没有给奖励


code.py 是一个基于 Q-learning强化学习的小例子
这个例子是 角色o 寻找最佳路径到达T
这个例子奖励方式是,如果S_到达目的点时,当前点s就得到奖励

///////////////////以下的奖励算法策略思维有所保留,似乎不适合讨论在这个例子上
然而,比较不好说服的是,只要向右走就给奖励!! 
	因为我们知道 位置T在最右边,直观知道一直往右走就是了,
	但这样就很难分辨是Q-learning算法的可靠性,
	意义上不是寻找最佳路径,而是推荐不断往右跑	
	这样就没有学习的意义了
推荐合理的给奖励方法之一是,距离近了一点就给奖励!!
推荐最合理的给给奖励方法之二:
	统计每一回合的移动数, 当前回合比前一回合移动步数少的时候,执行以下操作,
	地图上有6个位置, 对应Q表有6行
	统计每个位置当前回合,执行最多次数的操作,被选为 加奖励操作
	然后,下一回合,
	每个位置,当前操作是否加奖励,根据上回合的决定!!
////////////////////////////////////////
基于 Q-learning , sarsa , sarsa_lamda 的例子 后面再详细分析













20170425
deep q-learning network (DQN)
对比 q-learning ,DQN 是通过 深度网络,代替 q 表 的 Q-learning 强化学习方法 
初步了解:
	比如我们当前地点 s 通过网络, 我们得到 s 对应的各个行为 的 权值队列
	例如
			left     right        up      down
	s:3 ->NN -> 0.000000  0.368750  0.000000  0.004320
	这里的学习跟 q-learning 的学习 有点不一样:
	首先重申, q-learning 的学习原理:
	假如 模型充分学习, 游戏角色可以 通过 一条连贯的路径到达终点
		每个踩点之间有 基本的联系,
		表现为 s点最大权值行为,与下一个s_点的最大权值行为的权值差比较小
		即 Q(s_,max_a_)-Q(s,max_a) 比较小,
		即是说在 s很有机会采取max_a行为到达s_点后,
		在s_点又很有机会采取max_a_行为
		行为之间就像有连续性.
	我们知道, q-learning 的学习 是 即时更新 q 表 某个权值的
		Q(s,a) = Q(s,a) + Alpha * [ r + gama * Q(s_,max_a_) - Q(s,a) ]
	但 DQN 不是,他分开两个网络, A网络是 计算出 s->QA(s,a)
		B网络是 计算出 s_->QB(s_,max_a_)
		典型,每一回合开始前,AB网络相同
		1,行走多步,得到多个 s_->QB(s_,max_a_) s->QA(s_,max_a)
		2,然后随机抽取其中 batch 步,组成一包数据
		  统筹计算 loss = r + gama * QB(s_,max_a_) - QA(s,a) 
		  此刻, AB网络还是一样!!
		3,然后通过 minimize loss 更新 了A网络
		先不覆盖,这时 A网络与B网络已经不一样
		仍然重复 一次或多次 1,2,3,步,继续更新一次或多次 A网络后
		最后把A网络拷贝覆盖B网络!!
	这种做法,相当于这样更新 Q-learning 的q表!
		有两张 q 表, A表用来计算权值QA(s,a), B表用来计算权值QB(s_,max_a_)
		先执行多步,记录每一步的 QB(s_,max_a_) 和 QA(s,a) 记为 y?, z? ?表示步数记号
		但每一步并不即时更新 A表,
		即每一步不执行 QA(s,a) = QA(s,a) + Alpha * [ r + gama * QB(s_,max_a_) - QA(s,a) ]
		等到结束或者 走了n步后,
		才整体更新 A表,
		即执行 QA(s,a) = QA(s,a) + Alpha * [ r + gama * y1 - z1 ]
		       QA(s,a) = QA(s,a) + Alpha * [ r + gama * y2 - z2 ]
			....
		       QA(s,a) = QA(s,a) + Alpha * [ r + gama * yn - zn ]
		更新了A表, B 表仍然不变!! 
		A表依然用来计算算QA(s,a), B表依然用来计算QB(s_,max_a_)
		重复上述更新 A 表的 组合操作, 多次更新 A 表
		然后把 A 表 拷贝更新覆盖 B表
		这一轮的学习就算结束.
		下一轮的学习重复上述过程
初步了解 DOUBLE DQN	
	与 DQN 的区别是 r + gama * QB(s_,max_a_) 这一步,
	DQM是通过 B网络得到 QB(s_,max_a_)的
	DOUBLE DQN 是先 通过  当前 A 网络 得到 s_ 输出的 a_ 的权值
		其中 权值最大的 行为记为  angmax_a  
	然后, 替换成 r + gama * QB(s_,angmax_a)
初步了解 Prioritized Experience Replay DQN (优化记忆DQN)
	与 DQN 的区别是 "随机抽取其中 batch 步,组成一包数据" 这一步,
	PERDQN 并bu会随机抽选 而是有有选择针对的 的抽选
	一般  r + gama * QB(s_,max_a_) - QA(s,a) 我们称为 TD_error
	TD_error 越大, 表明这一步 误差 比较大,被认为应该是重点学习的对象
	所以选出多个数据组成 batch 数据包,会选上这一步!! 
初步了解 Dueling DQN (优化记忆DQN)
	这个比较不好理解,虽然内容简单
	与 DQN 的区别: 
	DQN是: s -> NN -> 每个行为的权值

                       /当前 s 得到的的一个状态值 V \
	DDQN是: s -> NN                             各个行为的权值
                       \  每个行为的相关数值 A     /
	为什么这么拆分还没有明白



详细分析 梯度下降的的应用和意义
本来梯度的概念用于函数 是描述函数的变化程度
	例如: y = x^2 + 5 =f(x) 那么: f(x) 的某x值的梯度为 ∂y/∂x
	所谓梯度下降就是 寻找函数的最凹值的位置 这里可直观看到  ∂y/∂x 等于0时对应的 x 使得 y 有最小值
	就是我们的寻找的最凹值
	虽然,我们直观地找到了最凹值,但是,由于计算机的限制,并不能这样直观工作
	所以我们的通过以下逼近的方式找到 x 
	x <- x + Δx = x + r * ∂y/∂x
	在计算机上 ∂y/∂x 表示为:
		∂y/∂x = ( f(x+Δ) - f(x) ) / Δ
	Δ是一个无穷小量,但计算机无法得到无穷小值,所以是个超级小值
	r是步长幅度,表示逐步逼近的步长大小
	通过不断加一个小步长Δx的方式找到合适的 x 逼近到最凹值的方法,也是面对复杂问题的最常规方法
	这种常规的方法就是我们所说的梯度下降法
接下来,我们用一个 符合2次曲线散点图的拟合例子来 讨论 梯度下降的实际应用
	有一堆样本,例如有 200 个 [x,y] 点
	我们不知道他合乎什么样的曲线规则,所以想法去拟合,
	如果我们要求高度拟合,即过拟合,即每个点都几乎在同一根曲线上,就需要匹配高次函数模型
		例如: A*x^6 + B*x^5 + C*x^4 + D*x^3 + E*x^2 + F*x + G = y = f(x)
	如果我们要求适当拟合, 匹配3次或者2次函数模型即可
		例如: D*x^3 + E*x^2 + F*x + G = y = f(x)
	这里我们匹配一个 3次函数模型.
	如果一个样本很好符合曲线,表示样本与曲线的误差error比较小,
		为了方便计算,我们要求误差大于0,
		即: error(x,y) = (f(x)-y)^2.
	如果一条曲线很好地符合所有样本,那么就是 200 个样本 每一个样本的误差加起来最小
		即: ∑ (error(x,y)) 最小
		也可以这样表示  ( ∑ (error(x,y)) ) / 200 最小
		我们记为 loss = ( ∑ (error(x,y)) ) / 200
	最小化loss,想相当于loss函数里,200个样本参数值作为已知常数,
		求未知数 D,E,F,G,的合理值,使得 loss最小
		即: loss = J(D,E,F,G).
	那么就是对 D,E,F,G 求偏导,并逐步逼近,即执行梯度下降
		D <- D + ΔD = D + r * ∂j/∂D,    ∂j/∂D = ( j(D+Δ) - j(D) ) / Δ
		E <- E + ΔE = E + r * ∂j/∂E,    ∂j/∂E = ( j(E+Δ) - j(E) ) / Δ
		F <- F + ΔF = F + r * ∂j/∂F,    ∂j/∂F = ( j(F+Δ) - j(F) ) / Δ
		G <- G + ΔG = G + r * ∂j/∂G,    ∂j/∂G = ( j(G+Δ) - j(G) ) / Δ
	重复多次,200个样品并当作常数代入loss执行梯度下降工作
	直到loss收敛到最小
	一般loss最小时,∂j/∂D,∂j/∂E,∂j/∂F,∂j/∂G 都是几乎接近0
		但是 ∂j/∂D,∂j/∂E,∂j/∂F,∂j/∂G 都几乎接近0的点可能有很多个
		所以我们得到的loss最小值可能是局部最小值不是全局的!!
		即有可能有很多凹点,得到的最凹点不一定是最好的最凹点
		一般来说,局部最凹点已经可以认为是最符合的结果了,
		如果结果不理想,要得到更好的或全局最凹点,就要做更多深入的工作!!
	至此得到的 D,E,F,G 组成的 D*x^3 + E*x^2 + F*x + G = y 就是我们拟合的最合理曲线
现在,假如我们有 10000000个样本, 把这么多个样本放入loss函数并不合理!!
	这时,我们可以随机选200个样品代入loss作为常数,对变量 D,E,F,G执行一次梯度下降
	重复多次,每次选200个样品并当作常数代入loss执行梯度下降工作
	这同样可以 逐步逼近 loss 的 局部最凹点,得到我们想要的结果
	这里 我们有一个 batch 的思维.
	代入loss的样本越少,拟合的曲线越有局部行为的匹配性, 
	但是每次都代入不同样品,局部匹配性就会打破,慢慢向全局匹配靠拢
	就是所,每次梯度下降操作仅针对当前填入的样品
	每一次都添加不同样品 每次梯度下降的方向都有不同程度的跑偏
	也就是说:
	每次都代入10000000个样本参数作为常数的loss执行梯度下降工作, 最后loss 得到的局部最后凹值
	和
	每次都代入随机200个样本为一batch的参数作为常数的loss执行梯度下降工作, 最后loss 收敛到的最小值 
	几乎是一样的
	这就是我们为什么 在庞大的样品库,可以 随机抽取多个组成一batch样品数据代入loss 执行梯度下降
最后就是我关于loss函数 思维的改变:
	以前未好好分析 梯度下降时,学习tensorflow 
	很多例子喂数据都是 一batch一batch的喂,而当时我却不解,
	我觉得应该是一个个样平地喂
	所以当后面看看到 loss 都是 loss = (∑ (error())) / batch,
		我就错认为等同于 loss = error( ∑(每个样品)/batch )
	这种错误的认识在于先  ∑(每个样品)/batch ,再 error() 
		会忽略了很多样品细节,破坏样品信息的完整性
		然后得到的 error 是比较有缺陷的,
		最后甚至不能让loss收敛
	事实上,每次只喂一个样平,理论上同样可行,上述虽然相当于每次只喂一个样品
		但他喂的是平均样品,会导致收敛速度几何级变慢
		同样地,即使每次喂的一个样品不是平均样品.也不建议,因为运算量大,收敛速度慢!!
	为什么喂batch数据 (∑ (error()))后会 / batch?
		只不过是让最后的loss值看起来不要太大.方便处理结果
		可以不需要的!!
		特别注意很多AI算法原理图都没有 / batch 这一步
机器学习的梯度收敛行为
	已知 x,y 符合一个模型A,而 x经过现有模型得到 y_ 与 y 有误差 loss
	将误差降到最低时,现有模型就几乎等于模型A了
	求loss 对现有模型参数 Θ 梯度,执行梯度下降
	取得最最优参数 Θ 使 loss 收敛到最小时 现有模型就几乎等于模型A
强化学习的梯度收敛行为
	以policy gradient 为例子
	机器最后学习到一种行为时,其行为策略会使得当前一套行为流程再整个episode取得 最大的价值 J(Θ)
	Θ是策略的参数
	自然这种情况下 J(Θ) 对 Θ 的梯度也是相当接近 0 的 !!
	因为如果梯度特别大,那证明同样的行为, J(Θ) 可以变得更大,
	Θ还有有变化的空间,那么表示策略还有改动的可能.即 行为的学习还有极大的空间











20180430
policy Gradients 与 Q-learning大家族不一样的另一个 RL家族
Policy Gradient 本质是以时间点为单元 当前状态 s 输入 策略网络,得到 的动作输出!!
初步了解
这里 s -> NN -> a
策略 πθ(s,a) 这里是 s通过网络选出的a,与实际选择的a的 交叉熵
注意,我们的目标要认清 policy gradient 的算法原理, 认清例子具体每一步对应原理的那个部分
目前观察看,例子好像缺了好多应有的部分
参考
https://blog.csdn.net/amds123/article/details/70242042

突然间忘记了梯度训练的相关概念:
理解 马尔可夫决策链（Markov Decision Process）
https://blog.csdn.net/bravacristina/article/details/78540779
http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching_files/MDP.pdf
RL系统课程
http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching_files/pg.pdf
http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html

https://blog.csdn.net/amds123/article/details/70242042
Gt = R.(t+1) + γ*R.(t+2) +...+ γ^(T−1)*R.(T)	//这是啥?? 这个是马尔可夫决策过程MDP的内容


https://www.cnblogs.com/mo-wang/p/4910855.html
另一家族无模型预测（Monte-Carlo Learning与Temporal-Difference Learning）


MDP 动态规划法(dynamic programming methods)
MDP 策略迭代方法是 每完成 一个 episode 才更新所有 的V(s) Q()等
    值迭代应该是 没走一步就更新一次当前V(s),并不等待完成了一个episode (后来认识到这个观点不属于值迭代)
    值迭代也有可能是完成 一个 episode 才更新所有 的V(s) 当更新当前的V(s) 是 跟上一个 episode的 V(S_)有关 
策略迭代:
从上面我们可以看到，策略迭代算法包含了一个策略估计的过程，而策略估计则需要扫描(sweep)所有的状态若干次，其中巨大的计算量直接影响了策略迭代算法的效率。我们必须要获得精确的Vπ值吗？事实上不必，有几种方法可以在保证算法收敛的情况下，缩短策略估计的过程。
这时由于 一个episode 到达 同一个 s 多次, 每一次都是一个新V(S),直到 完成这次迭代完成 V(S)才确定下来,所以过程中需要用到的V(S_)也在动态变化着
所以需要 (sweep)所有的状态若干次
值迭代:
它的每次迭代只扫描(sweep)了每个状态一次。
假如是 完成 一个 episode 才更新所有 的V(s) 因为 更新当前的V(s) 是 跟上一个 episode的 V(S_)有关 
所以再 更新所有 的V(s) 只 (sweep)一次所有的状态,就可以,因为 上一个 episode的 V(S_) 是已经确认下来的,并不会变化
动态规划的优点在于它有很好的数学上的解释，但是动态要求一个完全已知的环境模型，
这在现实中是很难做到的。另外，当状态数量较大的时候，动态规划法的效率也将是一个问题。
所以动态规划不需要实际上每个episode 都去执行实际的一连串行动 a,即不需要episode中执行过动作, 
每个episode 只有直接的迭代操作 ,即episode次数就是迭代次数, 还有用 k 表示 episode次数
只要知道所有的状态项位置,还有相关奖励机制,就开始迭代求出最优事件流!!
根本不需要 有agent 去执行一连串的实际操作 a 





MC  后面发现是我们 Qlearning 和sarar 的基础
注意 假如在一个不变的策略π 下,经过无限次episode 会收敛到这个π 下对应的行为
特别注意到,每个episode 的 每个 s 的 G 值都被记录!!都参与到 每次 得 V(s) 的计算中
如果策略π 也跟均实时的Q 在不断变化,经过无限次episode 就可能会收敛到这个系统的最好策略π,和最好的行为????
注意 http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching_files/MC-TD.pdf
     http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching_files/control.pdf
     ppt里的
	S1,A1,R2,...,Sk∼π
	{S1,A1,R2,...,ST}∼π
	都表示是 一个 episode 经过k(或者T)个timestep后 结束所记录下的参数
	例如 S1 表示 timestep 1 所处于的 状态s
	     A1 表示 timestep 1 所处于的 状态s下 采取的行动 a
	     R2 表示 timestep 1 所处于的 状态s下 采取了行动 a 后,将到达timestep 2 所处的状态s 而得到的奖励
	     Sk, ST 表示 timestep k (或者T) 所处于的 状态s 到达这个s时,episode已经结束,
	     所以不会有 Ak AT R(k+1) R(T+1)这些参数
注意 http://www.cnblogs.com/jinxulin/p/3560737.html里的
	的Vπ(s)≈ (2 + 1 – 5 + 4)/4 = 0.5 的 R1(s),R2(s),R3(s),,,
	对应着 http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching_files/MC-TD.pdf
     	       http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching_files/control.pdf
               ppt里的 的 Gt
	但是表达意义不一样
	这里 R2(s) 表示 第某个episode里,在 timestep t 到达状态s时 得到的 Rt+1+γRt+2+...+γT−1RT 
		而这次到达的 S 在所有episode 和所有timestep 统计中,是第2次到达
	而Gt仅仅就是在一个epispde中,在 timestep t 到达状态s时 得到的 Rt+1+γRt+2+...+γT−1RT 
	Rt+1+γRt+2+...+γT−1RT 表示 把 这个 episode 的 timestep t 后 所以 奖励R? 按加权平均得到的 价值,
		就是 现在所处 s 在这个 episode 的 timestep t 得到的应有价值
	特别注意 Gt 用于累计统计莫个特定 s 下的 V(s)
	所以 每个 Gt 都只为自己对应的那 个 s 服务, 不管是在哪个episode哪个timestep 到达这个 s
	只会被 这个 s 计算 V(s) 所用, 不会被其他 s 共用!!
	∑ (π Q(s,a)) = Vπ(s)??
	


policy gradient
πΘ(s,a) 是策略, 是关于参数 Θ 的 我们要求得到最好的 策略
可以通过 组建辅助观察函数 J(Θ) = ∑d(s)∑πθ(s,a)Rs,a
			        s    a
最大化(最小化??) J(Θ) 意味着 求得最好的 πΘ(s,a)

∑πθ(s,a)Rs,a 表示在当前s 每个行为a 通过策略 πθ(s,a) 后,得到的 奖励的统计值 
a
d(s)表示整个episode 能够在当前s 行使策略行为的概率,
∑d(s)(...) 表示把整个episode 每个 s 通过策略 πθ(s,a) 得到的 奖励的统计值 
s	即整个episode 在策略 πθ(s,a) 得到的统计奖励值!!
J(Θ) 应该是越大越好??


Nash equilibria 纳什均衡








20180528
经过漫长学习分析 终于切入了 policy gradient的思维
详见 policy gradient 代码, RL 是 policy gradient 算法
CartPole 和 MoutainCar 是两个例子
算法要点
根据算法定义:
function REINFORCE
    Initialise θ arbitrarily
    for each episode { s1,a1,r2,...,sT−1,aT−1,rT }∼πθ do
        for t = 1 to T−1 do
            θ ← θ + α * ∇θ ( logπθ(st,at)vt )
        end for
    end for
    returnθ
end function
	
这个例子的 policy gradient 算法 有几个重点
1,行为策略 a = πθ(s)
  在这里 在s状态 通过策略 πθ(s) 得到动作 a, 
  相应地 πθ(s,a) 的得到是 在s状态,获得动作a 对应的策略,表现为在s状态,获得动作几率
2,奖励 vt / v(t) / Gt / G(t) / R(sa) (有好多称谓),但都是同个概念,都是马尔科夫模型的一个概念之一
  当前 s 执行了a 得到 的奖励  v(t) = r + v(t+1)
  r是直接奖励,  v(t+1)是 下个 s 执行了a 得到 的奖励
  这个奖励模型拥有前瞻性意义!!,表示了当前s 执行a后得到的奖励 拥有了多少 导致episode成功结束 贡献价值
  即隐约告诉我们 是否更靠近成功的结束
3 根据 算法定义 我们的  loss 可以设置是 j(θ) =E[ ∑ -logπθ(st,at)vt ]
					    t
  logπθ(st,at) 与 πθ(s,a) 意义一样, 即在s状态,获得动作a 对应的策略,表现为在s状态,获得动作几率
  使用 logπθ(st,at) 是为了跟好计算
  假若 完全锻炼的 策略, loss 很小, 意味这 任一 timestep (即t) 都 选择了相当正确的行为a
   1, 大多数情况, 
      比如 t = 10, 执行了 a = πθ(s) 的 a , 得到很高的 vt (更近成功终点), πθ(s,a)几乎等于1
      然后 -logπθ(st,at) ≈ 0 所以 -logπθ(s,a)vt ≈ 0,表示对 loss 几乎没有贡献
      表示这一步使用 a 无比正确的策略,不用修正策略
   2, 几乎 有 1 - e_greedy 甚至更小 概率出现的情况 
      比如 t = 78, 执行了 a = πθ(s) 以外的 a1,但得到 很小的 Vt (更远离成功终点), πθ(s,a)比如等于0.1
      然后 -logπθ(st,at) ≈ 1 所以 -logπθ(s,a)vt ≈ vt, 表示 表示对 loss 几乎没有贡献
      表示这一步使用 a1 是错误的,所以策略还是正确的,不用修正策略 πθ(s),或者修正很小内容	
   2, 极少数情况, 
      比如 t = 49, 执行了 a = πθ(s) 以外的 a1 , 也得到很高的 vt (更近成功终点), πθ(s,a)等于0.3
      然后 logπθ(st,at) ≈ -0.5,即 -logπθ(st,at) ≈ 0.5 所以 -logπθ(s,a)vt >0,(例如 = 10)
      表示 这一步虽然使用了策略外的一步,但是得到很大的vt(更近成功终点),
      表示在这个 s 执行的 a = πθ(s) 的策略值得怀疑, 应该有所改变,应该更偏向 a1
4 实际情况 我们 没有使用 logπθ(s,a) 这个模块, 而是使用 all_act 和 self.tf_acts 的交叉熵来代替
  logπθ(s,a) 与 交叉熵 虽然算法不一样,但意义是一样的:
      当执行的行为是远离策略时, 值比较大, 
      当执行的行为等于策略导出的,值=0
  之所以这么替代,是因为 不知当如何构造 logπθ(s,a),相反,建立交叉熵比较简单
  all_act 是每个时刻t,策略导出 a = πθ(s) 组成的 数组
  self.tf_acts 是每个时刻t,真正执行的 a 组成的 数组
  计算 每个 t 得到 交叉熵,就等于比对那刻 执行的行为 是否等于 策略导出的,
  是则等于0,非则是一个比较大的值
  所以可完全取代了 logπθ(s,a)
5 然后这里学习 的策略参数θ 对应的 其实就是 策略网络里的 fc1 fc2 里所有的 W b
  而 这里的 self.tf_vt 是 每一刻t 执行的实际的 a 得到 奖励, 
  而不是 策略算出的行为a 对应的奖励
模型学习原理:(大多数梯度下降训练模型的原理,大抵是这种思维)
	完成了一个 episode A 后 我们得到了所有这个 episode 操作过的 a 和对应的奖励 vt
	代入 loss 得到loss数值 AA 
	把 这个 episode A 操作过的 a 和对应的奖励 vt 看成常数, 这时loss是一个 关于 w , b 的函数
	通过对 w,b 求梯度,我们更新了 w,b 
	更新的 w,b 只是让 当前 episode A 操作过的 a 和对应的奖励 vt 
	代入 loss后获得 更低的loss数值  AAA
	下一个 episode B操作过的 a 和对应的奖励 vt ,代入上次更新了w,b的loss  
	得到的 loss数值 BB, BB 不一定比上个 AAA 小. BB 和 AAA 没有什么表面上的直接联系
	根据 episode B 梯度执行第二次更新loss的 w,b 也是只让episode B 再第二次更新loss 得到更低的 BBB
	同样 下个 episode C 代入第二次更新后的loss 得到的 CC 与 BBB没有直接 关系
	CC 有可能 比 BBB 大 
	如此类推
	但是经过足够多的 episode 如此代入迭代后,比如经过100000次后,
	然后更后面的 episode 代入 loss 都会得到一个 比较相似,和 很低的 loss值
	这时我们就认为这个模型已经训练好.
	这是为什么呢? 收敛解释:
		因为假如每个episode 都是执行同样的行为结束,那么最终policy会收敛到这套动作上来,
		表现为 每次策略推导的行为就是实际执行的行为.所以每刻 t 的行为的交叉熵都为 0
		这种情况根本就可以无视奖励.因为不管奖励多少, loss最终都为 0
		但是我们设置有随机 情况 出现 策略推导的行为不是是实际执行的行为 的情况
		这时一些步得到的奖励也跟以前一直得到的不一样.
		如果 交叉熵和奖励都不同时为0,
		然后loss就不为0了, loss对w,b做梯度下降操作的意义在于:
		模型总是认为当前episode的所有实际行为,在以后的episode都会一直这么做.
		这也就是 loss 对 w,b做梯度下降时, 把所有的 s,a,vt 当作常数的行为
		做梯度下降就是 要把 这个实际行为变成是 策略可以推导出的行为 
		因为这样 所有行为的交叉熵会 变回0,loss会收敛会回到 0
		所以实际上 因为 实际行动常常与策略推导的不一样,所以策略一直在变
		直到一直到达一个死胡同, 虽然也执行了与策略推导不一样的行为,但是这时的奖励为0,
		所以 loss 还是 0, 所以loss并没有做对 w,b 的梯度下降操作,所以策略还是没有变
		这样子, 就相当于 这个模型的每次 episode 的整套动作大抵链被控制在一套方案里
		因为我们很小的随机机会作出与策略不一样的实际动作,但是这些意外的动作没有奖励
		所以那次 episode 下来 loss没变 ,策略也就没变
	但是,实际上loss 永远不为0, 交叉熵也永远不会出现0,而会接近0
		真实的行为是,最后收敛后的状态是
		在 s 选了非策略的行为a后得到很大的交叉熵, 得到一个很小的 vt  
		但是 给loss只带来了一点改变		
		loss 对 w,b 的梯度下降,把策略改偏了一点
		后面一轮 episode
		在 s 选的都是 策略导出行为为实际行为, 得到几乎接近0的交叉熵,得到比较大vt
		但是 给loss带来了 比之前非策略实际行为 还要大一点改变
		loss 对 w,b 的梯度下降,把策略偏回去了
		后面几轮 episode
		在 s 选的都是 策略导出行为为实际行为,
		因为我们只给了小几率执行非策略行为为实际行为,
		所以这时在 s 一般执行的都是策略导出行为
		这样对 loss就没有任何贡献了 ,loss也不会对 w,b做什么改变
		意味着策略并不会有什么改变
		这要反反复复,策略也没有什么实质大变化,这样模型实际上就是收敛了!
		这个模型的每次 episode 的整套动作大抵链被控制在一套方案里
		这套方案或许有几个分支可以明显分辨到的分支路线
		但其实他们都符合同一个策略,因为我们给出小几率执行非策略行为实际行为所导致
		一般分支点开始的地方都是 策略能导出多个行为的地方,
		策略之所以能导出多个行为,是因为这些行为得到的奖励大抵相同
		上一次的 episode 获得的 loss的细微改变,
		总会影响到当前 episode 在这个分支地方的策略导出
		影响策略究竟挑选哪个行为作为当前episode最优的分支行为
总结对比 归纳 两种 意义 完全不一样,但也是通过梯度收敛的的模型 以点线拟合行为例子
第一种, 点是按照固定规则 分布的. 点绝大多数分布都符合固定规则内
	比如说 点是按照某双曲线 随机分布的, 每次我们把若干散点点当常数代入拟合式子的loss式子
	然后loss式子对拟合参数作梯度下降操作,拟合式子会更像这个双曲线一些
	经过若干轮梯度下降操作,每次拟合式子都会更像这个双曲线一些
	很快现出了双曲线的影子
	这是我们见过的基本的 拟合散点曲线的例子
第二种, 点是按照动态规则 分布的,未到最后,还真不知道会拟合出上什么曲线出来
	比如说,最后得到的是双曲线的这个例子
	一开始,根据我们知道了一些隐晦的规则,也根据拟合式子的一些反馈,我们得到了一张随机的散点图,
	这张散点图 看着就像是拟合到一个椭圆似的.
	这张散点图分布并没有违反规则,
	因为不像第一种
	因为没有固定规则散点图分布得有双全线的样子,所以可以是其他样子
	然后散点图作常数代入拟合式子的loss式子 作梯度下降操作后,
	拟合式子会更像是 椭圆式子一些.
	然后根据继续 根据隐晦的规则,和拟合式子的反馈,我们得到了一张新的随机的散点图
	这张散点图 看着就像是一个三角形.	
	然后散点图作常数代入拟合式子的loss式子 作梯度下降操作后,
	拟合式子却改变方向,会更像 三角形式子一些
	若干次后, 后面出来的散点图 最后一般都会 像是双曲线的样子 而不是其他样子
	最后得到的 拟合式子也越来越像双曲线例子了
	那么是如何做到 散点图 从开始 椭圆形慢慢收敛到 最后的双曲线形?
	这就是神奇需要解释的地方!
		这可以理解为拟合式子和隐晦规则共同约束的结果
		比如说拟合式子此刻就是椭圆式子的样子.但是隐晦规则的存在,
		使得出现的散点图不局限维是 椭圆的样子,
		关键是,有机会是其他样子
		一旦是其他样子,拟合式子经梯度下降后就会变形
		当拟合式子变形成 双曲线式子,反而因为 隐晦规则的存在,
		使得出现的散点图仅仅局限在 双曲线.,不太可能是其他例子
		所以最终的散点图只会是双曲线的样子,而拟合式子也收敛到一个双曲线式子里	
而policy Gadient 一般就像是 第二种 收敛模型
机器学习, 有的符合第一种,有的符合第二种
强化学习,估计一般都符合第二种.
我们分析发现, 
policy Gradient 是 回合更性的(episode完后才跟新策略)
Q-learning,sara是单步更新, 每episode中每一步执行后都马上更新








20180527
AC actor-critic
从 policy gradient 说起,
从 policy-gradient 的例子看来 是回合更新的:
	loss: j(θ) =E[ ∑ -logπθ(st,at)vt ]
	               t
	把episode所有t 对应 的 st,at和vt 代入 loss 然后做梯度下降更新 policy πθ(s)
但是 policy gradient 的算法定义中看: 并没有说明 policy gradient 就一定是 回合更新
        for t = 1 to T−1 do
            θ ← θ + α * ∇θ ( logπθ(st,at)vt )
	看起来反而 更趋向建议单步更新,
	因为 ∇θ ( logπθ(st,at)vt ) 是单步对 policy πθ(s) 更新的操作
	就是说 loss更应该是这样的 j(θ) = -logπθ(st,at)vt , 
	每执行一步就 得到 一个 -logπθ(st,at)vt ,
	同样地, 如果被行使的这一步是策略导出的,-logπθ(st,at)就几乎等于0
	如果,被行使的这一步不是策略导出的,-logπθ(st,at)就相当大,
	而且vt也不等于0时,那么 -logπθ(st,at)vt就相当大
	算法就会 执行显著的策略更新了
	即 随即把这步的s,a,v当作常数代入 j(θ) = -logπθ(st,at)vt
	然后对 w,b 做梯度下降操作,更新policy πθ(s)
	经过足够多的迭代更新,我们同样可以得到最终的收敛结果
然而,vt是关键,由于一般 policy gradient 例子的 vt 都只能在一个episode结束后才能定下来,
	所以 policy gradient 就不得不在一个episode结束后才能执行更新
	对应的 loss变成 j(θ) = ∑ -logπθ(st,at)vt
			      t
	当然这样子也没有区别反而更利于计算:loss: j(θ) =E[ ∑ -logπθ(st,at)vt ]
						        t
为了得到更有效率的快速的收敛,我们希望能够做到 policy gradient 能 单步更新
	所以提出 是否有一个其他概念模型 能够代替 vt 可以使得  policy gradient 实现真正的单步更新之余
	也 与 vt 有差不多的 功能意义
	我们引入了 Q-learning 的用到的 td_error 代替,但不说明 我们引入了Q-learning的内容!!
首先我们回顾 Q-learning 的内容!!
	1,在一个 s 执行了 每个 动作 a 都有一个价值 Q(s,a)
	  一般在这个 s 上 ,我们会选择最大价值最大 的 a 执行
	  这个行为 a 价值只是价值,并不是 行为 a 的执行概率,
	  这个价值看起来有 执行概率一样的特征,因为如果是执行概率,在 s 也是优先执行执行概率最大的行为
	  但是这个价值的意义比执行概率要大的多
	2,这时我们 又对比一下 policy gradient  a = πθ(s) ,πθ(s,a)
	  在 s 经过策略 πθ(s)得到应当执行的行为 ,
	  πθ(s,a) 表示根据策略 在 s 执行 a 的执行概率!!
	  πθ(s) 导出的都是 执行概率最大的行为
	  即 policy gradient 引入了 执行概率的概念,但是 Q-learning 没有
	3,回到 Q-learning ,
	  假如 只有成功到达终点的那一步有 r 其他步 r = 0, 
	  当模型充分训练,就会有这样的结果
	  在 s 执行高价值a,到达s',在s'也执行高价值 a'
	  a 与 a' 的价值差表较小, 一般是a'比a的价值大一点
	  在 s' 执行高价值 a',到达s'',在 s'' 也执行高价值 a''
	  a 与 a' 的价值差表较小, 一般是a'比a的价值大一点
	  这就 规划出一套连续动作出来, 也就说, Q-learning 训练出来的一套动作之间有连贯性
	  而 Q-learning 是通过 td_error: [ r + gama * Q(s_,max_a_) - Q(s,a) ] 来 修正 Q(s,a)的
	  即:
		Q(s,a) = Q(s,a) + Alpha * [ r + gama * Q(s',max_a') - Q(s,a) ]
	  当模型充分训练后, a 与 a' 的价值价值差表较小, 
	  td_error: [ r + gama * Q(s_,max_a_) - Q(s,a) ] 也相当小 !!!
	  如果无休止地继续下去, a 与 a' 的价值本身永远在增大, a 与 a' 的价值差却永远在缩小
	  td_error 也永远在缩小
	  而所有 s 的其他行为 的价值 却基本保持不变或有机会小幅度变化
我们使用 td_error 代替 vt  由于 td_error 在每步行动后立即确定下来
	也就说 loss 可以这样: j(θ) = -logπθ(st,at)td_error,
	那么 每一步都可以 θ ← θ + α * ∇θ ( logπθ(st,at)td_error )
	即可以 单步更新 policy πθ(s)了,
	td_error : r + gama * Q(s_) - Q(s)代替 vt 称为 TD Actor-Critic
	策略部分 -logπθ(st,at) 是actor , td_error是 Critic
我们讨论 TD-AC 的可行性原理. 根据 Reinforcement Learning/Actor_Critic_Advantage/AC_CartPole.py具体例子说明
	ac 用了 td 误差,但斌没有 带入了 Q-leaning 的概念
	因为 Q-leaning 是用 价值衡量 每个 s 的每个a 所以对应 td_error: r + gama * Q(s_,max_a_) - Q(s,a) 
	但是 AC 是里 是用价值衡量 S 本身 对应用到的td_error : r + gama * Q(s_) - Q(s)
	Ac 为 policy πθ(s) 做了一个网络, 为 Q(s) 也做了一个网络

	假如模型充分训练
	在 s 我们通过策略得到 动作 a,一般都大概率执行策略导出行为
	s本身价值 Q(s)
	实际执行a 到达 s', 
	s本身价值 Q(s),	s'本身价值 Q(s') 都相当大
	但是他们的 td_error 却相当小	 
	由于在s执行了策略导出的行为, 所以 -logπθ(st,at) 几乎等于0
	所以  j(θ) = -logπθ(st,at)td_error 相当小
	所以更新策略的幅度非常小,而没有实质改变过策略
	即下次到达 s 时策略也会推荐执行 a 到达 s' 就是说收敛到一个动作上来
	而且由于游戏奖励的方式,靠近游戏终点的奖励是最大的!!靠近终点的s价值是最大的!!
	又应为 充分收敛后, j(θ) 都相当小,意味着 td_error 相当小
	所以对应的收敛的一套行为所经过的所有 s 的价值都是相当大的,
	而其他不怎么经过的s价值都比较小

	更深入的讨论!!
	如果 在某 必经 s 小概率执行了 非策略行为 a', 达到 s''
	s本身价值 还是 Q(s),而且是挺大的值 ,s''因为不是常常经过的,本身价值 Q(s'')一般是比较小的
	那么, s价值 和 s'价值 td_error 是负数
	minimize((td_error)^2) 反馈到 s本身价值,让Q(s)更靠近 Q(s''),但不能判断是让 Q(S) 变小还是变大
	因为有可能 让Q(s)变大一点, Q(s'')变大很多,但是 (td_error)^2 的确变小
	(与Q-Learning区别是,Q-learning会让 Q(s,a)直接添加 td_error !!来缩小与 Q(s'',a'')价值距离 )
	由于在s执行了非策略导出的实际行为, 所以 -logπθ(st,at) 相当大的正数	
	所以  j(θ) = -logπθ(st,at)td_error 是负数
	minimize j(θ),相当于 增大 -logπθ(st,at) 
	即让更新策略时,会让策略远离推荐 这个小概率行为 
	所以更新策略的幅度非常大,或者实质改变了策略	
	即下次到达 s 时策略或者会推荐执行 a'' 到达 s''

但是 AC 例子 虽然能单步更新 policy 看起来更有效率,但好像没想象中容易收敛??
	假设1 很多时候,由于大部分直接执行策略导出的行为 所以 -logπθ(st,at) 等于0,不管你td_error有多大
	      loss : j(θ) 都等于0 所以就 让大部分单步都没有充分对policy做修正,浪费了大量时间
	假设2,td_error 等于0或者相当小 的时候不说明 Q(s),Q(s')都是相当高,
	      而 最终收敛的结果是,我们经常经过的s 的Q(s)值是相当高的,
	      所以这种 td_error 太小的情况会影响到 policy 和 s价值网络的 有效率地更新
	假设3,minimize((td_error)^2) 修正价值网络 并没有遵循比较明确的修正路径
	     就是所 我们只知道修正后 td_error: r + gama * Q(s_) - Q(s) 减少了,
	     但是经过价值网络得到 Q(s_) , Q(s) 都变化,可能都变大,可能都变小,可能还有其他变化
	     对比 Q-learning的修正方式会发现 Q-learning要求 Q(s_)保持不变,
	     Q(s) 往Q(s_)贴近这样明确的修正方式来 降低 td_error的
	     所以有可能这个原因导致 收敛过程反反复复,效果不佳
补充一点可能并不太完整的 td_error 内容
	r + gama * Q(s_) - Q(s) -> 0 时
	意味着 Q(s) = r + gama * Q(s_)
	说明了RL 的一个马尔科夫模型的基本概念
	在 s 的价值 等于 从 s 到达 s_ 后得到的奖励 加上 gama乘以s_本身的价值 
总的来说, AC_CartPole.py 例子比较失败,就是说不能很好地收敛,
	并不是说不应该使用 AC 模型,而是说使用AC模型时,各方面细节处理还是得比较注意的
	AC 模型 的讨论就此暂告一段落

	







20180528
升级版AC : Deep Deterministic Policy Gradient (DDPG)
参考: https://blog.csdn.net/u013236946/article/details/73243310
看了代码,发现 这里 policy 直接修正十分奇怪!!更新policy 不是通过 log(πθ(s,a))修正,而是直接通过修正 πθ(s)
DDPG思维十分奇怪,虽然说是 基于 AC 的 但和AC有着许多本质的区别:
首先得讨论 DDPG 的算法原理:
首先俩个部分 actor critic 
	actor: 即 policy gradient 的 policy : a = πθ(s)
	critic: 就是 Q-learning 的 在s执行a的价值 : Q(s,a)
	actor部分 有两个独立网络, 估计网络: a = πθ(s) 和 目标网络: a' = π'θ'(s)
	critic部分 也有两个独立网络, 估计网络: Q(s,a) 和 目标网络: Q'(s,a)	
然后是算法过程 
	每个 epidode 的每一步:
		经过估计策略网络得到策略行为 a = πθ(s),
		然后加一点噪音得到实际的操作行为 ai = random(a)
		游戏执行 ai 到达 s_ 并得到直接奖励 r
		把每一步 (s,ai,r,s_) 存起来.比如存入 Memory
	当执行了足够步,或者到达终点后,停止游戏操作,开始更新网络:
		从 Menory 随机取出 N 条记录 (s,ai,r,s_)
		对于每条记录:
			s 通过 估计策略网络得回 a = πθ(s)
			s,a 通过 估计价值网络得到 Q(s,a)
			s,ai 通过 估计价值网络得到 Q(s,ai)
			s_ 通过 目标策略网络得到 a_' = π'θ'(s_)
			s_,a_' 通过 目标价值网络得到 Q'(s_,a_')
		然后得到 俩个 误差函数:
			估计策略网络的误差函数 : jp = 1/N * ∑( Q(s,a) )
			估计价值网络的误差函数 : jv = 1/N * ∑( (r + gama * Q'(s_,a_') - Q(s,ai))^2 )
		更新估计网络的参数:
			修正 估计策略网络 πθ 的 参数 θ :
				d jp   d 1/N * ∑( Q(s,a) )   d πθ(s)   d 1/N * ∑( Q(s,a) )
				____ = ___________________ * _____   = ___________________
				d θ	    d πθ(s)	      d θ             d θ
				这时,所有的 s, Q(s,a)网络的参数 都作为常数
			修正 估计价值网络 Q() 的 参数 Φ :
				d jv   d 1/N * ∑( (r + gama * Q'(s_,a_') - Q(s,ai))^2 )
				____ = ________________________________________________
				d Φ                           d Φ
				这时,所有的 s,ai,r,s_ 还有 Q'(s,a),πθ(s),π'θ'(s_)网络的参数 都作为常数
		更新目标网络的参数:
			修正 目标策略网络 π'θ'() 的 参数 θ' :		
				θ' =  τθ + (1-τ)θ'
			修正 目标价值网络 Q'() 的 参数 Φ' :
				Φ' =  τΦ + (1-τ)Φ'	
	如果是游戏结束时更新的网络,那么这个episode也就完了,开始下个episode
	如果是游戏中途更新的网络,清空 Memory, 继续执行游戏,并采集新的 (s,ai,r,)到 Memory,
		为下次网络更新做准备

可见,与 AC 十分不一样,特别是更新策略网络的方式, DDPG 使用到 πθ(s) ,但没有使用 πθ(s,a)
DDPG 直接通过策略行为得到的最大价值来更新策略网络,
AC 通过实际行为在策略中的执行概率来更新策略网络.
学习原理未分析!!
代码未完整分析,特别是第一個例子代码写得非常迷
(未完待续)
实际执行动作是 在以策略导出动作为均值的正太分布里选的动作 










20180529
A3C 并行的AC
https://blog.csdn.net/u013236946/article/details/73195035
使用到 tensorflow 的并行组件
	# 开启一个协调器  
	coord = tf.train.Coordinator()  
	#把开启的线程加入主线程，等待threads结束 
	coord.join(threads) 








20180530
ppo 基于 AC
创新了一种方法 动态调节 policy πθ(s) 的更新幅度
policy gradient 通过梯度下降 优化 policy时候 都是通过固定的学习率 alpha 执行的
这样可能会导致一次更新里,更新过度或者更新不显著的问题.
即 优化 πθ(s) 梯度下降步伐大小过于 固定
因此提出使用 动态学习率 alpha 的方法 优化 πθ(s)
ppo 算法实际上并没有使用 动态学习率 alpha 更新 policy,还是使用固定 alpha
但是他巧妙的算法实际上达到了如此效果,同样使得 优化 πθ(s) 梯度下降步伐 灵活变动
ppo是基于google 基于openai TRPO,发布的, DPPO是ppo基础上的并行版 类似A3C 与AC 关系
同样先看详细的算法:(本例子基于TRPO 与 ppo)
首先俩个部分 actor critic 
	actor: 即 policy gradient 的 policy : a = πθ(s)
	critic: 是 马尔科夫模型 s的价值 : Q(s)
	actor部分有两个网络 一个是旧网络 πθ',一个是当前网络 πθ
	critic部分只有一个价值网络
算法过程:
	每个 epidode 的每一步:
		经过策略网络得到策略行为 a = πθ(s),
		然后加一点噪音得到实际的操作行为 ai = random(a)
			(这里的方法同样,在以策略导出动作为均值的正太分布里,选出的动作实际的操作行为)
		游戏执行 ai 到达 s_ 并得到直接奖励 r
		把每一步 (s,ai,r,) 存起来.比如存入 Memory		
	当执行了足够步,或者到达终点后,停止游戏操作,开始更新网络:
		获得对比组的 Q'(s)
			假如执行了 T 步,存了 T 记录
			知道 第T步后 将到达 s_T+!, s_T+!对应的价值经过价值网络得到 Q(s_T+1)
			那么 执行第T步所在的 s_T 的价值是 不用通过价值网络,根据马尔科夫理论得 Q'(s_T) = r_T + gama * Q(s_T+1)
			然后 执行第T-1步所在的 s_T-1 的价值 Q'(s_T-1) = r_T-1 + gama * Q'(s_T)
			同理 执行第T-2步所在的 s_T-2 的价值 Q'(s_T-2) = r_T-2 + gama * Q'(s_T-1)
			即一般地, 执行第 t 步所在的 s_t 的价值 Q'(s_t) = r_t + gama * Q'(s_t+1)
			也就说,执行了第T步到达的新 s 的价值用价值网络算出, Q(s_T+1)
			而 从1到T 步本身所处的 s 的价值直接根据马尔科夫理论 叠加导出 Q'(s_t)
		把 actor的当前策略网络的内容 θ 直接拷贝替换掉  actor的旧策略网络的内容 θ'
			即: θ' <- θ
			这时 当前策略网络 跟 旧策略网络 是一样的.
		计算每一步的 td_error:
			就是: Q'(s_t) - Q(s_t)
			就是: r_t + gama * Q'(s_t+1) - Q(s_t)
			与以往不一样的是,以往是: r_t + gama * Q'(s_t+1) - Q(s_t)
			即以往的 s_t+1 也是 Q()价值网络输出, 而现在的 s_t+1, 是通过根据马尔科夫理论得到
		更新 actor:
			得到 loss: 
			                πθ(s_t)
			j(θ) = 1/T  ∑ ( _______   * (r_t + gama * Q'(s_t+1) - Q(s_t))  -  lamda * kl(πθ(s_t),πθ'(s_t))
			            t   πθ'(s_t)

			然后第一次更新当前策略网络 :  θ <- θ +  alpha ( d j(θ) / d θ)  
		  		(注意,第一次更新旧策略网络 πθ'(s_t)前,θ与θ'相等, 
				所以每一步的kl散度为0 : kl(πθ(s_t),πθ'(s_t) 等于0)
				注意,θ' 更新 actor 整个过程中都看作常数不改变
			第一次更新当前策略网络后, 重新计算 每一步的kl散度,并求出 均值 kl_mean
				
			kl_mean = 1/T  ∑ ( kl(πθ(s_t),πθ'(s_t))
			               t
			分析 kl_mean: 
			如果 kl_mean 超过阀值, 表示 更新后的当前策略网络 πθ(s_t) 比起 πθ'(s_t) 优化幅度比较显著
				结束这次 actor 的更新
			如果 kl_mean 太小, 表示 更新后的当前策略网络 πθ(s_t) 比起 πθ'(s_t) 优化幅度比较小
				根据 kl_mean 修正 lamda 
				把当前新 θ lamda 代回 loss: j(θ)
				然后执行第二次 θ <- θ +  alpha ( d j(θ) / d θ)  			
				重复 分析 kl_mean,
				.....
			如果反复执行了 x 次 (比如10次) θ <- θ +  alpha ( d j(θ) / d θ) 后,
				kl_mean 还是没有超过阀值,
				也就此作罢, 结束这次 actor 的更新
		更新 critic
			minimize ( (r_t + gama * Q'(s_t+1) - Q(s_t))^2 )
			从而更新实际的 价值网络
	如果是游戏结束时更新的网络,那么这个episode也就完了,开始下个episode
	如果是游戏中途更新的网络,清空 Memory, 继续执行游戏,并采集新的 (s,ai,r,)到 Memory,
		为下次网络更新做准备
学习原理分析(未完待续)







20180531
关于机械臂 运动 的问题 分析
机械臂 追逐到达 蓝点 , 我们可以容易感受到 通过强化学习,机械臂是可以迅速到达的,
	但是,如果 蓝点每个episode 都在不同的地方, 那这可怎么弄???
就相当与 学习走出迷宫的游戏, 只要迷宫是固定的,总会学习到路径
	但是如果 每个episode 迷宫的陷阱和出口都不同,那还可以顺利学习走出迷宫么?
	这种情况下,我们要吧这个动态的迷宫扩展成一个抽象的更高维度,但却是静态的迷宫
	那么在这个静态的迷宫下,也同理总会学习到路径.
	那么如何表达 agent 在走一个抽象的静态迷宫?
	其实就是扩展 s 向量维度 本来 s: {x,y}
	现在扩成 s : {x,y,陷阱x,陷阱y,终点x,终点y}
	虽然说 每个episode内, 陷阱x,陷阱y,终点x,终点y 是固定的,但是还是参与的训练上来
回到机械臂
	s的维度足够多还包括 蓝点参数的时候,是可以训练收敛的
	可以认为 机械臂是到达一个高维固定的蓝点上








20180601
安装 ROS: 
https://blog.csdn.net/hebbely/article/details/70598208
设置公钥什么的暂时不理解,所以记录下log:
infortech-ubuntu-1404@infortechubuntu1404-HP-EliteDesk-880-G2-TWR:~$ sudo sh -c 'echo "deb http://packages.ros.org/ros/ubuntu $(lsb_release -sc) main" > /etc/apt/sources.list.d/ros-latest.list'
[sudo] password for infortech-ubuntu-1404: 
infortech-ubuntu-1404@infortechubuntu1404-HP-EliteDesk-880-G2-TWR:~$ wget http://packages.ros.org/ros.key -O - | sudo apt-key add -
--2018-06-01 09:33:14--  http://packages.ros.org/ros.key
正在解析主机 packages.ros.org (packages.ros.org)... 64.50.233.100, 64.50.236.52, 140.211.166.134, ...
正在连接 packages.ros.org (packages.ros.org)|64.50.233.100|:80... 已连接。
已发出 HTTP 请求，正在等待回应... 200 OK
长度： 1162 (1.1K)
正在保存至: “STDOUT”
100%[======================================>] 1,162       --.-K/s   用时 0s    
2018-06-01 09:33:15 (140 MB/s) - 已写入至标准输出 [1162/1162]
OK

视野: https://blog.csdn.net/wxflamy/article/list/2?t=1

OpenSceneGraph（简称OSG）是一个开放源码，跨平台的图形开发包，
它为诸如飞行器仿真，游戏，虚拟现实，科学计算可视化这样的高性能图形应用程序开发而设计。
它基于场景图的概念，它提供一个在OpenGL之上的面向对象的框架，
从而能把开发者从实现和优化底层图形的调用中解脱出来，并且它为图形应用程序的快速开发提供很多附加的实用工具

ROS-Industrial
需要前期准备的安装环境：
1. ROS kinetic系统
2. ROS-Industrial 的 universal_robot包
UR5机器人控制器的镜像版本是 3.0。
驱动兼容性
需要检查 universal_robot包的驱动和控制器的兼容性。
universal_robot包通过以太网连接与硬件进行通信，当建立连接的时候ROS-Industrial上传由URScript编写的程序，
URScript是类似python的UR机器人自己的脚本语言。这段程序负责监听ROS-Industrial的 simple_messages 包发送的消息信息，并将这些消息解释成硬件指令。

MoveIt! RViz ROS模拟机器人模块 : https://blog.csdn.net/wxflamy/article/details/79160781
				https://blog.csdn.net/xuehuafeiwu123/article/details/54561737
				https://blog.csdn.net/jayandchuxu/article/details/54693870
				http://wiki.ros.org/ur_gazebo
				https://www.ncnynl.com/category/ros-moveit/
				http://moveit.ros.org/install/

用URDF从零开始构建一个机器人模型 :https://blog.csdn.net/wxflamy/article/details/79235493

RVIZ


自己机子安装 ROS
william@william-HP:~$ sudo sh -c 'echo "deb http://packages.ros.org/ros/ubuntu trusty main" > /etc/apt/sources.list.d/ros-latest.list' 
[sudo] william 的密码： 
william@william-HP:~$ wget http://packages.ros.org/ros.key -O - | sudo apt-key add -
--2018-06-03 18:53:46--  http://packages.ros.org/ros.key
正在解析主机 packages.ros.org (packages.ros.org)... 64.50.233.100, 140.211.166.134, 64.50.236.52, ...
正在连接 packages.ros.org (packages.ros.org)|64.50.233.100|:80... 已连接。
已发出 HTTP 请求，正在等待回应... 200 OK
长度： 1162 (1.1K)
正在保存至: “STDOUT”

-                   100%[===================>]   1.13K  --.-KB/s    in 0s      

2018-06-03 18:53:47 (101 MB/s) - 已写入至标准输出 [1162/1162]
OK

视野: https://blog.csdn.net/wxflamy/article/list/2?t=1

OpenSceneGraph（简称OSG）是一个开放源码，跨平台的图形开发包，
它为诸如飞行器仿真，游戏，虚拟现实，科学计算可视化这样的高性能图形应用程序开发而设计。
它基于场景图的概念，它提供一个在OpenGL之上的面向对象的框架，
从而能把开发者从实现和优化底层图形的调用中解脱出来，并且它为图形应用程序的快速开发提供很多附加的实用工具

ROS-Industrial
需要前期准备的安装环境：
1. ROS kinetic系统
2. ROS-Industrial 的 universal_robot包
UR5机器人控制器的镜像版本是 3.0。
驱动兼容性
需要检查 universal_robot包的驱动和控制器的兼容性。
universal_robot包通过以太网连接与硬件进行通信，当建立连接的时候ROS-Industrial上传由URScript编写的程序，
URScript是类似python的UR机器人自己的脚本语言。这段程序负责监听ROS-Industrial的 simple_messages 包发送的消息信息，并将这些消息解释成硬件指令。

MoveIt! RViz ROS模拟机器人模块 : https://blog.csdn.net/wxflamy/article/details/79160781
				https://blog.csdn.net/xuehuafeiwu123/article/details/54561737
				https://blog.csdn.net/jayandchuxu/article/details/54693870
				http://wiki.ros.org/ur_gazebo
				https://www.ncnynl.com/category/ros-moveit/

用URDF从零开始构建一个机器人模型 :https://blog.csdn.net/wxflamy/article/details/79235493

RVIZ

leap motion :做手势识别的要有设备才行,只有软件没用!1

开源云台算法,开源3d打印算法,了解pwm电机控制与精度


wil halcon  视觉库

ROS 入门:  https://www.ncnynl.com/archives/201608/503.html
source /opt/ros/indigo/setup.bash
	在每次打开终端时你都需要先运行上面这条命令后才能运行ros相关的命令
	为了避免这一繁琐过程，你可以事先在.bashrc文件
catkin方式创建ROS工作空间  暂时不学 rosbuild,因为快被淘汰
	mkdir -p ~/catkin_ws/src
	cd ~/catkin_ws/
	catkin_make
	cd ~/catkin_ws/src
	即使这个工作空间是空的（在'src'目录中没有任何软件包，只有一个CMakeLists.txt链接文件），你依然可以编译它
	cd ~/catkin_ws/
	当前目录应该能看到'build'和'devel'这两个文件夹。在'devel'文件夹里面你可以看到几个setup.sh文件。
	source这些文件中的任何一个都可以将当前工作空间设置在ROS工作环境的最顶层，
	source devel/setup.bash
	要想保证工作空间已配置正确需确保ROS_PACKAGE_PATH环境变量包含你的工作空间目录，采用以下命令查看：
	echo $ROS_PACKAGE_PATH
	/home/catkin_ws/src:/opt/ros/indigo/share:/opt/ros/indigo/stacks
ROS文件系统概念 针对catkin方式
	使用到的教程
	sudo apt-get install ros-indigo-ros-tutorials
	基本概念:
	Packages: 软件包，是ROS应用程序代码的组织单元，每个软件包都可以包含程序库、可执行文件、脚本或者其它手动创建的东西。
	Manifest (package.xml): 清单，是对于'软件包'相关信息的描述,用于定义软件包相关元信息之间的依赖关系，这些信息包括版本、维护者和许可协议等。
	在catkin中，你可以定义综合包来集成相似包并实现将多个包驻留在单个VCS库。这两个特征可替换功能包集的功能。
	rospack find roscpp	//示范获取软件包有关信息 这里获取 rosapp 软件包的有关信息
	roscd roscpp		//直接切换(cd)工作目录到某个软件包或者软件包集当中 ,例如 cd到 rosapp 软件包
		跳到了 /opt/ros/indigo/share/roscpp
		roscd只能切换到那些路径已经包含在ROS_PACKAGE_PATH环境变量中的软件包
    		要查看ROS_PACKAGE_PATH中包含的路径可以输入：echo $ROS_PACKAGE_PATH
		/home/catkin_ws/src:/opt/ros/indigo/share:/opt/ros/indigo/stacks
		可见 /opt/ros/indigo/share/roscpp 在 $ROS_PACKAGE_PATH 之中
	roscd roscpp/cmake	//cd到软件包里的子目录 例如 cd到 roscpp软件包的cmake 即/opt/ros/indigo/share/roscpp/cmake
	roscd log		//cd到ROS保存日记文件的目录下
	rosls roscpp_tutorials	//列出 软件包下的内容 例如 列出 roscpp_tutorials软件包里的内容
	输入命令时 不妨尝试使用 Tab 补全内容,例如软件包名字
catkin程序包 (软件包)
	一个catkin程序包必须符合以下要求：
	1.必须包含catkin compliant package.xml文件，这个package.xml文件提供有关程序包的元信息。
	2.必须包含一个catkin 版本的CMakeLists.txt文件，而Catkin metapackages中必须包含一个对CMakeList.txt文件的引用。
	3.每个目录下只能有一个程序包。这意味着在同一个目录下不能有嵌套的或者多个程序包存在。
		例如:
		workspace_folder/        -- WORKSPACE工作空间
		  src/                   -- SOURCE SPACE 源码目录
		    CMakeLists.txt       -- 'Toplevel' CMake file, provided by catkin
		    package_1/		 -- catkin程序1
		      CMakeLists.txt     -- CMakeLists.txt file for package_1
		      package.xml        -- Package manifest for package_1
		    ...
		    package_n/		 -- catkin程序n
		      CMakeLists.txt     -- CMakeLists.txt file for package_n
		      package.xml        -- Package manifest for package_n
	创建一个catkin程序包
		cd ~/catkin_ws/src
		catkin_create_pkg beginner_tutorials std_msgs rospy roscpp
			//创建一个名为'beginner_tutorials'的新程序包,依赖于 std_msgs、roscpp和rospy
			//这些依赖包信息会保存在 package.xml文件中
			//查看依赖信息: roscd beginner_tutorials && cat package.xml
			//<package>
			//...
			//  <buildtool_depend>catkin</buildtool_depend>
			//  <build_depend>roscpp</build_depend>
			//  <build_depend>rospy</build_depend>
			//  <build_depend>std_msgs</build_depend>
			//...
			//</package>
		rospack depends1 beginner_tutorials
			//查看 beginner_tutorials 的一级依赖,即 std_msgs、roscpp和rospy
		rospack depends1 beginner_tutorials
			//查看 beginner_tutorials 用到的间接的直接的所有依赖包
	自定义 package.xml
		//注释符号
		<!-- this is Examples -->
		//描述标签例子
		<description>The beginner_tutorials package</description>
		//维护者标签
		<!-- One maintainer tag required, multiple allowed, one person per tag --> 
		<!-- Example:  -->
 		<!-- <maintainer email="jane.doe@example.com">Jane Doe</maintainer> -->
 		<maintainer email="user@todo.todo">user</maintainer>
		//许可标签
		//一些常见的开源许可协议有BSD、MIT、Boost Software License、GPLv2、GPLv3、LGPLv2.1和LGPLv3
		<!-- One license tag required, multiple allowed, one license per tag -->
		<!-- Commonly used license strings: -->
 		<!--   BSD, MIT, Boost Software License, GPLv2, GPLv3, LGPLv2.1, LGPLv3 -->
		<license>BSD</license>
		//依赖项目标签
		<!-- The *_depend tags are used to specify dependencies -->
		<!-- Dependencies can be catkin packages or system dependencies -->
		<!-- Examples: -->
		<!-- Use build_depend for packages you need at compile time: -->
		<!--   <build_depend>genmsg</build_depend> -->
		<!-- Use buildtool_depend for build tool packages: -->
		<!--   <buildtool_depend>catkin</buildtool_depend> -->
		<!-- Use run_depend for packages you need at runtime: -->
		<!--   <run_depend>python-yaml</run_depend> -->
		<!-- Use test_depend for packages you need only for testing: -->
		<!--   <test_depend>gtest</test_depend> -->
		<buildtool_depend>catkin</buildtool_depend>
		<build_depend>roscpp</build_depend>
		<build_depend>rospy</build_depend>
		<build_depend>std_msgs</build_depend>
编译catkin程序包
	catkin_make 是一个命令行工具，它简化了catkin的标准工作流程。
	可以认为catkin_make是在CMake标准工作流程中依次调用了cmake 和 make
	回顾 cmake 项目编译的一般过程: (注意,运行以下方式命令是无效的，因为它只是一个演示CMake工作流程的例子)
		$ mkdir build
		$ cd build
		$ cmake ..
		$ make
		$ make install  # (可选)
	对比 catkin, 只要cd 到ROS工作空间, 以下命令就可以把 src文件夹下的所有catkin程序包都编译
		$ catkin_make
		$ catkin_make install  # (可选)
	catkin 工作空间下
	build目录是build space的默认所在位置，同时cmake和make也是在这里被调用来配置并编译你的程序包。
	devel目录是devel space的默认所在位置, 同时也是在你安装程序包之前存放可执行文件和库文件的地方
ROS 图（graph）概念
	Nodes:节点,一个节点即为一个可执行文件，它可以通过ROS与其它节点进行通信。
		ROS节点可以使用ROS客户库与其他节点通信。
		节点可以发布或接收一个话题。
		节点也可以提供或使用某种服务。
	Messages:消息，消息是一种ROS数据类型，用于订阅或发布到一个话题。
	Topics:话题,节点可以发布消息到话题，也可以订阅话题以接收消息。
	Master:节点管理器，ROS名称服务 (比如帮助节点找到彼此)。
	rosout: ROS中相当于stdout/stderr。
	roscore: 主机+ rosout + 参数服务器 (参数服务器会在后面介绍)。
		roscore 是你在运行所有ROS程序前首先要运行的命令
		如果roscore不能初始化并提示缺少权限，这可能是因为~/.ros文件夹归属于root用户（只有root用户才能访问）
		修改该文件夹的用户归属关系：
		sudo chown -R <your_username> ~/.ros
	ROS客户端库: 允许使用不同编程语言编写的节点之间互相通信:
	        rospy = python 客户端库
	        roscpp = c++ 客户端库
	        rosjs = javascripts客户端库
	        rosjava = java客户端库
ROS交互原理演示:
	roscore 在一个终端执行后,在另一个终端执行 rosnode list 得到 /rosout	
		这表示当前只有一个节点在运行: rosout。
		因为这个节点用于收集和记录节点调试输出信息，所以它总是在运行的。
		rosnode info /rosout 命令返回的是关于一个特定节点的信息。
	rosrun turtlesim turtlesim_node  
		在一个新端口执行
		rosrun 使用包名直接运行一个包内的节点(而不需要知道这个包的路径)。
		这里 执行了  turtlesim软件包 的 turtlesim_node 节点 ,打开了一个乌龟窗口
		这时,在另一个终端执行 rosnode list 得到 /rosout /turtlesim  俩个在个工作的节点
	rosrun turtlesim turtle_teleop_key
		在一个新端口执行
		这时是执行了一个键盘事件,然后在这个终端按键盘方向键,乌龟窗口的乌龟会动
		这时,在另一个终端执行 rosnode list 得到 /rosout /turtlesim  /teleop_turtle 三个在个工作的节点
		turtlesim_node节点 和 turtle_teleop_key节点 之间是通过一个ROS话题来互相通信的。
			turtle_teleop_key 在一个话题上发布按键输入消息，
			turtlesim 则订阅该话题以接收该消息。
	rosrun rqt_graph rqt_graph
		在一个新端口执行	
		打开 rqt_graph 窗口来显示当前运行的节点和话题。
		可以看到的 turtlesim_node和turtle_teleop_key节点正通过一个名为/turtle1/cmd_vel的话题来互相通信
		这时,在另一个终端执行 rosnode list 得到四个在个工作的节点
			/rosout 
			/turtlesim  
			/teleop_turtle
			/rqt_gui_py_node_10588
	rostopic echo /turtle1/cmd_vel
		在一个新端口执行
		可以监视 topic /turtle1/cmd_vel 的通讯内容
		在另一个终端执行 rosnode list 得到五个在个工作的节点
			/rosout 
			/turtlesim  
			/teleop_turtle
			/rqt_gui_py_node_10588		//rqt_graph 窗口 监视节点
			/rostopic_10707_1528265808190	//这个是 rostopic ho /turtle1/cmd_vel 生成的监视节点
		在 rqt_graph 窗口 刷新一下,发现 turtle_teleop_key节点 通过一个名为/turtle1/cmd_vel的话题
		同时給 turtlesim_node 和 /rostopic_10707_1528265808190 发信息
		topic /turtle1/cmd_vel  通过为 节点间传递 Messages 实现通讯
		这时的 turtle_teleop_key节点 是 消息发布器
		turtlesim_node 和 /rostopic_10707_1528265808190 节点 都是 消息订阅器
	rostopic type /turtle1/cmd_vel
		查看所发布话题的消息类型
		得到 geometry_msgs/Twist
	rosmsg show geometry_msgs/Twist
		查看所消息的详细情况,得到:
		geometry_msgs/Vector3 linear
		  float64 x
		  float64 y
		  float64 z
		geometry_msgs/Vector3 angular
		  float64 x
		  float64 y
		  float64 z
	rostopic pub -1 /turtle1/cmd_vel geometry_msgs/Twist -- '[2.0, 0.0, 0.0]' '[0.0, 0.0, 1.8]'
		试着手动在 topic /turtle1/cmd_vel 发布一条消息
		/turtlesim 会接收到,乌龟窗口的乌龟会画出一段圆弧
		-1 参数,后面接 指定的topic
		-- 后接数据
	rostopic pub /turtle1/cmd_vel geometry_msgs/Twist -r 1 -- '[2.0, 0.0, 0.0]' '[0.0, 0.0, 1.8]'	
		发布一个稳定的命令流
		这条命令以1Hz的频率发布速度命令到速度话题上
		刷新 rosnode list 发现多了一个 节点 /rostopic_10900_1528267591378
		刷性 rqt_graph 窗口 发现多了一个 这个多出的节点是一个新的 topic /turtle1/cmd_vel 的消息发布器
		rostopic hz /turtle1/pose
			监测 turtlesim_node 乌龟窗口的乌龟 受到指令后的变化情况,
			间接了解到 的 /rostopic_10900_1528267591378 给 turtlesim_node 传的消息的频率,大概 62hz 发一次
			其实这里 开通了一个新的 topic /turtle1/pose 消息发布者是 turtlesim_node
			没有消息订阅者,但消息直接输出到终端上
	rostopic type /turtle1/cmd_vel | rosmsg show
		获取关于话题的更深层次的信息
	rosrun rqt_plot rqt_plot
		打开 rqt_plot 窗口监视 topic 的变化曲线??
		同样 使用 topic /turtle1/pose 
		消息发布者是 turtlesim_node 不断把乌龟的位置信息发送到 rqt_plot 窗口 显示
		但是 rqt_plot 窗口 不算ROS体系内的节点内容,所以不算是真征意义上的 消息订阅者
		rqt_graph 里头也没有看到 代表 rqt_plot 的节点
继续讨论 服务 的演示
	服务（services）是节点之间通讯的另一种方式。
	服务允许节点发送请求（request） 并获得一个响应（response）
	rosservice list         输出可用服务的信息
		/clear						//turtlesim_node提供的基本可用的服务
		/kill
		/reset
		/spawn
		/rosout/get_loggers				// /rosout节点提供的服务
		/rosout/set_logger_level
		/rqt_gui_py_node_10588/get_loggers
		/rqt_gui_py_node_10588/set_logger_level
		/teleop_turtle/get_loggers
		/teleop_turtle/set_logger_level
		/turtle1/set_pen
		/turtle1/teleport_absolute
		/turtle1/teleport_relative
		/turtlesim/get_loggers
		/turtlesim/set_logger_level
	rosservice type clear		查看 clear 服务的类型
		std_srvs/Empty   
		//服务的类型为空（empty),这表明在调用这个服务是不需要参数（比如，请求不需要发送数据，响应也没有数据）
	rosservice call clear		清除了turtlesim_node的背景上的轨迹
	rosservice type spawn | rossrv show	查看 spawn 服务的类型
		type spawn | rossrv show
		float32 x
		float32 y
		float32 theta
		string name
		---
		string name
	rosservice call spawn 2 2 0.2 "yui"	产生的一只新乌龟,名字为 yui
		name: yui	
继续讨论 参数 的演示
	rosparam使得我们能够存储并操作ROS参数服务器（Parameter Server）上的数据
	参数服务器能够存储整型、浮点、布尔、字符串、字典和列表等数据类型。
	rosparam使用YAML标记语言的语法。
	一般而言，YAML的表述很自然：	
	    1是整型
	    1.0是浮点型
	    one是字符串
	    true是布尔
	    [1,2,3]是整型列表
	    {a:b,c:d}是字典
	rosparam list			获取参数
		/background_b		这三个是 turtlesim_node 公开的参数
		/background_g
		/background_r
		/roslaunch/uris/aqy:51932
		/run_id
	rosparam set background_r 150	修改参数值,但是仅仅修改了而已
	rosservice call clear		申请 turtlesim_node 的clear服务,  使得修改后的参数生效
		这时 乌龟窗口背景变色
	rosparam get /			显示参数服务器上的所有内容
		background_b: 255
		background_g: 86
		background_r: 150
		rosdistro: 'indigo
		...
	rosparam dump params.yaml	把所有参数都保存起来存入  params.yaml
	rosparam load params.yaml	把params.yaml 的内容 写入修改 参数服务器上,(还要rosservice call clear后参数变化才生效)
rqt_console		
	rqt_console属于ROS日志框架(logging framework)的一部分，用来显示节点的输出日志信息
	rqt_logger_level允许我们修改节点运行时输出信息的日志等级（logger levels）
	logger levels包括 DEBUG、WARN、INFO和ERROR
	日志的详细说明: https://www.ncnynl.com/archives/201608/504.html
roslaunch
	roscd beginner_tutorials	回到之前的程序包 
		(如果失败,是因为新打开了的中端,ROS_PACKAGE_PATH没有包含制定路径,)
		可这样 export ROS_PACKAGE_PATH=/home/infortech-ubuntu-1404/1tb_disk/other/ROS/catkin_ws:$ROS_PACKAGE_PATH
		添加上路径了 重新 roscd beginner_tutorials
	mkdir launch && cd launch
	gedit turtlemimic.launch 	//包含内容:
	   <launch>
	   
	     <group ns="turtlesim1">
	       <node pkg="turtlesim" name="sim" type="turtlesim_node"/>
	     </group>
	   
	     <group ns="turtlesim2">
	       <node pkg="turtlesim" name="sim" type="turtlesim_node"/>
	     </group>
	   
	     <node pkg="turtlesim" name="mimic" type="mimic">
	       <remap from="input" to="turtlesim1/turtle1"/>
	       <remap from="output" to="turtlesim2/turtle1"/>
	     </node>
	   
	   </launch>
	roslaunch beginner_tutorials turtlemimic.launch	
		启动了两个乌龟擦窗口
	rostopic pub /turtlesim1/turtle1/cmd_vel geometry_msgs/Twist -r 1 -- '[2.0, 0.0, 0.0]' '[0.0, 0.0, -1.8]'
		给其中一个窗口 发指令,会看到 两个窗口几乎同时画圆
		rosrun rqt_graph rqt_graph 查看话题 消息 节点的关系
创建ROS消息
	msg文件就是一个描述ROS中所使用消息类型的简单文本。它们会被用来生成不同语言的源代码。
	msg文件存放在package的msg目录下，srv文件则存放在srv目录下
	msg文件实际上就是每行声明一个数据类型和变量名。
		int8, int16, int32, int64 (plus uint*) 
		float32, float64, string, time, 
		duration other msg files variable-length array[] and fixed-length array[C]
	创建一个 msg
	roscd beginner_tutorials	//来到我们之前做的示例程序包
	mkdir msg
	echo "int64 num" > msg/Num.msg	//给 msg文件 添加内容
	beginner_tutorials 的 package.xml, 添加:
		<build_depend>message_generation</build_depend>
		<exec_depend>message_runtime<exec_depend>
		表示 在构建的时候，我们只需要"message_generation"。
		表示 在运行的时候，我们只需要"message_runtime"。
	beginner_tutorials 的 CMakeLists.txt,
		修改 (增加对message_generation的依赖,这样就可以生成消息)
		find_package(catkin REQUIRED COMPONENTS
		  roscpp
		  rospy
		  std_msgs
		  message_generation		//添加上的
		)
		还有 确保你设置了运行依赖
		catkin_package(
		  ...
		  CATKIN_DEPENDS message_runtime	//添加上的
		  ...)	
		还有修改	
		add_message_files(
		  FILES
		  Num.msg
		)	
		添加
		generate_messages(
		  DEPENDENCIES
		  std_msgs		//例子是依赖std_msgs
		)
	rosmsg show beginner_tutorials/Num	通过rosmsg show命令，检查ROS是否能够识别我们建立的消息
		int64 num
	rosmsg show Num 	如果你忘记了消息所在的package，你也可以省略掉package名
		[beginner_tutorials/Num]:
		int64 num
创建ROS服务
	服务(srv): 一个srv文件描述一项服务。它包含两个部分：请求和响应。
	创建一个 srv
	roscd beginner_tutorials
	mkdir srv
	roscp rospy_tutorials AddTwoInts.srv srv/AddTwoInts.srv	
		不再自己新建服务,可以使用别的程序包已经有的服务,这里我们拷贝了一个别人的服务
		gedit 打开 AddTwoInts.srv 看到
			int64 a
			int64 b
			---
			int64 sum
		"---"前是请求内容, "---"后是响应的内容
	beginner_tutorials 的 package.xml, 添加:
		<build_depend>message_generation</build_depend>
		<exec_depend>message_runtime<exec_depend>
		表示 在构建的时候，我们只需要"message_generation"。	
		表示 在运行的时候，我们只需要"message_runtime"。
		两者同样有效于 服务srv
	beginner_tutorials 的 CMakeLists.txt,	
		修改 (增加对message_generation的依赖,这项也有生成服务的意思)
		find_package(catkin REQUIRED COMPONENTS
		  roscpp
		  rospy
		  std_msgs
		  message_generation		//添加上的
		)
		修改
		add_service_files(
		  FILES
		  AddTwoInts.srv
		)
		添加 (同样对服务有效)
		generate_messages(
		  DEPENDENCIES
		  std_msgs		//例子是依赖std_msgs
		)
	rossrv show beginner_tutorials/AddTwoInts	通过rossrv show命令，检查ROS是否能够识别我们建立的服务
		int64 a
		int64 b
		---
		int64 sum
	rossrv show AddTwoInts		如果你忘记了消息所在的package，你也可以省略掉package名
		[beginner_tutorials/AddTwoInts]:
		int64 a
		int64 b
		---
		int64 sum
重新编译,把新建立的 msg srv 编译到程序包里
	cd /home/infortech-ubuntu-1404/1tb_disk/other/ROS/catkin_ws
	catkin_make
	所有在msg路径下的.msg文件都将转换为ROS所支持语言的源代码。
	生成的C++头文件将会放置在~/catkin_ws/devel/include/beginner_tutorials/
	Python脚本语言会在 ~/catkin_ws/devel/lib/python2.7/dist-packages/beginner_tutorials/msg目录下创建。
	lisp文件会出现在~/catkin_ws/devel/share/common-lisp/ros/beginner_tutorials/msg/路径下.
	详尽的消息格式请参考Message Description Language 页面	
编写简单的消息发布器和订阅器 (C++ catkin)
	学会创建消息和服务后,就得开始尝试建立 消息发布和消息订阅器了
	roscd beginner_tutorials
	mkdir -p src
	写一个 talker.cpp 消息发布器节点
	#include "ros/ros.h"   //是一个实用的头文件，它引用了ROS系统中大部分常用的头文件
	#include "std_msgs/String.h"	//引用了std_msgs/String 消息, 它存放在std_msgs package里，是由String.msg文件自动生成的头文件
	#include <sstream>

	int main(int argc, char **argv)
	{
	  ros::init(argc, argv, "talker");	//初始化ROS。它允许ROS通过命令行进行名称重映射 在这里指定我们节点的名称——必须唯一
						//这里的名称必须是一个base name，不能包含/。
	  ros::NodeHandle n;	//为这个进程的节点创建一个句柄,
				//第一个创建的NodeHandle会为节点进行初始化，最后一个销毁的会清理节点使用的所有资源。
	  ros::Publisher chatter_pub = n.advertise<std_msgs::String>("chatter", 1000);
		//告诉master我们将要在chatter topic上发布一个std_msgs/String的消息
		//这样master就会告诉所有订阅了chatter topic的节点，将要有数据发布
		//第二个参数是发布序列的大小
		//如果发布的消息太快，缓冲区中的消息在大于1000个的时候就会开始丢弃先前发布的消息。
		//NodeHandle::advertise() 
		//返回一个 ros::Publisher对象,它有两个作用:
            	//它有一个 publish()成员函数可以让你在topic上发布消息；
            	//如果消息类型不对,它会拒绝发布。
	  ros::Rate loop_rate(10);
		//ros::Rate对象可以允许你指定自循环的频率。
		//它会追踪记录自上一次调用Rate::sleep()后时间的流逝，并休眠直到一个频率周期的时间。
		//在这个例子中，我们让它以10hz的频率运行。
	
	  int count = 0;
	  while (ros::ok())	//roscpp会默认安装一个SIGINT句柄，它负责处理Ctrl-C键盘操作——使得ros::ok()返回FALSE。
	  {			//ros::ok()返回false，如果下列条件之一发生:
				//	SIGINT接收到(Ctrl-C)
				//	被另一同名节点踢出ROS网络
				//	ros::shutdown()被程序的另一部分调用
				//	所有的ros::NodeHandles都已经被销毁
				//一旦ros::ok()返回false, 所有的ROS调用都会失效。	
	    std_msgs::String msg;	
	    std::stringstream ss;
	    ss << "hello world " << count;
	    msg.data = ss.str();
			//我们使用一个由msg file文件产生的‘消息自适应’类在ROS网络中广播消息。
			//现在我们使用标准的String消息，它只有一个数据成员"data"
	    ROS_INFO("%s", msg.data.c_str());
			//ROS_INFO和类似的函数用来替代printf/cout.
	    chatter_pub.publish(msg);	//现在我们已经向所有连接到chatter topic的节点发送了消息。
	    ros::spinOnce();
			//在这个例子中并不是一定要调用ros::spinOnce()，因为我们不接受回调。
			//然而，如果你想拓展这个程序，却又没有在这调用ros::spinOnce()，你的回调函数就永远也不会被调用。
			//所以，在这里最好还是加上这一语句。
	    loop_rate.sleep();
			//这条语句是调用ros::Rate对象来休眠一段时间以使得发布频率为10hz
	    ++count;
	  }
	  return 0;
	}

	写一个 订阅器节点listener.cpp
	#include "ros/ros.h"
	#include "std_msgs/String.h"

	void chatterCallback(const std_msgs::String::ConstPtr& msg)
	{					
	  // chatterCallback是一个回调函数，当消息到达chatter topic的时候就会被调用
	  ROS_INFO("I heard: [%s]", msg->data.c_str());
	}

	int main(int argc, char **argv)
	{
	  ros::init(argc, argv, "listener");	
	  ros::Subscriber sub = n.subscribe("chatter", 1000, chatterCallback);
		//告诉master我们要订阅chatter topic上的消息	
		//当有消息到达topic时，ROS就会调用chatterCallback()函数。
		//第二个参数是队列大小，以防我们处理消息的速度不够快，在缓存了1000个消息后，再有新的消息到来就将开始丢弃先前接收的消息。
		//NodeHandle::subscribe()返回ros::Subscriber对象,必须让它处于活动状态直到你不再想订阅该消息。
		//当这个对象销毁时，它将自动退订上的消息。
		//有各种不同的NodeHandle::subscribe()函数，允许你指定类的成员函数，
		//甚至是Boost.Function对象可以调用的任何数据类型。roscpp overview 提供了更为详尽的信息。
	  ros::spin();
		//ros::spin()进入自循环，可以尽可能快的调用消息回调函数
		//如果没有消息到达，它不会占用很多CPU，所以不用担心。
		//一旦ros::ok()返回FALSE，ros::spin()就会立刻跳出自循环。


	  return 0;
	}
编译发布器和订阅器
	在beginner_tutorials 的 CMakeLists.txt文件末尾加入几条语句:
		include_directories(include ${catkin_INCLUDE_DIRS})
		add_executable(talker src/talker.cpp)
		target_link_libraries(talker ${catkin_LIBRARIES})
		add_dependencies(talker beginner_tutorials_generate_messages_cpp)	//为可执行文件添加对生成的消息文件的依赖
		add_executable(listener src/listener.cpp)
		target_link_libraries(listener ${catkin_LIBRARIES})
		add_dependencies(listener beginner_tutorials_generate_messages_cpp)	
	回到 catkin_ws
	catkin_make
		生成两个可执行文件, talker 和 listener, 默认存储到devel space目录
		具体是在catkin_ws/devel/lib/beginner_tutorials/里
		注意这里的 订阅器和发布器 都没有 使用到上面制作的 Num.msg,而是使用默认的
	测试:
	新一个终端
	roscore
	新一个终端
	export ROS_PACKAGE_PATH=/home/infortech-ubuntu-1404/1tb_disk/other/ROS/catkin_ws:$ROS_PACKAGE_PATH
	cd /home/infortech-ubuntu-1404/1tb_disk/other/ROS/catkin_ws
	source ./devel/setup.bash
	rosrun beginner_tutorials talker	
	新一个终端
	export ROS_PACKAGE_PATH=/home/infortech-ubuntu-1404/1tb_disk/other/ROS/catkin_ws:$ROS_PACKAGE_PATH
	cd /home/infortech-ubuntu-1404/1tb_disk/other/ROS/catkin_ws
	source ./devel/setup.bash
	rosrun beginner_tutorials listener
	新一个终端
	通过 rqt_graph 可以查看到 topic  订阅器和发布器 的关系
编写python格式的  订阅器和发布器
	roscd beginner_tutorials
	mkdir scripts
	cd scripts
	wget https://raw.github.com/ros/ros_tutorials/kinetic-devel/rospy_tutorials/001_talker_listener/talker.py	
		//懒得写,直接下哉 talker 脚本
	chmod +x talker.py	//改权限
	tacker脚本内容:
		#!/usr/bin/env python	
		import rospy				//导入rospy客户端库
		from std_msgs.msg import String		//导入std_msgs.msg 重用std_msgs/String消息类型
		
		def talker():
		    pub = rospy.Publisher('chatter', String, queue_size=10)
				//表示节点发布chatter话题，使用String字符类型
				//实际上就是类std_msgs.msg.String
				//queue_size表示队列的大小,如果队列消息处理不够快，就会丢弃旧的消息。
		    rospy.init_node('talker', anonymous=True)
				//初始化节点，开始跟ROS master通讯
				//保持节点名称在网络中是唯一的，不能包含斜杠"/"
		    rate = rospy.Rate(10) # 10hz
				//创建Rate对象，与sleep()函数结合使用，控制话题消息的发布频率。
				//10hz表示每秒发布10次
		    while not rospy.is_shutdown():
		        hello_str = "hello world %s" % rospy.get_time()
		        rospy.loginfo(hello_str)
		        pub.publish(hello_str)		
		        rate.sleep()
				//loop 是rospy的标准结构，检查rospy.is_shutdown()标识没返回值就会一直运行。
				//rospy.is_shutdown()返回false就会退出（例如按下Ctrl-C）
				//执行pub.publish(String(str))，在chatter话题发布String消息
				//rate.sleep()通过睡眠来，保持消息发送频率
				//rospy.loginfo(str)函数在屏幕输出调试信息，同时写入到节点日志文件和rosout节点
				//rosout节点对于调式来说是便利的方式。可以通过rqt_console查看调式信息。
		if __name__ == '__main__':
		    try:
		        talker()
		    except rospy.ROSInterruptException:
		        pass
				//这个会捕获rospy.ROSInterruptException异常，
				//当按下Ctrl-C或节点关闭的话，即使在rospy.sleep()和rospy.Rate.sleep()函数里都会抛出异常
	wget https://raw.github.com/ros/ros_tutorials/kinetic-devel/rospy_tutorials/001_talker_listener/listener.py
		//直接下哉 listener 脚本	
	chmod +x listener.py
	listener脚本内容:
		#!/usr/bin/env python	
		import rospy
		from std_msgs.msg import String
		
		def callback(data):
		    rospy.loginfo(rospy.get_caller_id() + 'I heard %s', data.data)
		
		def listener():

		    rospy.init_node('listener', anonymous=True)
			//初始化节点
			//ROS要求每个节点要有唯一名称，如果相同的名称，就会中止之前同名的节点。
			//调用rospy.init_node() ，增加anonymous=True关键词参数。
			//anonymous=True标识就会告诉rospy，要生成一个唯一的节点名称，因此你可以有多个listener.py同时运行	
		    rospy.Subscriber('chatter', String, callback)
			//节点订阅话题chatter，消息类型是 std_msgs.msgs.String
		    rospy.spin()
			//rospy.spin()简单保持你的节点一直运行，直到程序关闭
			//不像roscpp，rospy.spin()不影响到订阅的回调函数，因为他们有自己的独立线程。		
		if __name__ == '__main__':
		    listener()
	编译
	暂时不需要修改 CMakeLists.txt
	回到 catkin_ws
	catkin_make	
	测试:
	新一个终端
	roscore
	新一个终端
	export ROS_PACKAGE_PATH=/home/infortech-ubuntu-1404/1tb_disk/other/ROS/catkin_ws:$ROS_PACKAGE_PATH
	cd /home/infortech-ubuntu-1404/1tb_disk/other/ROS/catkin_ws
	source ./devel/setup.bash
	rosrun beginner_tutorials talker.py	
	新一个终端
	export ROS_PACKAGE_PATH=/home/infortech-ubuntu-1404/1tb_disk/other/ROS/catkin_ws:$ROS_PACKAGE_PATH
	cd /home/infortech-ubuntu-1404/1tb_disk/other/ROS/catkin_ws
	source ./devel/setup.bash
	rosrun beginner_tutorials listener.py
	新一个终端
	通过 rqt_graph 可以查看到 topic  订阅器和发布器 的关系
	由于 使用py时, 添加了anonymous=True标识, 所以talker listener 分别是独立分配的标示
	而不是 C++ 例子那 简单的 /talker /listener 标识
编写简单 服务 和 客户端 (C++)
	使用上述建立的 AddTwoInts.srv 文件
	创建server节点
	在 beginner_tutorials/src 添加 add_two_ints_server.cpp 	
	内容:
		#include "ros/ros.h"
		#include "beginner_tutorials/AddTwoInts.h"	
			//从 AddTwoInts.srv 文件 编译出来的 AddTwoInts.h
	
		bool add(beginner_tutorials::AddTwoInts::Request  &req,
		         beginner_tutorials::AddTwoInts::Response &res)
		{
		  res.sum = req.a + req.b;
		  ROS_INFO("request: x=%ld, y=%ld", (long int)req.a, (long int)req.b);
		  ROS_INFO("sending back response: [%ld]", (long int)res.sum);
		  return true;
		}
		
		int main(int argc, char **argv)
		{
		  ros::init(argc, argv, "add_two_ints_server");
		  ros::NodeHandle n;
		
		  ros::ServiceServer service = n.advertiseService("add_two_ints", add);
		  ROS_INFO("Ready to add two ints.");
		  ros::spin();
	
		  return 0;
		}
	编写Client节点
	在 beginner_tutorials/src 添加 add_two_ints_client.cpp 	
	内容:
		#include "ros/ros.h"
		#include "beginner_tutorials/AddTwoInts.h"
		#include <cstdlib>
		
		int main(int argc, char **argv)
		{
		  ros::init(argc, argv, "add_two_ints_client");
		  if (argc != 3)
		  {
		    ROS_INFO("usage: add_two_ints_client X Y");
		    return 1;
		  }
		
		  ros::NodeHandle n;
		  ros::ServiceClient client = n.serviceClient<beginner_tutorials::AddTwoInts>("add_two_ints");
		  beginner_tutorials::AddTwoInts srv;
		  srv.request.a = atoll(argv[1]);
		  srv.request.b = atoll(argv[2]);
		  if (client.call(srv))
			//如果service调用成功，call()函数将返回true，srv.response里面的值将是合法的值。
			//如果调用失败，call()函数将返回false，srv.response里面的值将是非法的
		  {
		    ROS_INFO("Sum: %ld", (long int)srv.response.sum);
		  }
		  else
		  {
		    ROS_ERROR("Failed to call service add_two_ints");
		    return 1;
		  }
		
		  return 0;
		}
	编译:
	beginner_tutorials 的 CMakeLists.txt文件:
		add_executable(add_two_ints_server src/add_two_ints_server.cpp)
		target_link_libraries(add_two_ints_server ${catkin_LIBRARIES})
		add_dependencies(add_two_ints_server beginner_tutorials_gencpp)
		
		add_executable(add_two_ints_client src/add_two_ints_client.cpp)
		target_link_libraries(add_two_ints_client ${catkin_LIBRARIES})
		add_dependencies(add_two_ints_client beginner_tutorials_gencpp)		
	回到 catkin_ws
	catkin_make
		//生成两个可执行程序"add_two_ints_server"和"add_two_ints_client"，
		//这两个可执行程序默认被放在你的devel space下的包目录下，默认为~/catkin_ws/devel/lib/share/。
	新一个终端
	roscore
	新一个终端 打开服务
	export ROS_PACKAGE_PATH=/home/infortech-ubuntu-1404/1tb_disk/other/ROS/catkin_ws:$ROS_PACKAGE_PATH
	cd /home/infortech-ubuntu-1404/1tb_disk/other/ROS/catkin_ws
	source ./devel/setup.bash
	rosrun beginner_tutorials add_two_ints_server
	新一个终端 申请服务
	export ROS_PACKAGE_PATH=/home/infortech-ubuntu-1404/1tb_disk/other/ROS/catkin_ws:$ROS_PACKAGE_PATH
	cd /home/infortech-ubuntu-1404/1tb_disk/other/ROS/catkin_ws
	source ./devel/setup.bash
	rosrun beginner_tutorials add_two_ints_client  60 30
	通过 rqt_graph 观察
	由于请求服务的行为 很快结束了,所以 toppic 用后就关闭,所以没有看到 topic
编写简单 服务 和 客户端 (python)
	使用上述建立的 AddTwoInts.srv 文件
	创建server节点
	在beginner_tutorials包创建scripts/add_two_ints_server.py文件
	内容:
		#!/usr/bin/env python

		from beginner_tutorials.srv import *
		import rospy
		
		def handle_add_two_ints(req):
		    print "Returning [%s + %s = %s]"%(req.a, req.b, (req.a + req.b))
		    return AddTwoIntsResponse(req.a + req.b)
		
		def add_two_ints_server():
		    rospy.init_node('add_two_ints_server')
		    s = rospy.Service('add_two_ints', AddTwoInts, handle_add_two_ints)
		    print "Ready to add two ints."
		    rospy.spin()
		
		if __name__ == "__main__":
		    add_two_ints_server()
	chmod +x scripts/add_two_ints_server.py		//改权限	
	编写Client节点
	在beginner_tutorials包创建scripts/add_two_ints_client.py
	内容
		#!/usr/bin/env python

		import sys
		import rospy
		from beginner_tutorials.srv import *
		
		def add_two_ints_client(x, y):
		    rospy.wait_for_service('add_two_ints')
		    try:
		        add_two_ints = rospy.ServiceProxy('add_two_ints', AddTwoInts)
		        resp1 = add_two_ints(x, y)
		        return resp1.sum
		    except rospy.ServiceException, e:
		        print "Service call failed: %s"%e
		
		def usage():
		    return "%s [x y]"%sys.argv[0]
		
		if __name__ == "__main__":
		    if len(sys.argv) == 3:
		        x = int(sys.argv[1])
		        y = int(sys.argv[2])
		    else:
		        print usage()
		        sys.exit(1)
		    print "Requesting %s+%s"%(x, y)
		    print "%s + %s = %s"%(x, y, add_two_ints_client(x, y))
	编译测试
	回到 catkin_ws
	catkin_make
		//生成两个可执行程序"add_two_ints_server"和"add_two_ints_client"，
		//这两个可执行程序默认被放在你的devel space下的包目录下，默认为~/catkin_ws/devel/lib/share/。
	新一个终端
	roscore
	新一个终端 打开服务
	export ROS_PACKAGE_PATH=/home/infortech-ubuntu-1404/1tb_disk/other/ROS/catkin_ws:$ROS_PACKAGE_PATH
	cd /home/infortech-ubuntu-1404/1tb_disk/other/ROS/catkin_ws
	source ./devel/setup.bash
	rosrun beginner_tutorials add_two_ints_server.py
	新一个终端 申请服务
	export ROS_PACKAGE_PATH=/home/infortech-ubuntu-1404/1tb_disk/other/ROS/catkin_ws:$ROS_PACKAGE_PATH
	cd /home/infortech-ubuntu-1404/1tb_disk/other/ROS/catkin_ws
	source ./devel/setup.bash
	rosrun beginner_tutorials add_two_ints_client.py  60 30
	通过 rqt_graph 观察
	由于请求服务的行为 很快结束了,所以 toppic 用后就关闭,所以没有看到 topic
录制和回访 topic 内容:
	打开 乌龟窗口例子 ,三个终端分别执行:
	roscore		//启动 master
	rosrun turtlesim turtlesim_node 
	rosrun turtlesim turtle_teleop_key
	新终端列出存在的的 话题topic
	rostopic list -v
		可以看到很多 topic 但很多都是ros系统本身的
		然而在 rqt_graph 上我们就只看到 /turtle1/cmd_vel
	录制
	cd ~/1tb_disk/other/ROS/catkin_ws
	mkdir bagfiles
	cd bagfiles/
	rosbag record -a	//-a 表示录制所有topic上的内容
		[ INFO] [1528423704.079876539]: Recording to 2018-06-08-10-08-24.bag.
		[ INFO] [1528423704.080102511]: Subscribing to /turtle1/color_sensor
		[ INFO] [1528423704.082482935]: Subscribing to /turtle1/cmd_vel		//控制乌龟动作的topic
		[ INFO] [1528423704.084757144]: Subscribing to /rosout
		[ INFO] [1528423704.087093686]: Subscribing to /rosout_agg
		[ INFO] [1528423704.089288376]: Subscribing to /turtle1/pose
	操作乌龟一段时间后 ctrl+c 推出录制
	查看录制信息
	rosbag info 2018-06-08-10-08-24.bag 	//查看录制文件 2018-06-08-10-08-24.bag
		path:        2018-06-08-10-08-24.bag
		version:     2.0
		duration:    2:38s (158s)
		start:       Jun 08 2018 10:08:24.09 (1528423704.09)
		end:         Jun 08 2018 10:11:02.70 (1528423862.70)
		size:        1.3 MB
		messages:    19876
		compression: none [2/2 chunks]
		types:       geometry_msgs/Twist [9f195f881246fdfa2798d1d3eebca84a]
		             rosgraph_msgs/Log   [acffd30cd6b6de30f120938c17c593fb]
		             turtlesim/Color     [353891e354491c51aabe32df673fb446]
		             turtlesim/Pose      [863b248d5016ca62ea2e895ae5265cf9]
		topics:      /rosout                    4 msgs    : rosgraph_msgs/Log   (2 connections)
		             /turtle1/cmd_vel          78 msgs    : geometry_msgs/Twist
		             /turtle1/color_sensor   9897 msgs    : turtlesim/Color    
		             /turtle1/pose           9897 msgs    : turtlesim/Pose
	rosbag play 2018-06-08-10-08-24.bag	//回播
		注意 录制是以秒为单位录制视频的
		回播的时候以原本速度执行, 指定的同样时刻向所有指定 topic, 发布录制了的 信息message
		这样 我们会看到 乌龟又执行了一遍我们执行过的操作
		rqt_graph 会看到一个新节点 play 同样接到 /turtle1/cmd_vel 给 乌龟发布操作命令
	rosbag play -r 4 <your bagfile> 	//4倍速回播
		意味着 发布消息的间隔是原来的 1/4, 指令执行时间是原来1/4,
		然而 指令执行的速度加快了!!
		这样乌龟运动的轨迹就变了, 与原来的轨迹完全不一样
		也就是说 录制的只是 话题中消息发布的时机和间隔
		回访只是 重复给话题发布消息的过程
		斌不是真的录制整个运动过程		
	rosbag record -O subset /turtle1/cmd_vel /turtle1/pose
		录制指定的 topic,生成指定的录制文件 subset.bag
系统错误分析工具
	如果发现被一个编译或者通信之类的问题困扰，可以尝试运行roswtf检查一下。
	roscd		//直接跳到 ros根目录: /opt/ros/indigo
	roswtf		//系统问题检查工具
	详细百度
进阶  URDF RViz actionlib navigation MoveIt TF程序包 :https://www.ncnynl.com/archives/201608/518.html
创建 程序包 ROS package(catkin) 第二种方式,不同于beginner_tutorials
	回到 catkin_ws
	mkdir -p src/foobar
	cd src/foobar
	手动建立 foobar/package.xml
	内容:
		<package>
		  <name>foobar</name>
		  <version>1.2.4</version>
		  <description>
		  This package provides foo capability.
		  </description>
		  <maintainer email="foobar@foo.bar.willowgarage.com">PR-foobar</maintainer>
		  <license>BSD</license>
		
		  <buildtool_depend>catkin</buildtool_depend>
		
		  <build_depend>roscpp</build_depend>
		  <build_depend>std_msgs</build_depend>
		
		  <run_depend>roscpp</run_depend>
		  <run_depend>std_msgs</run_depend>
		</package>	
	rospack find foobar
		已经包含配置文件(package.xml)，ROS能够找到它:
	手动建立foobar/CMakeLists.txt
	内容:
		cmake_minimum_required(VERSION 2.8.3)
		project(foobar)
		find_package(catkin REQUIRED roscpp std_msgs)
		catkin_package()
	这样子 一个程序包的创建就算是 创建好了
管理系统依赖项
	ROS packages有时会需要操作系统提供一些外部函数库，这些函数库就是所谓的“系统依赖项”。
	在一些情况下，这些依赖项并没有被系统默认安装，因此，ROS提供了一个工具rosdep来下载并安装所需系统依赖项。
	ROS packages必须在配置文件中声明他们需要哪些系统依赖项。
	例子:
	roscd turtlesim		来到乌龟项目程序包
	rosdep install turtlesim	安装乌龟项目 需要用到的系统依赖项
		因为我装ros是全面安装的,所以已经安装了,所以出现
		All required rosdeps installed successfully
Roslaunch 大型项目中使用技巧
	这个没有实际demo操作真不好弄！！
	ROS采用rosrun命令可以启动一个节点，如果需要同时启动节点管理器（master）和多个节点，就需要采用launch文件来配置
	

SLAM
	入门书籍，简单实现及代码《SLAM for Dummies》《STATE ESTIMATION FOR ROBOTICS》
	作者Joan Sola关于Graph-SLAM的教程，包含位姿变换、传感器模型、图优化以及SLAM中的稀疏性求解《Course on SLAM》	
	
	国内SLAMTEC-思岚科技公司将为大家分享SLAM算法入门必备资料

	SLAM涵盖的东西比较多，分为前端和后端两大块。
	前端主要是研究相邻帧的拼接，又叫配准。根据传感器不一样，有激光点云、图像、RGB-D拼接几种，
	其中图像配准中又分基于稀疏特征(Sparse)的和稠密(Dense)的两种。
	后端主要是研究地图拼接(前端)中累积误差的校正，主流就两种，
	基于概率学理论的贝叶斯滤波器（EKF，PF）以及基于优化的方法。
	EKF已经用得很少了，PF也就在2D地SLAM（Gmapping）中用得多，大多还是用优化的方法在做
	
	尝试玩一些现有的SLAM包，推荐两个地方，一个是 OpenSLAM：https://openslam.org/，里面有各种SLAM包，主流的SLAM算法，在这一般都有源码。
	另外一个就是ROS了，里面有很多现成的SLAM包，像Gmapping，RGB-D SLAM，上手非常快，甚至你没有任何设备，
	你也可以利用ROS中的仿真环境（如Gazebo）跑。建议先试试Gmapping，百度上有很多中文教程，

	Thrun的《probabilistic robotics》，了解下概率学是如何解决机器人中的问题的，
	关键学习贝叶斯滤波，也是就是贝叶斯公式在各个问题(定位，SLAM)中的应用。
	另外，优化的话，建议先把最小二乘优化中给弄透彻，数学推导要会，因为很多问题，最后都是归结到最小二乘优化，
	然后就是梯度下降、求Jacobian之类的。
	
	Kitti图库，可以做simulation：http://www.cvlibs.net/datasets/kitti/

	苏黎世理工学习练习excise3看完后可以使用Javier Civera 的程序进行试手，注意对calibration的调整
	http://webdiis.unizar.es/~jcivera/code/1p-ransac-ekf-monoslam.html
	对于Javier Civera的1p RANSAC-monoSLAM有一定了解了，可以试试用SURF去实现
	南理工论文可以参考 http://cdmd.cnki.com.cn/Article/CDMD-10288-1012319519.htm

	RGB-D SLAM Dataset and Benchmark：http://cvpr.in.tum.de/data/datasets/rgbd-dataset
	慕尼黑工业大学，还有其他的数据库，如单目视觉里程计数据库，详见http://vision.in.tum.de/data/datasets

	Monocular SLAM：http://vision.ia.ac.cn/Students/gzp/monocularslam.html
	The research in monocular SLAM technology is mainly based on the EKF(Extended Kalman Filter) SLAM approaches.

	MRPT：http://www.mrpt.org/ the mobile robot programming toolkit非常好的东西

	PTAM：http://www.robots.ox.ac.uk/~gk/PTAM/
	libCVD：http://www.edwardrosten.com/index.html
	编译PTAM：http://www.fx114.net/qa-207-77156.aspx
	windows下编译PTAM：http://blog.csdn.net/cgf_909/article/details/24457771

	ORB_SLAM：http://webdiis.unizar.es/~raulmur/orbslam/

	LSD_SLAM：http://www.cnblogs.com/hitcm/category/763753.html

	主流开源SLAM方案
	摄像头方案
		稀疏法(特征点):
			ORB-SLAM(单目，双目，RGBD)[1](ORB-SLAM: a Versatile and Accurate Monocular SLAM System中文翻译)[2]
			PTAM(单目)[3]
			MonoSLAM(单目)[4]
		半稠密法:
			LSD-SLAM(单目，双目，RGBD)[5]
			SVO(单目, 仅VO)[6]
		稠密法:
			DTAM(RGBD): Paper: [7] Open source code:[8]
			Elastic Fusion(RGBD): Open source code:[9]
			Kintinous(RGBD):Open source code: [10]
			DVO: Open source code: [11]
			RGBD-SLAM-V2: Open source code: [12]
		其他
			ScaViSLAM: Open source code [13]
	激光传感器方案：
		Hector SLAM[14]
		Gmapping [15]
	视觉(Visual)与IMU融合(VI)
		Release of OKVIS: Open Keyframe-based Visual Inertial SLAM[16]
	工具
		g2o:[17]
		ceres: [18]
算法论文更新 https://arxiv.org/


slam ros 模拟 https://blog.csdn.net/sinat_31425585/article/details/52856593

https://www.cnblogs.com/TooyLee/p/7736732.html
https://blog.csdn.net/qq_29797957/article/details/78089420
https://www.ncnynl.com/archives/201707/1787.html
https://blog.csdn.net/sinat_31425585/article/details/52856593


装ros
sudo apt-get install ros-kinetic-desktop-full  
sudo rosdep init  
rosdep update 
sudo apt-get install python-rosinstall  
sudo apt-get install python-rosinstall-generator python-wstool build-essential 
echo "source /opt/ros/kinetic/setup.bash" >> ~/.bashrc    
source ~/.bashrc  
	使环境变量设置立即生效
export | grep ROS

装依赖
sudo apt-get install ros-kinetic-turtlebot-bringup \
ros-kinetic-turtlebot-create ros-kinetic-openni-* \
ros-kinetic-openni2-* ros-kinetic-freenect-* ros-kinetic-usb-cam \
ros-kinetic-laser-* \
ros-kinetic-audio-common \
ros-kinetic-slam-gmapping \
ros-kinetic-joystick-drivers python-rosinstall \
ros-kinetic-orocos-kdl ros-kinetic-python-orocos-kdl \
python-setuptools ros-kinetic-dynamixel-motor \
libopencv-dev python-opencv ros-kinetic-vision-opencv \
ros-kinetic-depthimage-to-laserscan \
ros-kinetic-turtlebot-teleop ros-kinetic-move-base \
ros-kinetic-map-server ros-kinetic-fake-localization \
ros-kinetic-amcl git subversion mercurial

<br>其中，ros-kinetic-hokuyo-node（北阳激光雷达）、gstreamer0.10-pocketsphinx和ros-kinetic-pocketsphinx（语音识别库）<br>
和ros-kinetic-arbotix-*（模拟器）在ros kinetic版本中没有，如果用到需要源码编译

sudo aptitude install ros-kinetic-turtlebot-bringup \
ros-kinetic-turtlebot-create-desktop ros-kinetic-openni-* \
ros-kinetic-openni2-* ros-kinetic-freenect-* ros-kinetic-usb-cam \
ros-kinetic-laser-* \
ros-kinetic-audio-common \
ros-kinetic-slam-gmapping \
ros-kinetic-joystick-drivers python-rosinstall \
ros-kinetic-orocos-kdl ros-kinetic-python-orocos-kdl \
python-setuptools ros-kinetic-dynamixel-motor-* \
libopencv-dev python-opencv ros-kinetic-vision-opencv \
ros-kinetic-depthimage-to-laserscan \
ros-kinetic-turtlebot-teleop ros-kinetic-move-base \
ros-kinetic-map-server ros-kinetic-fake-localization \
ros-kinetic-amcl git subversion mercurial


sudo apt-get remove ros-kinetic-turtlebot-bringup \
 ros-kinetic-openni-* \
ros-kinetic-openni2-* ros-kinetic-freenect-* ros-kinetic-usb-cam \
ros-kinetic-laser-* \
ros-kinetic-audio-common \
ros-kinetic-slam-gmapping \
ros-kinetic-joystick-drivers python-rosinstall \
ros-kinetic-orocos-kdl ros-kinetic-python-orocos-kdl \
python-setuptools ros-kinetic-dynamixel-motor-* \
libopencv-dev python-opencv ros-kinetic-vision-opencv \
ros-kinetic-depthimage-to-laserscan \
ros-kinetic-map-server ros-kinetic-fake-localization \
ros-kinetic-amcl git subversion mercurial









装 rbx1 模拟机器人
cd ~~/catkin_ws/src
git clone https://github.com/pirobot/rbx1.git
cd ..
catkin_make

装 navigation 不推荐(多错误,未解决)
sudo apt-get install libmrpt-dev  
sudo apt-get install libbullet-dev
cd ~~/catkin_ws/src
git clone https://github.com/ros-planning/navigation.git
cd ..
catkin_make

sudo apt-get install ros-kinetic-navigation
sudo apt-get install ros-kinetic-gmapping
sudo apt-get install ros-kinetic-hector-mapping

装ARBOTIX模拟器
cd ~~/catkin_ws/src
git clone  https://github.com/vanadiumlabs/arbotix_ros
cd ..
catkin_make


终端1 启动master
	roscore	
终端2 启动rbx1
	source ros/catkin_ws/devel/setup.bash
	roslaunch rbx1_bringup fake_turtlebot.launch 
终端3 启动可视化工具
	source ros/catkin_ws/devel/setup.bash
	rosrun rviz rviz -d `rospack find rbx1_nav`/sim.rviz 
终端4 规划运动
	source ros/catkin_ws/devel/setup.bash
	rostopic pub -r 10 /cmd_vel geometry_msgs/Twist '{linear: {x: 0.2, y: 0, z: 0}, angular: {x: 0, y: 0, z: 0.5}}'


sudo aptitude install libsdl-image1.2-dev  
sudo aptitude install ros-indigo-navigation  
sudo aptitude install ros-indigo-map-server  

slam模拟
终端1 启动master
	roscore	
终端2 启动rbx1
	source ros/catkin_ws/devel/setup.bash
	roslaunch rbx1_bringup fake_turtlebot.launch 		
终端3 蒙特卡洛粒子滤波算法节点，同时加载地图	
	source ros/catkin_ws/devel/setup.bash
	roslaunch rbx1_nav fake_amcl.launch map:=test_map.yaml  
	启动 fake_amcl.launch 发现 move_base 出错未解决!!
	ERROR: cannot launch node of type [move_base/move_base]: can't locate node [move_base] in package [move_base]
	https://blog.csdn.net/Richard_2018/article/details/80390369
I also got an ERROR saying there is no move_base/move_base to launch. I found move_base (both the package folder AND executable) in /opt/ros/kinetic/lib/ There were also move_base folders in catkin_ws/devel/lib/ and catkin_ws/devel/share/ However, these catkin_ws move_base folders were empty or incomplete due to a bad build or an incomplete build that I had attempted initially. The problem was caused by not cleaning up a bad build in my catkin_ws directories. I deleted move_base folders from catkin_ws/devel/ lib and share, along with many other navigation related directories that should not have been there. I then could launch move_base.launch
	然后安装今天的主角：
		sudo apt-get install aptitude
	然后用aptitude替代apt-get来安装软件。
		sudo aptitude install build-essential

终端4 启动可视化工具
	source ros/catkin_ws/devel/setup.bash
	rosrun rviz rviz -d `rospack find rbx1_nav`/sim.rviz 
	看不到maping: map topic下拉选择

gmapping
https://blog.csdn.net/qq_24747993/article/details/53759308	
https://blog.csdn.net/qq_24747993/article/details/53759308

最后不知道是否成功运行了slam,运行结果比较不满意,但是也暂告一段落









20180618
moveit 详细学习!!
https://blog.csdn.net/crazyquhezheng/article/details/42840955

https://blog.csdn.net/gpeng832/article/details/72853709?locationNum=2&fps=1

https://blog.csdn.net/akunainiannian/article/details/44985423

https://blog.csdn.net/BBZZ2/article/details/51375623 重点
http://www.cnblogs.com/gaoxiang12/default.html?page=3 RGB SLAM 的重要参考
博客资料
    古月居的一篇博客(2013-04-16 21:56)： http://blog.csdn.net/hcx25909/article/details/8811313#t1

    古月居的CSDN博客系列(2012.7-2013.9)： http://blog.csdn.net/hcx25909/article/category/1191901/1

    小菜鸟上校的CSDN博客（2013.10-11）(20篇)： http://blog.csdn.net/xiaocainiaoshangxiao/article/category/1710543/2 

    周学伟的博客园(2016.3-2016.5) :http://www.cnblogs.com/zxouxuewei/tag/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%B3%BB%E7%BB%9FROS%E7%9A%84%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/

    永顿007的新浪博客(2011.01-09): http://blog.sina.com.cn/s/articlelist_1712413141_7_1.html
    小庄的博客(2014.10-2015.11): http://blog.csdn.net/junshen1314/article/category/2614195










201806026
urdf 实践 :http://www.guyuehome.com/column/ros-explore/page/3
小车实践
环境变量重定位到 catkin_ws 
	source ~/ros/catkin_ws/devel/setup.bash
在 catkin_ws 建立一个新的程序包 smartcar
	cd ~/ros/catkin_ws/src 
	catkin_create_pkg smartcar std_msgs rospy roscpp urdf
在 ~/ros/catkin_ws/src/smartcar下,创建 urdf,和launch文件夹,并分别添加以下文件:
/***** smartcar.urdf  ****/
<?xml version="1.0"?>
<robot name="smartcar">
  <link name="base_link">
    <visual>
      <geometry>
        <box size="0.25 .16 .05"/>
	</geometry>
	<origin rpy="0 0 1.57075" xyz="0 0 0"/>
    <material name="blue">
		<color rgba="0 0 .8 1"/>
	</material>
    </visual>
 </link>
 
 <link name="right_front_wheel">
    <visual>
      <geometry>
        <cylinder length=".02" radius="0.025"/>
      </geometry>
      <material name="black">
        <color rgba="0 0 0 1"/>
      </material>
    </visual>
  </link>
 
  <joint name="right_front_wheel_joint" type="continuous">
    <axis xyz="0 0 1"/>
    <parent link="base_link"/>
    <child link="right_front_wheel"/>
    <origin rpy="0 1.57075 0" xyz="0.08 0.1 -0.03"/>
    <limit effort="100" velocity="100"/>
    <joint_properties damping="0.0" friction="0.0"/>
  </joint>
 
  <link name="right_back_wheel">
    <visual>
      <geometry>
        <cylinder length=".02" radius="0.025"/>
      </geometry>
      <material name="black">
        <color rgba="0 0 0 1"/>
      </material>
    </visual>
  </link>
 
  <joint name="right_back_wheel_joint" type="continuous">
    <axis xyz="0 0 1"/>
    <parent link="base_link"/>
    <child link="right_back_wheel"/>
    <origin rpy="0 1.57075 0" xyz="0.08 -0.1 -0.03"/>
    <limit effort="100" velocity="100"/>
    <joint_properties damping="0.0" friction="0.0"/>
 </joint>
 
 <link name="left_front_wheel">
    <visual>
      <geometry>
        <cylinder length=".02" radius="0.025"/>
      </geometry>
      <material name="black">
        <color rgba="0 0 0 1"/>
      </material>
    </visual>
  </link>
 
  <joint name="left_front_wheel_joint" type="continuous">
    <axis xyz="0 0 1"/>
    <parent link="base_link"/>
    <child link="left_front_wheel"/>
    <origin rpy="0 1.57075 0" xyz="-0.08 0.1 -0.03"/>
    <limit effort="100" velocity="100"/>
    <joint_properties damping="0.0" friction="0.0"/>
  </joint>
 
  <link name="left_back_wheel">
    <visual>
      <geometry>
        <cylinder length=".02" radius="0.025"/>
      </geometry>
      <material name="black">
        <color rgba="0 0 0 1"/>
      </material>
    </visual>
  </link>
 
  <joint name="left_back_wheel_joint" type="continuous">
    <axis xyz="0 0 1"/>
    <parent link="base_link"/>
    <child link="left_back_wheel"/>
    <origin rpy="0 1.57075 0" xyz="-0.08 -0.1 -0.03"/>
    <limit effort="100" velocity="100"/>
    <joint_properties damping="0.0" friction="0.0"/>
  </joint>
 
  <link name="head">
    <visual>
      <geometry>
        <box size=".02 .03 .03"/>
      </geometry>
	  <material name="white">
		  <color rgba="1 1 1 1"/>
	  </material>
    </visual>
  </link>
 
  <joint name="tobox" type="fixed">
    <parent link="base_link"/>
    <child link="head"/>
    <origin xyz="0 0.08 0.025"/>
  </joint>
</robot> 

/******* display.launch *******/
    <launch>
    	<arg name="model" />
    	<arg name="gui" default="False" />
    	<param name="robot_description" textfile="$(find smartcar)/urdf/smartcar.urdf)" />
    	<param name="use_gui" value="$(arg gui)"/>
    	<node name="joint_state_publisher" pkg="joint_state_publisher" type="joint_state_publisher" />
    	<node name="robot_state_publisher" pkg="robot_state_publisher" type="state_publisher" />
    	<node name="rviz" pkg="rviz" type="rviz" args="-d $(find urdf_tutorial)/urdf.rviz" required="true" />
    </launch>
检查:
	rospack find smartcar //可以找到
	cd ~/ros/catkin_ws && catkin_make  //编译一次
	roscd smartcar	//这样就可以找到并跳到 samrtcar 软件包的路径
运行一下:
	roscd samrtcar/
	roslaunch smartcar display.launch model:=urdf/smartcar.urdf gui:=true
	手动 ADD 选择 robotmodel 便可以看到小车
	手动 ADD 选择 tf 便可以看到 link 的坐标系
问题总结:
	发现 rviz 不能正常显示小车什么的, Fixed Frame [map] does not exist
		原因是 Global Options->Fixed Frame: map ,这时历史配置问题,
		smartcar.urdf里并没有马屁,把 map 换回 base_link 即可
	No transform from [tyer_back_left] to [base_link] 这是urdf 文件的格式问题
		urdf只支持ascii编码，不支持Unicode
	No tf data.  Actual error: Fixed Frame [base_link] does not exist 尽量同一个终端执行










20180626
参考:
	http://gazebosim.org/ 
	https://www.cnblogs.com/damens/p/6459581.html
gazebo 安装 使用 :
安装: sudo apt-get install ros-kinetic-simulators
更新环境变量: source /opt/ros/kinetic/setup.bash
测试gazebo程序: roslaunch gazebo_ros empty_world.launch
	rosrun gazebo_ros spawn_model -file /home/william/ros/catkin_ws/src/smartcar/urdf/test.urdf -urdf -z 1 -model my_object1
/**** test.urdf ***/
<robot name="simple_box">
  <link name="my_box">
    <inertial>
      <origin xyz="2 0 0" />
      <mass value="1.0" />
      <inertia  ixx="1.0" ixy="0.0"  ixz="0.0"  iyy="100.0"  iyz="0.0"  izz="1.0" />
    </inertial>
    <visual>
      <origin xyz="2 0 1"/>
      <geometry>
        <box size="1 1 2" />
      </geometry>
    </visual>
    <collision>
      <origin xyz="2 0 1"/>
      <geometry>
        <box size="1 1 2" />
      </geometry>
    </collision>
  </link>
  <gazebo reference="my_box">
    <material>Gazebo/Blue</material>
  </gazebo>
</robot>










针对工业机器人的ros模块 ROS Industrial (ROS-I)
http://www.guyuehome.com/484
完整安装ros后还有补充安装
	sudo apt-get install ros-kinetic-industrial-core
安装UR机器人的ROS功能包集
	sudo apt-get install ros-kinetic-universal-robot
运行:
	终端1 看到机器人
	roslaunch ur_gazebo ur10.launch	//看到机器人
	终端2 我们让机械臂动起来,需要运行MoveIt运动规划的节点
	roslaunch ur10_moveit_config ur10_moveit_planning_execution.launch sim:=true
	终端3 运行rviz
	roslaunch ur10_moveit_config moveit_rviz.launch config:=true
	我们可以看到rivz中的机器人模型和gazebo中的机器人模型应该是一样的姿态。
	在rviz中，我们可以用鼠标拖动机器人的终端，然后点击planning标签页中的plan nad execute
	可以让机器人规划路径并且到达目标位置了，gazebo中的模型也会跟随变化。
	但是显然并不完整!!



TurtleBot  gazebo仿真实验: http://www.guyuehome.com/1809
TurtleBot2
安装所有turtlebot功能包
	sudo apt-get install ros-kinetic-turtlebot-*
启动gazebo仿真环境，并且加载Turtlebot机器人
	这里需要使用环境变量TURTLEBOT_GAZEBO_WORLD_FILE为仿真环境指定地图，否则会显示找不到地图的错误
	export TURTLEBOT_GAZEBO_WORLD_FILE="/opt/ros/kinetic/share/turtlebot_gazebo/worlds/playground.world"
	roslaunch turtlebot_gazebo turtlebot_world.launch
	然后看到gazebo 一个被打开了的场景
新终端打开rviz:
	rosrun rviz rviz	//不知道命令对不对
然后新终端打开键盘操纵:
	roslaunch turtlebot_teleop keyboard_teleop.launch
	然后键盘控制就能操作 机器人了
	但是 riviz 没有反应,只有gazebo有,未知如何解决

TurtleBot3
安装所有turtlebot功能包
	sudo apt-get install ros-kinetic-turtlebot3-*
启动gazebo仿真环境，并且加载Turtlebot机器人
	export TURTLEBOT3_MODEL=burger
	roslaunch turtlebot3_gazebo turtlebot3_world.launch
	然后看到gazebo 一个被打开了的场景
新终端打开slam 配置下的rviz:
	export TURTLEBOT3_MODEL=burger
	roslaunch turtlebot3_slam turtlebot3_slam.launch
	可以看到slam模拟场景
然后新终端打开键盘操纵:
	export TURTLEBOT3_MODEL=burger
	roslaunch turtlebot3_teleop turtlebot3_teleop_key.launch
	然后键盘控制就能操作 机器人了
	但是 riviz,gazebo 都有反应

TRAC-IK和KDL类似，也是一种基于数值解的运动学插件:
sudo apt-get install ros-kinetic-trac-ik-kinematics-plugin

IKFAST 一种基于解析算法的运动学插件
依赖:
sudo apt-get install libassimp-dev libavcodec-dev libavformat-dev libavformat-dev libboost-all-dev
sudo apt-get install libboost-date-time-dev libbullet-dev libfaac-dev libglew-dev libgsm1-dev liblapack-dev liblog4cxx-dev 
sudo apt-get install libmpfr-dev libode-dev libogg-dev libpcrecpp0v5 libpcre3-dev libqhull-dev libqt4-dev 
sudo apt-get install libsoqt-dev-common libsoqt4-dev libswscale-dev libswscale-dev libvorbis-dev libx264-dev libxml2-dev libxvidcore-dev
sudo apt-get install cmake g++ git ipython minizip python-dev python-h5py python-numpy python-scipy qt4-dev-tools
安装OpenSceneGraph-3.4:	ros/other
sudo apt-get install libcairo2-dev libjasper-dev libpoppler-glib-dev libsdl2-dev libtiff5-dev libxrandr-dev
git clone https://github.com/openscenegraph/OpenSceneGraph.git --branch OpenSceneGraph-3.4
cd OpenSceneGraph
mkdir build && cd build
cmake .. -DDESIRED_QT_VERSION=4
make -j$(nproc)
sudo make install
安装sympy	//不在python的虚拟环境,在直接环境安装,安装不成功:
		//Command "python setup.py egg_info" failed with error code 1 in /tmp/pip-build-rd5mlutx/sympy/
		//原来没有装 python2 对应的pip2
pip install --upgrade --user sympy==0.7.1
删除mpmath (并没有做这步)
sudo apt remove python-mpmath
安装IKFast功能包
sudo apt-get install ros-kinetic-moveit-kinematics
安装OpenRave
sudo apt-get install ros-kinetic-openrave
后面的是测试这个运动学插件过程: http://www.guyuehome.com/1921
创建collada文件
    export MYROBOT_NAME="marm"
    rosrun xacro xacro --inorder -o "$MYROBOT_NAME".urdf "$MYROBOT_NAME".xacro    
    rosrun collada_urdf urdf_to_collada "$MYROBOT_NAME".urdf "$MYROBOT_NAME".dae<br></font>
创建dae文件
    export IKFAST_PRECISION="5"
    cp "$MYROBOT_NAME".dae "$MYROBOT_NAME".backup.dae # create a backup of your full precision dae.
    rosrun moveit_kinematics round_collada_numbers.py "$MYROBOT_NAME".dae "$MYROBOT_NAME".dae "$IKFAST_PRECISION"
略...


moveit 系统地学习:
http://docs.ros.org/kinetic/api/moveit_tutorials/html/doc/getting_started/getting_started.html


现在用的的 kinetic 是 ros1, 后面还有个ros2 ,是一个使用dds, 不需要master的新架构


dynamixel的伺服
EtherCAT 通讯








20180627
http://www.guyuehome.com/1228
现在用的的 kinetic 是 ros1, 后面还有个ros2 ,是一个使用dds, 不需要master的新架构,更稳定面向产品开发
安装ROS2 第一个正式版——Ardent Apalone
添加软件源
	sudo apt update && sudo apt install curl
	curl http://repo.ros2.org/repos.key | sudo apt-key add -
	sudo sh -c 'echo "deb [arch=amd64,arm64] http://repo.ros2.org/ubuntu/main xenial main" > /etc/apt/sources.list.d/ros2-latest.list'
安装ROS2
	sudo apt-get update
	sudo apt install `apt list ros-ardent-* 2> /dev/null | grep "/" | awk -F/ '{print $1}' | grep -v -e ros-ardent-ros1-bridge -e ros-ardent-turtlebot2- | tr "\n" " "`
	以上安装命令排除了ros-ardent-ros1-bridge和ros-ardent-turtlebot2-*等功能包，这些功能包需要依赖ROS1，可以在后续单独安装
设置环境变量
	source /opt/ros/ardent/setup.bash
安装python3-argcomplete >= 0.8.5	//不在python的虚拟环境,在直接环境安装,安装不成功:
	sudo pip3 install argcomplete
	source /opt/ros/ardent/share/ros2cli/environment/ros2-argcomplete.bash
	这样可完成诸如ros2这样的命令行工具
ROS2默认使用的RMW是FastRPTS，我们也可以通过以下环境变量将默认RMW修改为OpenSplice
	RMW_IMPLEMENTATION=rmw_opensplice_cpp
安装依赖ROS1的功能包
	sudo apt install ros-ardent-ros1-bridge  ros-ardent-turtlebot2-*





补充安装 包:
sudo apt-get install ros-kinetic-pr2-desktop
sudo apt-get install ros-kinetic-uvc-camera ros-kinetic-image-* ros-kinetic-rqt-image-view

#(建议另外编译安装) sudo apt-get install ros-kinetic-turtlebot3 ros-kinetic-turtlebot3-msgs ros-kinetic-turtlebot3-simulations

sudo apt-get install ros-kinetic-rosserial ros-kinetic-rosserial-server ros-kinetic-rosserial-arduino

sudo apt-get install ros-kinetic-joy ros-kinetic-teleop-twist-joy ros-kinetic-teleop-twist-keyboard ros-kinetic-laser-proc ros-kinetic-rgbd-launch ros-kinetic-depthimage-to-laserscan ros-kinetic-rosserial-arduino ros-kinetic-rosserial-python ros-kinetic-rosserial-server ros-kinetic-rosserial-client ros-kinetic-rosserial-msgs ros-kinetic-amcl ros-kinetic-map-server ros-kinetic-move-base ros-kinetic-urdf ros-kinetic-xacro ros-kinetic-compressed-image-transport ros-kinetic-rqt-image-view ros-kinetic-gmapping ros-kinetic-navigation

sudo apt-get install ros-kinetic-joy ros-kinetic-teleop-twist-joy ros-kinetic-teleop-twist-keyboard ros-kinetic-laser-proc ros-kinetic-rgbd-launch ros-kinetic-depthimage-to-laserscan ros-kinetic-rosserial-arduino ros-kinetic-rosserial-python ros-kinetic-rosserial-server ros-kinetic-rosserial-client ros-kinetic-rosserial-msgs ros-kinetic-amcl ros-kinetic-map-server ros-kinetic-move-base ros-kinetic-urdf ros-kinetic-xacro ros-kinetic-compressed-image-transport ros-kinetic-rqt-image-view ros-kinetic-gmapping ros-kinetic-navigation

sudo apt-get install ros-kinetic-rosjava-build-tools

sudo apt-get install ros-kinetic-ros-controllers ros-kinetic-gazebo* ros-kinetic-moveit* ros-kinetic-dynamixel-sdk ros-kinetic-dynamixel-workbench-toolbox ros-kinetic-robotis-math ros-kinetic-industrial-core

sudo apt-get install ros-kinetic-joy ros-kinetic-joystick-drivers ros-kinetic-teleop-twist-joy















20180703
重新实践学习 ros 的基本服务 应用框架
	cd ~/ catkin _ ws / src
	catkin_create_pkg ros_tutorials_service message_generation std_msgs roscpp
修改功能包配置文件(package.xml)加入了
	<exec_depend>message_runtime</exec_depend>
修改构建配置文件(CMakeLists.txt)
	## 服务声明: SrvTutorial . srv
	add_service_files( FILES SrvTutorial.srv )

	## 这是一个设置依赖消息的选项。
	## 如果未安装 std _ msgs ,则在构建过程中会发生错误。
	generate_messages( DEPENDENCIES std_msgs )

	## 这是 catkin 功能包选项,它描述了库、 catkin 构建依赖和依赖系统的功能包。
	catkin _ package ( LIBRARIES ros_tutorials_service )

	## 设置包含目录。
	include_directories (${catkin_INCLUDE_DIRS})

	## 这是 service_server 节点的构建选项。
	## 设置可执行文件、目标链接库和附加依赖项。
	add_executable(service_server src/service_server.cpp)
	add_dependencies(service_server ${${PROJECT_NAME}_EXPORTED_TARGETS} ${catkin_EXPORTED_TARGETS})
	target_link_libraries(service_server ${catkin_LIBRARIES})

	## 这是节点的构建选项。
	add_executable(service_client src/service_client.cpp)
	add_dependencies(service_client ${${PROJECT_NAME}_EXPORTED_TARGETS} ${catkin_EXPORTED_TARGETS})
	target_link_libraries(service_client ${catkin_LIBRARIES})
在 ros_tutorials_service/srv 书写服务数据规范表格 SrvTutorial.srv
在 ros_tutorials_service/src 书写服务端 service_server 客户端 service_client
回到 catkin_ws
catkin_make 	
	生成的文件位于“~/catkin_ws/build”目录和“~/catkin_ws/devel”目录。
	目录“~/catkin_ws/build”包含catkin构建中使用的配置,
	“~/catkin_ws/devel/lib/ros_tutorials_service”包含可执行文件,
	“~/catkin_ws/devel/include/ros_tutorials_service” 保存从消息文件自动生成的服务头文件。
	如果您想知道这些文件,请进入各自的目录查看。
运行测试:
	终端一:
	roscore
	终端二:
	source ~/ros/catkin_ws/devel/setup.bash
	rosrun ros_tutorials_service service_server	
	终端三:
	source ~/ros/catkin_ws/devel/setup.bash
	rosrun ros_tutorials_service service_client 67 90
也可以通过命令来获取 待命的服务
	//ros_tutorial_srv 是 service_server 启动了的服务
	rosservice call /ros_tutorial_srv 10 2	
通过ros GUI工具获取 待命的服务
	rqt
	[Plugins]→[Service]→[Service Caller]


动作服务器和客户端 应用框架
	在 catkin_ws/src
	catkin_create_pkg ros_tutorials_action message_generation std_msgs actionlib_msgs actionlib roscpp
修改功能包配置文件(package.xml)加入了
	<exec_depend>message_runtime</exec_depend>
修改构建配置文件(CMakeLists.txt)
	find_package(Boost REQUIRED COMPONENTS system)
	add_action_files(FILES Fibonacci.action)
	generate_messages(DEPENDENCIES actionlib_msgs std_msgs)
	catkin_package (
	  LIBRARIES ros_tutorials_action
	  CATKIN_DEPENDS std_msgs actionlib_msgs actionlib roscpp
	  DEPENDS Boost
	)
	include_directories(${catkin_INCLUDE_DIRS} ${Boost_INCLUDE_DIRS})
	add_executable(action_server src/action_server.cpp)
	add_dependencies(action_server ${${PROJECT_NAME}_EXPORTED_TARGETS} ${catkin_EXPORTED_TARGETS})
	target_link_libraries(action_server ${catkin_LIBRARIES})
	add_executable(action_client src/action_client.cpp)
	add_dependencies(action_client ${${PROJECT_NAME}_EXPORTED_TARGETS} ${catkin_EXPORTED_TARGETS})
	target_link_libraries(action_client ${catkin_LIBRARIES})
在 ros_tutorials_service/action 书写服务数据规范表格 Fibonacci.action
	可以在动作文件中找到的目标(goal)、结果(result)和反馈(feedback)之外,
	动作基本上还使用两个额外的消息:取消(cancel)和状态(status)。
	取消(cancel)消息使用actionlib_msgs/GoalID,它在动作运行时可以取消动作客户端和单独节点上的动作的执行。
	状态(status)消息可以根据状态转换 (如PENDING、ACTIVE、PREEMPTED和SUCCEEDED )检查当前动作的状态。
在 ros_tutorials_service/src 书写服务端 service_server 客户端 service_client
回到 catkin_ws
catkin_make 
运行测试:
	终端一:
	roscore
	终端二:
	source ~/ros/catkin_ws/devel/setup.bash
	rosrun ros_tutorials_action action_server	
	终端三:
	rostopic list -v	//查看 查看当前动作消息的使用情况	
	终端四:
	source ~/ros/catkin_ws/devel/setup.bash
	rosrun ros_tutorials_action action_client
也可以通过命令来获取 待命的服务

param 参数的用法,基于 基本服务 应用框架 的例子修改, (未实践)
把 service_server 改成,不只是 加法,还可以减法,乘除法,
	通过 nh.setParam(“calculation_method”,1); 给ros环境里注册一个 calculation_method 的参数, 默认值是 1
	通过 nh.getParam(“calculation_method”,g_operator); 把 calculation_method 的值
	赋予到 service_server 程序中的 g_operator 变量
编译运行 service_server 后 rosparam list 可以看到注册上的 calculation_method 参数
设置参数 rosparam set /calculation_method 2
	例如这里 calculation_method 设为 2 即service_server的 g_operator 变量很快也会变为2,对应着会执行减法 











20180706
重新学习 rviz gazebo
以下是 移动机器人的 初步实践
使用 turtlebot3 的例子时出错,因此把之前预安装的 turtlebot3 相关包删掉
sudo apt-get remove ros-kinetic-turtlebot3 ros-kinetic-turtlebot3-msgs ros-kinetic-turtlebot3-simulations
然后使用编译编译安装的方式,重新安装
	cd ~/ros/catkin_ws/src
	git clone https://github.com/ROBOTIS-GIT/turtlebot3.git
	git clone https://github.com/ROBOTIS-GIT/turtlebot3_msgs.git
	git clone https://github.com/ROBOTIS-GIT/turtlebot3_simulations.git	
	cd ../ && catkin_make
rviz 模拟运行 turtlebot3
	终端1:
	export TURTLEBOT3_MODEL=burger
	roslaunch turtlebot3_fake turtlebot3_fake.launch
	//会自动启动 roscore
	终端2:
	export TURTLEBOT3_MODEL=burger
	roslaunch turtlebot3_teleop turtlebot3_teleop_key.launch
	//键盘控制, rviz 可看到模型在动
	终端3:
	rqt_graph
	//查看 话题 服务...	
	rosrun rqt_tf_tree rqt_tf_tree
	//查看机器人每个部分的 tf 变换
gazebo 模拟运行
	实验一
		gazebo //打开gazebo
	实验二
		显示 turtlebot-waffle 在gazebo 
		export TURTLEBOT3_MODEL=waffle	
		roslaunch turtlebot3_gazebo turtlebot3_empty_world.launch
	实验三
		创建的 turtlebot3.world 环境模型
		终端一
		export TURTLEBOT3_MODEL=waffle	
		roslaunch turtlebot3_gazebo turtlebot3_world.launch	
		终端二
		export TURTLEBOT3_MODEL=waffle	
		roslaunch turtlebot3_teleop turtlebot3_teleop_key.launch
		//键盘控制, gazebo 可看到模型在动
		终端三
		export TURTLEBOT3_MODEL=waffle	
		roslaunch turtlebot3_gazebo turtlebot3_gazebo_rviz.launch
		//调用 rviz 监控,RViz可以检查Gazebo中运行的机器人的位置、距离传感器值和摄像机图像
		add -> camera 可以打开 turtlebot-waffle 的摄像头 !! 
	实验四:
		虚拟 slam  建立地图
		终端一
		export TURTLEBOT3_MODEL=waffle	
		roslaunch turtlebot3_gazebo turtlebot3_world.launch	
		终端二
		export TURTLEBOT3_MODEL=waffle	
		roslaunch turtlebot3_teleop turtlebot3_teleop_key.launch
		//键盘控制, gazebo 可看到模型在动
		终端三
		export TURTLEBOT3_MODEL=waffle	
		roslaunch turtlebot3_slam turtlebot3_slam.launch
		//并且打开了 rviz 观察
		终端四
		rosrun map_server map_saver -f ~/map	//保存 rviz 上辨别出的地图
	实验五:
		利用地图导航
		终端一
		export TURTLEBOT3_MODEL=waffle	
		roslaunch turtlebot3_gazebo turtlebot3_world.launch	
		终端二
		export TURTLEBOT3_MODEL=waffle
		roslaunch turtlebot3_navigation turtlebot3_navigation.launch map_file:=$HOME/map.yaml
		//加载刚刚slam得到的地图!! 并启用 rviz
		//但出现 turtlebot  can't locate node [amcl] in package [amcl]的错误
		//初步认为是  catkin_ws/devel/lib/amcl 与 opt/ros/kinetic/lib/amcl 冲突了,
		//程序优先选用前者,但是并没有,所以把opt/ros/kinetic/lib/amcl 
		//	拷贝到  catkin_ws/devel/lib/amcl
		//在 rviz 上 可以设置设置目的地 尝试导航
		终端三
		export TURTLEBOT3_MODEL=waffle	
		roslaunch turtlebot3_teleop turtlebot3_teleop_key.launch
		导航操作:
		rviz上: 先 2D Pose Estimate 点到 waffle 上,并指定正方向
		然后,键盘随便控制小车一会, 小车会自动识别出身处地图位置
		rviz上: 先 2D Nav Goal 随便点到一个目地点上,waffle 便会自动规划路线,并跑到指定地方!!
		// 2D Nav Goal 这一步出现问题:
/opt/ros/kinetic/lib/move_base/move_base: symbol lookup error: /opt/ros/kinetic/lib//libdwa_local_planner.so: undefined symbol: _ZN18base_local_planner16LocalPlannerUtil12getLocalPlanERN2tf7StampedINS1_9TransformEEERSt6vectorIN13geometry_msgs12PoseStamped_ISaIvEEESaISA_EE
[move_base-4] process has died [pid 6311, exit code 127, cmd /opt/ros/kinetic/lib/move_base/move_base cmd_vel:=/cmd_vel odom:=odom __name:=move_base __log:=/home/william/.ros/log/79e7336e-80ec-11e8-b092-a0b3cc2758b2/move_base-4.log].
log file: /home/william/.ros/log/79e7336e-80ec-11e8-b092-a0b3cc2758b2/move_base-4*.log
^C[rviz-5] killing on exit
		把 move_base 和 libdwa_local_planner.so 拷贝到 catkin_ws/devel/lib 下,也没有用
/home/william/ros/catkin_ws/devel/lib/move_base/move_base: symbol lookup error: /home/william/ros/catkin_ws/devel/lib//libdwa_local_planner.so: undefined symbol: _ZN18base_local_planner16LocalPlannerUtil12getLocalPlanERN2tf7StampedINS1_9TransformEEERSt6vectorIN13geometry_msgs12PoseStamped_ISaIvEEESaISA_EE
[move_base-4] process has died [pid 14715, exit code 127, cmd /home/william/ros/catkin_ws/devel/lib/move_base/move_base cmd_vel:=/cmd_vel odom:=odom __name:=move_base __log:=/home/william/.ros/log/79e7336e-80ec-11e8-b092-a0b3cc2758b2/move_base-4.log].
log file: /home/william/.ros/log/79e7336e-80ec-11e8-b092-a0b3cc2758b2/move_base-4*.log
		这个问题暂时搁置!!


以下是机械臂的基础实践
实验一:
	cd ~/ros/catkin_ws/src
	catkin_create_pkg testbot_description urdf
	cd testbot_description
	catkin_make
	//写机械臂模型
	mkdir urdf
	cd urdf
	gedit testbot.urdf	

注意 urdf 的标签意义:
<link>: 	连杆
  <visual>: 	设置连杆的可视化信息
    <origin>: 	设置相对于连杆相对坐标系的移动和旋转
    <geometry>: 输入模型的形状。提供box、cylinder、sphere等形态,也可以导入COLLADA(.dae)、STL(.stl)格式的设计文件。
    <material>: 设置连杆的颜色和纹理
  <collision>: 	设置连杆的碰撞计算的信息
    <origin>: 	同样 设置相对于连杆相对坐标系的移动和旋转
    <geometry>: 同样 输入模型的形状。在<collision>标签中,可以指定为简单的形态来减少计算时间
  <inertial>:	设置连杆的惯性信息
    <mass>: 	连杆重量(单位:kg)的设置	
    <inertia>: 	惯性张量(Inertia tensor )设置

<joint>: 	与连杆有关系 的 关节
		type: revolute(旋转运动型)、prismatic(平移运动型)、continuous(连续旋转的轮)、fixed(固定型)、
		floating(非固定)和planar(在与轴垂直的平面移动的形态)
  <parent>: 	关节的父连杆
  <child>: 	关节的子连杆
  <origin>: 	将父连杆坐标系转换为子连杆坐标系	
  <axis>:	设置旋转轴 	
  <limit>: 	设置关节的速度、力和半径(仅当关节是revolute或prismatic时)
		属性包括: 关节的力(effort,单位N),最小、最大角度(lower下限,upper上限,以弧度为单位)
		角速度(velocity以弧度/秒) ...
	
	//check_urdf命令来检查已创建的URDF的语法错误以及每个连杆的连接关系
	check_urdf testbot.urdf
	//用关系图表示urdf_to_graphiz程序创建的模型,会创建一个.gv文件和一个.pdf文件
	urdf_to_graphiz testbot.urdf
	//RViz检查机器人模型
	cd ~/ros/catkin_ws/src/testbot_description
	mkdir launch
	cd launch
	gedit testbot.launch	// 写 launch
	catkin_make
<launch>
	<arg name="model" default="$(find testbot_description)/urdf/testbot.urdf"/>
	<arg name="gui" default="True"/>
	<param name="robot_description"textfile="$(arg model)"/>
	<param name="use_gui" value="$(arg gui)"/>
	<node pkg="joint_state_publisher" type="joint_state_publisher" name="joint_state_publisher"/>
	<node pkg="robot_state_publisher" type="state_publisher" name="robot_state_publisher"/>
</launch>
	终端1	
	roscore
	终端2	(出错,一般考虑更改环境变量 source ~/ros/catkin_ws/devel/setup.bash)
	roslaunch testbot_description testbot.launch
	//会看到 一个 joint_state_pubisher 
	终端3	(显示了设计的机械臂)
	rviz
	// add robotmodel
	// 移动 joint_state_pubisher 会看到 rviz 机械臂模型在动!!  

实验二: 测试 OpenManipulator 机械臂
	cd ~/ros/catkin_ws/src	
	git clone https://github.com/ROBOTIS-GIT/open_manipulator.git
	git clone https://github.com/ROBOTIS-GIT/open_manipulator_msgs
	git clone https://github.com/ROBOTIS-GIT/open_manipulator_simulations
	git clone https://github.com/ROBOTIS-GIT/open_manipulator_perceptions
	git clone https://github.com/ROBOTIS-GIT/open_manipulator_with_tb3
	git clone https://github.com/ROBOTIS-GIT/open_manipulator_with_tb3_msgs
	git clone https://github.com/ROBOTIS-GIT/open_manipulator_with_tb3_simulations
	git clone https://github.com/ros-perception/ar_track_alvar
	cd ../ && catkin_make

	OpenManipulator文件目录信息
	open_manipulator_description 	→ 建模功能包
		/urdf
			materials.xacro 			→ 材质信息
			open_manipulator.urdf.xacro 		→ 机械手臂建模		
			open_manipulator.gazebo.xacro	→ 机械手臂 Gazebo 建模
		/rviz
			open_manipulator.rviz			→ RViz 配置文件
		/launch
			open_manipulator_model.launch 	→ 机械手臂状态信息发布者节点运行文件
			open_manipulator_rviz.launch 	→ 机器人建模信息可视化节点运行文件	

	open_manipulator_gazebo 	→ Gazebo package
		/launch
			open_manipulator_controller.launch 	→ ROS - CONTROL 运行文件
			open_manipulator_gazebo.launch	 	→ Gazebo 运行文件
	open_manipulator_dynamixel_ctrl → Dynamixel 控制功能包		
	open_manipulator_moveit		→ MoveIt !功能包
	open_manipulator_msgs		→ 信息功能包
	open_manipulator_position_ctrl	→ 位置控制功能包
	open_manipulator_with_tb3 	→ OpenManipulator 和 TurtleBot3 功能包	

	实验1: RViz中显示机械手臂
	roslaunch open_manipulator_description open_manipulator_rviz.launch
	
	实验2: 在Gazebo仿真空间中看到 OpenManipulator
	roslaunch open_manipulator_gazebo open_manipulator_gazebo.launch
	rostopic list	//查看话题
	在gazebo 点击播放按钮, 即开始物理模拟!!
	输入 joint2 的位置指令
	rostopic pub /open_manipulator/joint2_position/command std_msgs/Float64 "data: -1.0" --once
	可以看到 gazebo的 机械臂在动

实验三: MoveIt 学习	
	实验1
	//打开 Setup Assistant
	roslaunch moveit_setup_assistant setup_assistant.launch 
		创建一个新的功能包 Create New MoveIt Configuration Package
		选择 open_manipulator.urdf.xacro 文件路径, 然后load file
		Sampling density,默认值设置为10,000 然后单击Generate Collision Matrix按钮。
			Sampling density(采样密度),可以根据需要确定组成机器人的连杆之间的碰撞范围,
			Sampling density越高,需要越多的计算来防止机器人在各种姿态中的连杆之间的碰撞,
			用户根据工作环境进行适当的设置。
		创建 两个运动组:
		Planning Groups -> add group, group name: arm, 
		Kinematic Slover: cached_ik_kinematics_plugin/CachedKDLKinematicsPlugin
		Add joint 选择第一个关节到第四个关节joint,然后单击 >, 再单击Save按钮
		Planning Groups -> add group, group name : gripper
		Kinematic Slover: cached_ik_kinematics_plugin/CachedKDLKinematicsPlugin
		Add joint 选择相关的关节grip_joint和grip_joint_sub,然后单击 >, 再单击Save按钮		
			MoveIt! 通过 Planning Groups 将机械手臂分为几个组,
			通过(Kinematic Slover)为各组分别选择合适运动规划 
		创建 2个特殊姿态	
		Robot Poses -> Add Pose, pose name : start, Planning Groups arm
		所有joint 设为 0 
		Robot Poses -> Add Pose, pose name : end, Planning Groups arm
		joint2 设-1, 其他 joint 设为 0
			Robot Poses页面允许您创建和注册机器人的特殊姿态。
		注册 末端执行器
		End Effectors -> Add End Effector, End Effector Name: liner-gripper
		End Effector Group: gripper, Parent Link: link5
			End Effectors页面可以注册机械手臂的end-effector(末端执行器)
		passive joint页面跳过
			passive Joints页面允许您指定不属于运动规划的关节.
			在OpenManipulator中,没有passive joint,因此不作任何改变进入下一阶段,
		Author information页面不能跳过,随便填
			Author information页面上,输入创建功能包的用户的姓名和电子邮件
		Configuration Files页面上完成配置,单击Browse,
		在open_manipulator目录中创建open_manipulator_moveit_example目录
		单击Generate Package
		最后会在 open_manipulator_moveit_example 存在相关 moveit Configuration文件
	cd ~/ros/catkin_ws
	catkin_make
	roslaunch open_manipulator_moveit_example demo.launch
	//这样就可以在 rviz 看到 机械臂 也可以 执行 motionplaning
	//不过还是有很多 奇怪的问题, 可以执行 moveit_setup_assistant 重新修改配置文件
	最大问题是没有 绿色位置球(Interactive marker), 可视化操作机械臂,
	roscd open_manipulator_moveit_example/config/
	gedit kinematics.yaml
	添加:
		arm:
		  kinematics_solver: kdl_kinematics_plugin/KDLKinematicsPlugin
		  kinematics_solver_search_resolution: 0.005
		  kinematics_solver_timeout: 0.005
		  kinematics_solver_attempts: 3
		position_only_ik: true
	重新
	roslaunch open_manipulator_moveit_example demo.launch
	也可以对比 官方做好的例子, 也可以用 moveit_setup_assistant 查看官方的配置而学习!!
 	roslaunch open_manipulator_moveit open_manipulator_demo.launch use_gazebo:=false

	实验2
	接上述, industrial_trajectory_filters 应用
	cd ~/ros/catkin_ws/src	
	git clone https://github.com/ros-industrial/industrial_core.git
	cd ../ && catkin_make
	cd src/industrial_core/industrial_trajectory_filters/
	cp -r planning_request_adapters_plugin_description.xml src include ~/ros/catkin_ws/src/open_manipulator/open_manipulator_moveit_example/
	然后,在config目录中创建smoothing_filter_params.yaml并指定系数
	cd ~/ros/catkin_ws/src/open_manipulator/open_manipulator_moveit_example/config
	gedit smoothing_filter_params.yaml
	添加:
	smoothing_filter_name: /move_group/smoothing_5_coef
	smoothing_5_coef:
	  -0.25
	  -0.50
	  -1.00
	  -0.50
	  -0.25	
	打开launch目录中的ompl_planning_pipeline.launch.xml,
	cd ~/ros/catkin_ws/src/open_manipulator/open_manipulator_moveit_example/launch
	gedit ompl_planning_pipeline.launch.xml
	在planning_adapters添加以下过滤器
		industrial_trajectory_filters/UniformSampleFilter
		industrial_trajectory_filters/AddSmoothingFilter
	添加以下参数:
		<param name="sample_duration" value="0.04"/>
		<rosparam command="load" file="$(find open_manipulator_moveit)/config/smoothing_filter_params.yaml"/>
	最后重新执行:
	roslaunch open_manipulator_moveit_example demo.launch
	可以看到 motionplaning 动作运行更加合理

实验四: OpenManipulator gazebo实验  http://emanual.robotis.com/docs/en/platform/openmanipulator/#manipulation
	先启动 moveit 和rviz :
	roslaunch open_manipulator_moveit open_manipulator_demo.launch use_gazebo:=true
	新终端再启动 gazebo :
	roslaunch open_manipulator_gazebo open_manipulator_gazebo.launch
	这时在 rviz motionplaning 的行为,可以在 gazebo 看到.
	使用消息通信来控制Gazebo仿真器的机器人的实现代码
	在 open_manipulator_position_ctrl/src
	新终端,操纵抓手操作 :
	闭合
	rostopic pub /open_manipulator/gripper std_msgs/String "data: 'grip_on'" --once
	张开
	rostopic pub /open_manipulator/gripper std_msgs/String "data: 'grip_off'" --once

关于实际控制由于没有实际器材,无法操作.

实验五: OpenManipulator + turtlebot3
	roslaunch open_manipulator_with_tb3_waffle_moveit demo.launch
	

学习 .xacro 扩张的urdf
包含外部 .xacro
	<xacro:include filename="$(find open_manipulator_description)/urdf/open_manipulator.gazebo.xacro" />
	<xacro:include filename="$(find open_manipulator_description)/urdf/materials.xacro" />
<transmission>是与ROS-CONTROL一起运行所必须的标签,它输入关节与舵机之间的命令接口,
	它输入关节与舵机之间的命令接口。命令接口有力(effort)、速度(velocity)和位置(position),
	<transmission>:	 设置关节和舵机之间的变量
	  <type>: 	 设置力的传递方式的形状
	  <joint>: 	 设置关节信息设置
	  <actuator>: 	 设置舵机信息
	    <hardwareInterface>: 	设置硬件接口	
	    <mechanicalReduction>:	设置舵机与关节之间的齿轮比	
	例如:
	  <!-- Transmission 1 -->
	  <transmission name="tran1">
	    <type>transmission_interface/SimpleTransmission</type>
	    <joint name="joint1">
	      <hardwareInterface>hardware_interface/EffortJointInterface</hardwareInterface>
	    </joint>
	    <actuator name="motor1">
	      <hardwareInterface>hardware_interface/EffortJointInterface</hardwareInterface>
	      <mechanicalReduction>1</mechanicalReduction>
	    </actuator>
	  </transmission>	
关于用于 gazebo 模拟的的 标签内容
	<gazebo>: 	设置Gazebo仿真的参数
	  <mu1>:	设置摩擦系数
	  <mu2>:	设置摩擦系数
	  <material>: 	设置连杆颜色
	  <plugin>:	plugin 是使用ROS消息和服务通信支持通过URDF或SDF创建的机器人模型的传感器和电机的状态和控制的工具	
	    <robotNamespace>:	设置在Gazebo中使用的机器人名称
	    <robotSimType>:	设置机器人仿真界面的插件名称
	这里要配合说说 使用gazebo模拟 的一些内容
 	查看:catkin_ws/src/open_manipulator_simulations/open_manipulator_gazebo/launch/open_manipulator_gazebo.launch
	launch 要打开的包含:
		empty_world.launch文件
		spawn_model节点 
		position_controller.launch文件
	empty_world.launch文件包含运行Gazebo的节点,因此可以设置仿真环境、GUI和时间。Gazebo仿真环境支持SDF格式的文件。
	spawn_model节点根据上面创建的URDF调用机器人,
	position_controller.launch负责设置和运行ROS-CONTROL。
	
关于 move_group, MoveIt! 简单概念
	move_group节点从URDF、Semantic Robot Description Format(SRDF) 和 MoveIt! Configuration接收关于机器人的信息。
	URDF使用先前创建的文件,而SRDF和 MoveIt! Configuration 将通过MoveIt!提供的Setup Assistant 创建。
	move_group节点通过ROS话题和动作提供机器人的状态与控制,还提供周围环境。
	关节状态是通过sensor_msgs/JointStates消息,
	变换信息是通过tf库,
	控制器是通过FollowTrajectoryAction接口向用户发送关于机器人的信息。
	另外,通过planning scene向用户提供关于机器人工作的环境信息和机器人的状态。
	move_group为其可扩展性提供了一个plugin功能,
	并提供了一个通过开源库将各种功能(控制、路径生成、动力学等)应用到用户的机器人的机会。
	MoveIt!内置的插件是已经被许多人验证过的优秀的库,并且还有许多最近开发的开源库也待发布。
	典型有The Open Motion Planning Library(OMPL) 、Kinematic and Dynamic Library(KDL) 和 A Flexible Collision Library(FCL)
	为了创建用于机械手臂的MoveIt!功能包,需要URDF、SRDF以及用于MoveIt!Configuration的文件。
	MoveIt!提供的Setup Assistant(设置助手)基于URDF生成用于创建MoveIt!功能包的SRDF和MoveIt! Configuration文件。
	有必要讨论Setup Assistant 生成的文件:
	






		





















??????????????????
未深入 DQN
约等于 ≈
　　1、几何符号
　　⊥   ∥   ∠   ⌒   ⊙   ≡   ≌    △
　　2、代数符号
　　∝   ∧   ∨   ～   ∫   ≠    ≤   ≥   ≈   ∞   ∶
　　3、运算符号
　　如加号（＋），减号（－），乘号（×或·），除号（÷或／），两个集合的并集（∪），交集（∩），根号（√），对数（log，lg，ln），比（：），微分（dx），积分（∫），曲线积分（∮）等。
　　4、集合符号
　　∪   ∩   ∈
　　5、特殊符号
　　∑    π（圆周率）
　　6、推理符号
　　|a|    ⊥    ∽    △    ∠    ∩    ∪    ≠    ≡    ±    ≥    ≤    ∈    ←
　　↑    →    ↓    ↖    ↗    ↘    ↙    ∥    ∧    ∨
　　&;   §
　　①   ②   ③   ④   ⑤   ⑥   ⑦   ⑧   ⑨   ⑩
　　Γ    Δ    Θ     Λ    Ξ    Ο    Π     Σ    Φ     Χ    Ψ    Ω
　　α    β    γ    δ    ε    ζ    η    θ    ι    κ    λ    μ     ν
　　ξ    ο    π    ρ    σ    τ    υ    φ    χ    ψ    ω
　　Ⅰ Ⅱ Ⅲ Ⅳ Ⅴ Ⅵ Ⅶ Ⅷ Ⅸ Ⅹ Ⅺ Ⅻ
　　ⅰ ⅱ ⅲ ⅳ ⅴ ⅵ ⅶ ⅷ ⅸ ⅹ
　　∈   ∏   ∑   ∕   √   ∝   ∞   ∟ ∠    ∣   ∥   ∧   ∨   ∩   ∪   ∫   ∮
　　∴   ∵   ∶   ∷   ∽   ≈   ≌   ≒   ≠   ≡   ≤   ≥   ≦   ≧    ≮   ≯   ⊕   ⊙    ⊥
　　⊿   ⌒     ℃
　　指数0123：o123
　　7、数量符号
　　如：i，2+i，a，x，自然对数底e，圆周率π。
　　8、关系符号
　　如“＝”是等号，“≈”是近似符号，“≠”是不等号，“＞”是大于符号，“＜”是小于符号，“≥”是大于或等于符号（也可写作“≮”），“≤”是小于或等于符号（也可写作“≯”），。“→ ”表示变量变化的趋势，“∽”是相似符号，“≌”是全等号，“∥”是平行符号，“⊥”是垂直符号，“∝”是成正比符号，（没有成反比符号，但可以用成正比符号配倒数当作成反比）“∈”是属于符号，“⊆ ⊂ ⊇ ⊃”是“包含”符号等。
　　9、结合符号
　　如小括号“（）”中括号“［］”，大括号“｛｝”横线“—”
　　10、性质符号
　　如正号“＋”，负号“－”，绝对值符号“| |”正负号“±”
　　11、省略符号
　　如三角形（△），直角三角形（Rt△），正弦（sin），余弦（cos），x的函数（f(x)），极限（lim），角（∠），
　　∵因为，（一个脚站着的，站不住）
　　∴所以，（两个脚站着的，能站住） 总和（∑），连乘（∏），从n个元素中每次取出r个元素所有不同的组合数（C(r)(n) ），幂（A，Ac，Aq，x^n）等。
　　12、排列组合符号
　　C-组合数
　　A-排列数
　　N-元素的总个数
　　R-参与选择的元素个数
　　!-阶乘 ，如5！=5×4×3×2×1=120
　　C-Combination- 组合
　　A-Arrangement-排列
　　13、离散数学符号
　　├ 断定符（公式在L中可证）
　　╞ 满足符（公式在E上有效，公式在E上可满足）
　　┐ 命题的“非”运算
　　∧ 命题的“合取”（“与”）运算
　　∨ 命题的“析取”（“或”，“可兼或”）运算
　　→ 命题的“条件”运算
　　A<=>B 命题A 与B 等价关系
　　A=>B 命题 A与 B的蕴涵关系
　　A* 公式A 的对偶公式
　　wff 合式公式
　　iff 当且仅当
　　↑ 命题的“与非” 运算（ “与非门” ）
　　↓ 命题的“或非”运算（ “或非门” ）
　　□ 模态词“必然”
　　◇ 模态词“可能”
　　φ 空集
　　∈ 属于（??不属于）
　　P（A） 集合A的幂集
　　|A| 集合A的点数
　　R^2=R○R [R^n=R^(n-1)○R] 关系R的“复合”
　　（或下面加 ≠） 真包含
　　∪ 集合的并运算
　　∩ 集合的交运算
　　- （～） 集合的差运算
　　〡 限制
　　[X](右下角R) 集合关于关系R的等价类
　　A/ R 集合A上关于R的商集
　　[a] 元素a 产生的循环群
　　I (i大写) 环，理想
　　Z/(n) 模n的同余类集合
　　r(R) 关系 R的自反闭包
　　s(R) 关系 的对称闭包
　　CP 命题演绎的定理（CP 规则）
　　EG 存在推广规则（存在量词引入规则）
　　ES 存在量词特指规则（存在量词消去规则）
　　UG 全称推广规则（全称量词引入规则）
　　US 全称特指规则（全称量词消去规则）
　　R 关系
　　r 相容关系
　　R○S 关系 与关系 的复合
　　domf 函数 的定义域（前域）
　　ranf 函数 的值域
　　f:X→Y f是X到Y的函数
　　GCD(x,y) x,y最大公约数
　　LCM(x,y) x,y最小公倍数
　　aH(Ha) H 关于a的左（右）陪集
　　Ker(f) 同态映射f的核（或称 f同态核）
　　[1，n] 1到n的整数集合
　　d(u,v) 点u与点v间的距离
　　d(v) 点v的度数
　　G=(V,E) 点集为V，边集为E的图
　　W(G) 图G的连通分支数
　　k(G) 图G的点连通度
　　△（G) 图G的最大点度
　　A(G) 图G的邻接矩阵
　　P(G) 图G的可达矩阵
　　M(G) 图G的关联矩阵
　　C 复数集
　　N 自然数集（包含0在内）
　　N* 正自然数集
　　P 素数集
　　Q 有理数集
　　R 实数集
　　Z 整数集
　　Set 集范畴
　　Top 拓扑空间范畴
　　Ab 交换群范畴
　　Grp 群范畴
　　Mon 单元半群范畴
　　Ring 有单位元的（结合）环范畴
　　Rng 环范畴
　　CRng 交换环范畴
　　R-mod 环R的左模范畴
　　mod-R 环R的右模范畴
　　Field 域范畴
　　Poset 偏序集范畴 


















北京大学 沈阳 性侵犯女学生
西安交大 导师周筠施压杨宝德至其跳楼
上海西南模范中学包庇 市三好学生李明泰猥琐女生
武汉理工大学包庇 王攀教授奴役陶崇明逼跳楼,大学威逼其胞姐道歉



